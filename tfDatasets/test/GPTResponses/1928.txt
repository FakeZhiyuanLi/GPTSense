No, the Spanish did not conquer land in what is now the United States. They established several colonies in what are now California, Arizona, New Mexico, and Texas, but much of the land they claimed was in Mexico, Central America, South America, and the Caribbean. The United States gained much of its territory from other European powers, such as Britain and France, and from Mexico through the Mexican-American War.