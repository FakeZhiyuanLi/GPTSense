Britain gained substantial territory in North America following the end of the war, including control over all French territory east of the Mississippi River. Britain also gained control over Spanish Florida and Canada. These victories ultimately resulted in the redrawing of the map of North America, transforming Britain into the dominant colonial power on the continent. Additionally, Britain's victory in the war secured its economic and political interests in North America and helped to establish a hegemony of British culture and influence throughout the region.