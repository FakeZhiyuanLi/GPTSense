The result of the War of Independence was the establishment of the United States of America as a separate nation from Great Britain. The signing of the Treaty of Paris in 1783 formally ended the war and recognized the independence of the United States. The war also solidified the principles of democracy, freedom, and independence upon which the nation was founded.