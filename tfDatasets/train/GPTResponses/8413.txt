Unicode encodes characters by assigning each character a unique numerical value, known as a code point. These code points range from 0 to 1,114,111 (hexadecimal 0x10FFFF) and cover various scripts, symbols, and characters from different languages around the world.For example, the letter "A" is assigned the code point U+0041, while the heart symbol "‚ù§" is assigned the code point U+2764. These code points are then translated into binary code, which computers use to store and manipulate data.Unicode uses different encoding schemes, such as UTF-8, UTF-16, and UTF-32, to represent these code points in binary form. UTF-8 is the most widely used and can represent any Unicode character using one to four bytes. UTF-16 uses two bytes for most characters and up to four bytes for some characters, while UTF-32 uses four bytes for all characters.