Germany's imperialistic past can be traced back to the late 19th century, specifically to the period of German Unification in 1871, when the newly formed German Empire began to acquire colonies and territories abroad. Germany held colonies in Africa, Asia, and the Pacific until the end of World War I in 1918 when they were stripped of their overseas territories by the Allied powers. However, it should be noted that Germany's imperial ambitions and actions can be traced back even further to the early modern period, including its role in the colonization and exploitation of the Americas in the 16th and 17th centuries.