<Blog>



<date>29,May,2004</date>
<post>

	 
      One well known guy has this to say about another even famous guy.  "....The first version of Linux was like a time machine. It went back to a system worse than what he already had on his desk. Of course, he was just a kid and didn't know better (although if he had paid better attention in class he should have), but producing a system that was fundamentally different from the base he started with seems pretty good proof that it was a redesign....."   Guessed right?   Andrew Tanenbaum on Linus Torvalds.  More at the article on his homepage: http://www.cs.vu.nl/~ast/brown/ 
     

    
</post>

<date>28,May,2004</date>
<post>

	 
      There are 10 kinds of people in the world... Those that understand binary, and those who do not                                            
     

    
</post>

<date>26,May,2004</date>
<post>

	 
      Check out the  urlLink Wired 40 . Infy moved from 34 to 11. Kudos! Guys, thats too good and a real India Shining Campaign. Anyone willing to discuss!  Shed the Comments 
     

    
</post>

<date>24,May,2004</date>
<post>

	 
       urlLink JoG  Writes " Ijust got back from a road trip through the UK and India. I got to meet up with lots of Java developers. Lots of interesting folks doing lots of cool projects. The high point was at the end of one talk in Hyderabad one of the conference crew brought a birthday cake on stage (it was my birthday) and they had 3000 people sing happy birthday to me. Quite overwhelming. Thanks, everyone!  The peace movement is quite strong everywhere. I took this photograph of a chalk drawing on a street in London. Folks aren't exactly anti-American: they're anti-war. Everyone was hugely excited about Michael Moore's new movie. I'm a very liberal Canadian living in America: I got lots of questions. "   Reza, shared a very interesting note on his Sun Tech Days Experience. The post made me feel the missing out of the event. :(  JoG's Birthday on that day is a news. and BTW, Michael Moore's Fahrenheit 9/11 seems to have wom the Cannes Film award.
     

    
</post>

<date>23,May,2004</date>
<post>

	 
      If the builders built building the way programmers wrote programs, then the first woodpecker to come along would destroy civilization.   urlLink Phoe6  
     

    
</post>

<date>23,May,2004</date>
<post>

	 
      Hello World!    This is the blog version of our Yahoo Group  Start Tech Blogginn   Adios!    
     

    
</post>

<date>24,June,2004</date>
<post>

	 
            More you dig deeper,more you are amazed by what was it like... Was like it Google, or more , or lesser? Was like the real turner? Yeah it was.  "Tim Berners Lee created the WWW, but who put it on the Boards of the Trucks and Lorries?It was Netscape"  Scott (from Mozilla) urlLink SSL    urlLink Javascript  (Enjoying Gmail??)   urlLink Mozilla  come from Netscape and also this guy  urlLink jwz  , urlLink this guy   and lets add more ... when come across. There seems lotta hackers and geeks at Netscape during its yore. 
     

    
</post>

<date>23,June,2004</date>
<post>

	 
      A Simple One!    Longfellow's Bees       The Poet Longfellow in his novel Kavanagh, introduced several clever mathematical problems from an ancient Sanskrit work. One of them is as below :  If one fifth of a hive of bees flew to the ladambaflower, one third flew to the slandbara, three times the difference of these two numbers flew to an arbor, and one bee continued to fly about, attracted on each side by the fragrant ketaki and the malati, what was the number of bees?
     

    
</post>

<date>22,June,2004</date>
<post>

	 
         The most intriguing figure in 19th century cryptanalysis is Charles Babbage, the eccentric British Genius best known for developing the blueprint for the modern computer. He was born in 1791, the son of Benjamin Babbage, a wealthy London banker. When Charles married without his father's permission, he no longer had access to the Babbage fortune, but he still had enough money to be financially secure, and he pursued the life of a roving scholar, applying his mind to whatever problem tickled his fancy. His inventions include the speedometer and the cowcatcher, a device that could be fixed to the front of steam locomotives to clear cattle from railway tracks. In terms of scientific breakthroughs, he was the first to realize that the width of a tree ring depended on that year's weather, and he deduced that it was possible to determine past climates by studying ancient trees. He was also intrigued by statistics, and as a diversion he drew up a set of mortality tables, a basic tool for today's insurance industry.  
     

    
</post>

<date>17,June,2004</date>
<post>

	 
      A Ride into the World of Philosophy. Some intro to  urlLink Sophies World . This is a book that proceeds along with intresting dialogues between a young Norwegian girl Sophie and a fifty year old philosopher monk Alberto.  
     

    
</post>

<date>10,June,2004</date>
<post>

	 
       urlLink http://www.iit.edu/~shartan/tagore/crescent.html    Observe the above url clicking twice or thrice. Add ur comments if any. 
     

    
</post>

<date>09,June,2004</date>
<post>

	 
      A number puzzle from the Guardian Newspaper U.K. - (As seen on a website)  This puzzle was given to my class in a Functional Programming lecture at uni. It is listed as originating from the Guardian but I'm sure I've also heard it on a BBC Radio 4 programme - Puzzle Corner I think.  A 14 digit number is to be constructed in the following way.   It starts with the digit 4 and ends with the digit 3.   It contains two 7's, separated by 7 digits, two 6's, separated by 6 digits, etc.  What is the number??  (Guardian,7 December 2002) 
     

    
</post>

<date>08,June,2004</date>
<post>

	 
      Just now was glancing through a few pages of our president's book "ignited minds".  In this passage he categorically states that Aryabhatta was THE first to give an approximation to Pi. The passage goes this way    "Aryabhatta, born in 476 AD in Kusumapura(now called Patna), was an astronomer and mathematician. He was reputed to be a repository of all the mathematical knowledge known at that point of time. He was only twenty-three years old when he wrote Aryabhatiyam in two parts. The text covers arithmetic, algebra and trignometry and of course, astronomy. He gave a formula for the areas of a triangle and a circle and attempted to give the volumes of a sphere and a pyramid. He was the first to give an approximation to pi as the ratio of a circle's circumference and diameter, arriving at the value of 3.1416. to celebrate this great astronomer, India named its first satellite launched in 1975 Aryabhata" 
     

    
</post>

<date>06,June,2004</date>
<post>

	 
      Synopsis of the latest book from Arthur.C.Clarke goes this way       Based on the recent sensational proof of Fermat's Theorem 350 years later by a young British mathematician, Andrew Wiles, THE LAST THEOREM charts the story of Ranjit Subramanian, a man fascinated by Fermat's Last Theorem - so simple that anyone can understand it, yet not proved for more than three centuries. Ranjit learns about the Indian mathematical genius Ramanujan (1887-1920) and discovers a three-page proof of the Last Theorem: this might even be Fermat's own proof. The discovery of the Theorem wins Ranjit the Fields Medal - and the attention of the NSA cryptography branch. However, Ranjit soon finds himself drawn by physics rather than cryptography, as there have been some spectacular advances in fusion technology. And these in turn lead to a plasma drive that can open up the Solar System ... 
     

    
</post>

<date>01,June,2004</date>
<post>

	 
       :::GTray:::    GTray  , a software to access GMail account.  urlLink http://torrez.us/archives/2004/05/23/gtray.zip  (52.9kb)   :::Gmail -> POP3 converter:::    PGtGM  sits between you GMail account and your email client, converting messages from the web based mailbox into POP3 messages that a program such as Outlook Express or Firebird can understand.   POP3 server now fully works. You are able to download gmail emails through any email client. SMTP server appears to work.  More info:  urlLink http://www.neowin.net/forum/index.php?showtopic=169789    Download Link:  urlLink http://jaybe.org/pgtgm/pgtgm.zip (122.9kb)   NOTE: You need to have the .net runtimes installed  
     

    
</post>


<date>01,June,2004</date>
<post>


       
      Sandwich quote:  The computer is ... the beating heart of virtually every modern military system you can think of, with the exception of the foot soldier.    By Diana ben-Aaron   Professor Joseph Weizenbaum is well-known, both as a teacher of computer science and as an activist for scientific and educational responsibility. He designed the first computerized banking system before coming to MIT in the 1960s. He invented ELIZA, the first "psychiatric" program, and was moved by the reaction to it to write the best-selling Computer Power and Human Reason.   Q: What, if anything, do you think should be the role of the computer in education?   I'll tell you my reaction to that question without answering it directly. There's a Russian joke that goes something like this: Two people are standing in a very large breadline in Moscow, and they're talking about the fact that the harvest failed once more and that's why there's a shortage of bread, and one of them says to the other, "You know, it's all the fault of the Jews and the bicyclists." The other one says, "Why the bicyclists?" and the first one answers, "Why the Jews?"   You might have said "What is the role of computers and bicycles in education?" Then I would have said, "Why the bicycles?" and you "Why the computer?"   A: Yours is an often-asked question. In a sense, it is upside-down. You start with the instrument; the question makes the assumption that of course the computer is good for something in education, that it is the solution to some educational problem. Specifically, [your] question is, what is it good for?   But where does the underlying assumption come from? Why are we talking about computers?I understand [you asked because] I'm a computer scientist, not a bicycle mechanic. But There is something about the computer -- the computer has almost since its beginning been basically a solution looking for a problem.   People come to MIT and to other places, people from all sorts of establishments -- the medical establishment, the legal establishment, the education establishment, and in effect they say, "You have there a very wonderful instrument which solves a lot of problems. Surely there must be problems in my establishment -- in this case, the educational establishment, for which your wonderful instrument is a solution. Please tell me for what problems your wonderful instrument is a solution.   The questioning should start the other way -- it should perhaps start with the question of what education is supposed to accomplish in the first place.Then perhaps [one should] state some priorities -- it should accomplish this, it should do that, it should do the other thing. Then one might ask, in terms of what it's supposed to do, what are the priorities? What are the most urgent problems? And once one has identified the urgent problems, then one can perhaps say, "Here is a problem for which the computer seems to be well-suited." I think that's the way it has to begin.   Q: What are the problems of the educational establishment?   A: The first priority has to be, it seems to me, to lend to those to be educated a mastery of their own language so that they can express themselves clearly and with precision, in speech and in writing.That's the very first priority. The second priority is to give students an entree to and an identity within the culture of their society, which implies a study of history, literature, and all that.   And the third, very close to the second, is to prepare people for living in a society in which science is important, which means to teach them mathematics, or at least arithmetic, and the fundamental skills important to observing the world.   A school system which meets these main objectives might think about introducing something new. Meanwhile, researchers should certainly work on innovative education -- including computer-aided education. But we ought not to use entire generations of schoolchildren as experimental subjects.   In part, this response is based on my belief that what primary and secondary schools teach about computers now is either wrong or can be learned by a reasonably educated person in a few weeks.   Q: Where do you think the study of ethics fits in[to] all that?   A: Without being able to express themselves clearly, without having a mastery of their own language, I think it would be very difficult, to the point of impossibility, for people to think through ethical considerations. I think that mastery of the languages has to be first even in that respect as well. In the study of history of the culture, the literature of the culture, the politics of the culture, and so on -- that's where I think ethics are exemplified.   A question that we should ask is,   Now how well are the schools fulfilling the first priorities? Certainly the answer with respect to language is miserably, absolutely miserably.   MIT certainly gets the cream of the crop of the product of the American school establishment, yet there was a headline in your paper just a few months ago which said that out of a 1000-some freshmen who took the writing test, 800 flunked. How is it then for people who are going to junior colleges? How does it look for people who aren't going to college at all? How does it look for people who dropped out of school when they were 14 or 15? Clearly the American school establishment is failing very seriously.   It is terribly important to ask the reasons the schools are failing so miserably. I think that even if one could show that the introduction of the computer into schools actually effected an improvement, say for example in reading scores, even if one could show that, the question, "Why can't Johnny read?" must still be asked.   There is a very good reason that questions of that kind are uncomfortable. When we ask this question, we may discover that Johnny is hungry when he comes to school, or that Johnny comes from a milieu in which reading is irrelevant to concrete problems or survival on the street -- that is, there is no chance to read, it is a violent milieu, and so on.   You might discover that, and then you might ask the next question: "Why is it that Johnny comes to school hungry? Don't we have school breakfast programs and lunch programs?" The answer to that might be, yes, we used to, but we don't any more.   Why is there so much poverty in our world, in the United States, especially in the large cities? Why is it that classes are so large? Why is it that fully half the science and math teachers in the United States are underqualified and are operating on emergency certificates?   When you ask questions like that, you come upon some very important and very tragic facts about America. One of the things you would discover is that education has a very much lower priority in the United States than do a great many other things, most particularly the military.   It is much nicer, it is much more comfortable, to have some device, say the computer, with which to flood the schools, and then to sit back and say, "You see, we are doing something about it, we are helping," than to confront ugly social realities.   Q: What do you think should be done instead?   A: I think that further questions should be asked, always "why?" just in the way I've indicated. And then I think it becomes necessary to respond to what these questions uncover, to change the fundamental facts that account for the difficulties, as opposed to papering them over by introducing some technological fix.   Q: Do you think that the computer is creating a technical elite, reinforcing old power structures, or remaking American society?   A: I think the computer has from the beginning been a fundamentally conservative force. It has made possible the saving of institutions pretty much as they were, which otherwise might have had to be changed. For example, banking. Superficially, it looks as if banking has been revolutionized by the computer. But only very superficially. Consider that, say 20, 25 years ago, the banks were faced with the fact that the population was growing at a very rapid rate, many more checks would be written than before, and so on. Their response was to bring in the computer. By the way, I helped design the first computer banking system in the United States, for the Bank of America 25 years ago.   Now if it had not been for the computer, if the computer had not been invented, what would the banks have had to do? They might have had to decentralize, or they might have had to regionalize in some way. In other words, it might have been necessary to introduce a social invention, as opposed to the technical invention.   What the coming of the computer did, "just in time," was to make it unnecessary to create social inventions, to change the system in any way. So in that sense, the computer has acted as fundamentally a conservative force, a force which kept power or even solidified power where is already existed.   Q: Did you have these concerns when you were designing the banking system?   A: Not in the slightest. It was a very technical job, it was a very hard job, there were a number of very, very difficult problems., for example, to design a machine that would handle paper checks of various sizes, some of which might have been crumpled in a person's pockets and so on, to handle those the way punch cards are handled in a punch card machine and so on. There were many very hard technical problems. It was a whale of a lot of fun attacking those hard problems, and it never occurred to me at the time that I was cooperating in a technological venture which had certain social side effects which I might come to regret. That never occured to me; I was totally wrapped up in my identity as a professional, and besides, it was just too much fun.   Q: When did it occur to you?   A: I think after spending say 10 years at MIT -- I came here in 1963. Much of that time, much of [the next] 10 years were very turbulent years politically ... Soon after I got here, President Kennedy was assassinated. There was the dream of the Great Society that President Johnson announced, and the civil rights movement, it was very hard-fought, and I of course participated, and the Vietnam War.   The knowledge of behavior of German academics during the Hitler time weighed on me very heavily. I was born in Germany, I couldn't relax and sit by and watch the university in which I now participated behaving in the same way. I had to become engaged in social and political questions. Once that happened I started to think and write about issues of this kind, some realities became increasingly clear to me.   Writing is very much like computer programming; when you sit down to write a program chances are you have a very good idea of what it is you want to do, you have a very good idea of what algorithm you're going to use. In a certain sense, you believe, or you act as if, you've already solved the problem and it's only a question of writing down the solution. So it is when you start to write in ordinary language. It's perfectly clear to many people, at MIT certainly, that in the act of programming you discover new ideas, and most particularly you discover that there are deep holes in your knowledge that you have to fill before you go on. That happens with writing too. So when I started to write about these things, sometimes just more or less for myself, or in letters to others, the realities I am talking about became clear to me.   Q: What about computers and the military?   A: The computer was of course born to the military, so to speak. In the United States, the first fully functioning computer was created in order to compute ballistifc trajectories. And in England, to help decipher military codes, Carl Zuse built his computer in order to deal with mathematical problems wich aries in the design of military aircraft.   In all three instances, the computer was the child of the military to begin with. Certainly after the Second World War the baton, so to speak, was passed to the Americans, the leadership for developing the computer came into American hands, and from that point to this I think it is safe to say that by far most of the research and development of computers has been paid for with military money, directly or indirectly.   It is also safe to say, it is simply a matter of fact, that to date weapons which threaten to wipe out the human species altogether could not be made and could certainly not be delivered with any sort of precision were it not for the computers which guide these weapons.   The computer is very deeply involved with the military. Today it counts as the beating heart of virtually every modern military system you can think of with the exception of the foot soldier.   In their book on the fifth generation, Ed Feigenbaum of Stanford University and Pamela McCorduck say that present "smart weapons" will seem like the wind-up toys compared to the weapons we will have once we've entered the use of the fifth generation of computers; that is, have properly introduced artificial intelligence, vision and so on, into weapons.   So from the very beginning, the computer was basically a military instrument, it's continued to be, and now with the so-called Strategic Defense Initiative, the computer promises to be firmly embedded in the military systems of the world. There is just no doubt about that.   Q: So to be a computer science professional very often means to be working in defense?   A: I would endorse that sentence, except that I would wish either that the last word be put in quotes, or that you change the sentence to read "...to be involved in the military."   And you know, "the military" certainly is very considerably less euphemistic than to say "defense." Now I understand that we're threatened by great forces, like Grenada, Cuba, and Nicaragua, for example, and we have to defend ourselves against them, but the terminology "the military" still hides the reality.   When we think today, for example, of the masses of computers in helicopters, and in all sorts of mobile things like tanks and airplanes, and we think of the many places on earth where these machines are being used every day, whether it is in Afghanistan or someplace in Africa, then the term "the military" also deserves to be replaced with something considerably harsher.   Instead of saying the computer is involved with the military, say the computer is involved with killing people. It is only when you come to that vocabulary, I think, that the euphemism begins to disappear, and I think it's very important that it disappear.   Q: How can people continue to do this, knowing that the things they build will be involved in killing people?   A: People have a series of rationalizations. People say for example that science and technology have their own logic, that they are in fact autonomous. This particular rationalization is profoundly false. It is not true that science marches on in defiance of human will, independent of human will, that just is not the case. But it is comfortable, as I said: it leads to the position that "if I don't do it, someone else will."   Of course if one takes that as an ethical principle then obviously it can serve as a license to do anything at all. "People will be murdered; if I don't do it, someone else will." "Women will be raped; if I don't do it, someone else will." That is just a license for violence.   Other people say, and I think this is a widely used rationalization, that fundamentally the tools we work on are "mere" tools; This means that whether they get use for good or evil depends on the person who ultimately buys them and so on.   There's nothing bad about working in computer vision, for example. Computer vision may very well some day be used to heal people who would otherwise die. Of course, it could also be used to guide missiles, cruise missiles for example, to their destination, and all that. You see, tthe technology itself is neutral and value-free and it just depends how one uses it. And besides -- consistent with that -- we can't know, we scientists cannot know how it is going to be used. So therefore we have no responsibility.   Well, that is false. It is true that a computer, for example, can be used for good or evil. It is true that a helicopter can be used as a gunship and it can also be used to rescue people from a mountain pass. And if the question arises of how a specific device is going to be used, in what I call an abstract ideal society, then one might very well say one cannot know.   But we live in a concrete society, [and] with concrete social and historical circumstances and political realities in this society, it is perfectly obvious that when something like a computer is invented, then it is going to be adopted will be for military purposes. It follows from the concrete realities in which we live, it does not follow from pure logic. But we're not living in an abstract society, we're living in the society in which we in fact live.   If you look at the enormous fruits of human genius that mankind has developed in the last 50 years, atomic energy and rocketry and flying to the moon and coherent light, and it goes on and on and on -- and then it turns out that every one of these triumphs is used primarily in military terms. So it is not reasonable for a scientist or technologist to insist that he or she does not know -- or ca not know -- how it is going to be used.   Q: Do you think the younger generation of computer scientists coming out of MIT has these concerns?   A: I do not know. I just do not know. I should think that if concern were very widespread, if it were deep-rooted, then perhaps progress in computer development might be somewhat slower than it is. So I do not think that younger people are concerned about these things today. But I have very little way of measuring it. I hope I am wrong.   Of course, it's not only computers that come into play here.the support that the Institute generally gets from the military, which is to say the Department of Defense and to a certain extent the Department of Energy, makes it pretty clear that it is not only computer science which is involved here.   By the way, let me say an additional rationalization for working on these things is that there will be wonderful fallout. We get the space program, and out of the space program we get missiles which can devastate the earth in a very few minutes, but we also get other things -- Teflon, for example, and all this computer stuff, and the miniaturization, the microminiaturization of components and so on, eventually gives us electronic watches. This is a derivative of "things can be used for good or evil."   But I think the following: that if one were to ask the medical community in the United States to do research on bacteriological warfare -- that is, to actually go to a laboratory somewhere in the United States and engage in that work -- that most medical people would refuse.   If they were told that out of this would come antitoxins and all sorts of other good and useful products, just as fallout, I think the medical community would in general say, "Well, if we're after antitoxins and other medicines, then let us work on that. To work on that by way of working on bacteriological warfare seems to us insane." I think the rest of the scientific and engineering community might adopt a similar stance.   Q: But they haven't already?   A: Certainly not ... Certainly the most frequent justification one hears for working, for example, on the strategic computing initiative is described by the military to be fundamentally three weapons systems and nothing else, there is no mystery about it, and , is that we will have wonderful consumer products. for example, one member of the laboratory I am in, the Laboratory for Computer Science, seriously and in print suggested we might have television sets on which we can change the channels by voice command as a by-product of the Strategic Computing Initiative -- see, isn't that wonderful?   Q: What is your greatest fear for the future?   A: I have children, let me say, first of all. And of course at this university as at others one sees very many young people. My greatest concern is that these young people won't ever be permitted to grow up, ever to get as old as I am now. I think that is a very realistic fear.   HomePage : http://www-tech.mit.edu/  ---- Copyright 1985 by The Tech. All rights reserved. This story was published on  Tuesday,  April 9, 1985. Volume 105, Number 16 The story was printed on page  2. This article may be freely distributed electronically, provided it is distributed in its entirety and includes this notice, but may not be reprinted without the express written permission of The Tech.  Write to archive@the-tech.mit.edu for additional details.
     

    
</post>

<date>01,June,2004</date>
<post>


       
      While listening to music through a television might seem odd, it is crucial to the M.I.T. plan. The quirk in the law that makes the system legal, Mr. Winstein said, has much to do with the difference between digital and analog technology. The advent of the digital age, with the possibility of perfect copies spread around the world with the click of a mouse, has spurred the entertainment industry to push for stronger restrictions on the distribution of digital works, and to be reluctant to license their recording catalogues to permit the distribution of music over the Internet.  Two students at the Massachusetts Institute of Technology have developed a system for sharing music within their campus community that they say can avoid the copyright battles that have pitted the music industry against many customers.  The students, Keith Winstein and Josh Mandel, drew the idea for their campus-wide network from a blend of libraries and from radio. Their effort, the Libraries Access to Music Project, which is backed by M.I.T. and financed by research money from the Microsoft Corporation, will provide music from some 3,500 CD's through a novel source: the university's cable television network.   A novel approach to serving up music on demand from one of the nation's leading technical institutions is only fitting, admirers of the project say. The music industry's woes started on college campuses, where fast Internet connections and a population of music lovers with time on their hands sparked a file-sharing revolution.   Mr. Winstein, a graduate student in electrical engineering and computer science, described the result as "a new kind of library."  "We certainly hope," he said, "that by having access to all this music immediately, on demand, any time you want, students would be less likely to break the law.'"  So the M.I.T. system, using the analog campus cable system, simply bypasses the Internet and digital distribution, and takes advantage of the relatively less-restrictive licensing that the industry makes available to radio stations and others for the analog transmission.   Although the M.I.T. music could still be recorded by students and shared on the Internet, Professor Abelson said that the situation would be no different from recording songs from conventional FM broadcasts. The system provides music quality that listeners say is not quite as good as a CD on a home stereo but is better than FM radio.   Mr. Winstein said that the equipment cost about $10,000, and the music, which was bought through a company that provides music on hard drives for the radio industry, for about $25,000. Mr. Winstein said they were making the software available to other colleges.  Students have been using a test version for months, and Mr. Winstein said the system was still evolving. The prototype, for example, shows the name of the person who is programming whatever 80-minute block of music is playing. Mr. Winstein said he once received an e-mail message from a fellow student complimenting him on his choice of music (Antonin Dvorak's Symphony No. 8) and telling him "I'd like to get to know you better." She signed the note, "Sex depraved freshman."  Mr. Winstein, who has a girlfriend, politely declined the offer, and said he realized that he might need to add a feature that would let users control the system anonymously.
     

    
</post>

<date>01,June,2004</date>
<post>


       
      But this is no bubble redux. Instead, Silicon Valley, the entrepreneurial hub of the US's high-tech economy, is rebooting, just as a computer does after it crashes. And this time, the geeks are the ones with the upper hand.   The valley is populated with people of various talents, but its essence begins with the software and hardware engineers. They create technology tools that then find investors and users in the marketplace. It is, first and foremost, a high-tech tool shop.   That fundamental truth was forgotten in the boom years. The short-lived dot-coms were just marketing plans lashed to the Internet. They had no technology edge; they were run by marketers and M.B.A.'s. But most of the young companies that survived the crash - and the start-ups that have risen since - are based on innovation and are run by people with deep technical skills.
     

    
</post>

<date>01,June,2004</date>
<post>


       
      They say it takes a minute to find a special person , an hour to appreciate them ,a day to love them , but then an entire life to forget them .   For all you people who say, "I love you" when you have no clue what  love is exactly!!! Something to ponder upon...   Are your palms sweaty, is your heart racing and is your voice caught within your chest?? It isn't love, it's LIKE.   You can't keep your eyes or hands off of her, am I right?? It isn't love, it's LUST.   Are you proud, and eager to show her off?? It isn't love, it's LUCK.   Do you want her because you know she's there?? It isn't love, it's LONELINESS.   Are you with her because it's what everyone wants?? It isn't love, it's LOYALTY.   Are you with her because she kissed you, or held your hand? It isn't love, it's LOW CONFIDENCE.   Do you stay for her confessions of love, because you don't want to hurt her? It isn't love, it's PITY.   Do you belong to her because the sight of her makes your heart skip a beat?? It isn't love, it's INFATUATION.   Do you pardon her faults because you care about her? It isn't love, it's FRIENDSHIP.  Do you tell her every day she is the only one you think of? It isn't love, it's a LIE.  Are you willing to give up all of your favorite things for her sake? It isn't love, it's CHARITY.  Does your heart ache and break when she's sad? Then it's LOVE.   Do you cry for her pain, even when she's strong? Then it's LOVE.  Do her eyes see your true heart, and touch your soul so deeply it hurts? Then it's LOVE.  Do you stay because a blinding, incomprehensible mix of pain and relation pulls you close and holds you to her? Then it's LOVE.   Do you accept her faults because it's a part of who she is? Then it's LOVE.  Are you attracted to others, but stay with her faithfully without regret?? Then it's LOVE.   Would you give her your heart, your life, your death??  Then it's LOVE.  Now, if love is painful, and tortures us so, why do we love? Why is it all we search for in life? This pain, this agony? Why is it all we long for? This torture, this powerful death of self? Why?   The answer is so simple cause it's...LOVE.   It is such an addictive thing that even people who are not having it wish to experience it and share it with others as well. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      by Alfred D. Chandler Jr. HBS-WK  Inventing the Electronic Century: The Epic Story of the Consumer Electronics and Computer Industries — Begin with consumer electronics. That industry began with radio. Two enterprises that commercialized, that is, brought the technology into public use: Radio Corporation of America (RCA), a joint venture of the three leading U.S. electrical and telecommunication manufacturers (General Electric, Westinghouse, and AT&T), and Telefunken (a joint venture of the two European leaders, Siemens and AEG). After World War II knocked out Telefunken, RCA took the lead in commercializing television worldwide. It was then solely responsible for color television, a major managerial and technological achievement. In the 1960s, however, it began to self-destruct by diversifying, first in attempting to compete with IBM in the production of mainframes and then by becoming a conglomerate, purchasing, among others, Hertz Rent A Car, frozen-food companies, savings and loan enterprises, and others. RCA died in the late 1970s, taking with it a number of smaller U.S. enterprises. The latter were acquired by Japanese companies and Europe's Philips.  In the same brief historical period, from the late 1960s to the late 1970s, the Japanese industry led by Sony and Matsushita conquered world markets. Sony became the world's foremost commercializer of products of new technologies in consumer electronics including the Walkman, Triton Color TV, the VCR, the CD (and CD-ROM), and the DVD. Matsushita became the industry's most successful firm in product development, production, and marketing worldwide. By the late 1980s, these two, with Sanyo and Sharp, had driven both the U.S. and European consumer electronics companies out of their own home markets. Japan's achievements are unparalleled in the annals of industrial history, a particularly spectacular performance in a mass-producing, mass-marketing, high-tech industry.  The technological and institutional infrastructure of the new consumer electronics industry was determined in the crucible of international competition between four companies in the 1970s and 1980s—RCA, Sony, Matsushita, and Philips.  Sony provides a model for the successful strategy of commercializing new technologies by using the learning and income from the previous set of successful innovations. Matsushita's story is different and unique. In 1952, Matsushita arranged to acquire the technical capabilities of the Dutch company Philips in return for 35 percent of the Japanese company's equity. It then concentrated on enhancing its functional capabilities in product development, production, and marketing. These learned capabilities permitted it to enter related electronic commercial, industrial, and even information technology markets. As a result, by 1962 only 28 percent of its sales revenues of $64 billion came from consumer electronics.  The story of Europe's Philips provides still another fascinating chapter for business, industrial, and technological historians. Philips played a critical role in providing the technical capabilities that Matsushita and Sony used to commercialize their new products. Then it was driven out of business by these same two Japanese firms and Sharp. Philips had on its own attempted to produce a CD for television, comparable to the earlier CD-ROM for computers, losing half a billion dollars in the effort. As a result, it lacked the funds necessary to build a DVD factory and exited the consumer electronic industry almost entirely at the end of the 1990s. Again, this relatively unknown story provides an intriguing opportunity for description and analysis by business historians.  The opportunities in the evolution of the computer industry The evolution of the computer industry also has an exciting, largely untold story. In no major industry has a single enterprise so shaped its evolution as did IBM in computers. During the half century since the electronic computer industry began, IBM has dominated in terms of revenues and product lines developed. Moreover, in no other industry have the leader's most successful competitors been those that produced products that the leader had commercialized. In the late 1970s, the Japanese industry became competitive by making and marketing IBM plug-compatible mainframes. By the late 1980s, the most successful producers of computers were those that produced and marketed IBM personal computer clones.  In 1963 IBM's revenues were three times those of its major U.S. competitors combined. In 1984 they were six times those of its nearest competitors, Digital Equipment and Japan's Fujitsu; in 1996 they were two and a half times those of its largest competitors, Japan's Fujitsu and Hewlett-Packard.  IBM had, of course, been the dominant enterprise in the data processing industry well before the coming of the electronic computer. Its evolution provides a classic illustration of first-mover advantages. From its beginning in 1914, it created an integrated learning base to commercialize a new data processing punched-card technology. By the 1920s, its initial factory in New York and a new one in Europe were supplying its worldwide marketing organization with electrically driven data processing equipment. In 1927 Remington Rand, the first mover in typewriters, entered the industry by acquiring a small maker of punched-card tabulators. But during the 1930s, Remington Rand never gained more than 15 percent of the market. After World War II, Remington Rand acquired two of the four projects developing high-speed analytical devices for military purposes. In 1951 it introduced the UNIVAC, the first giant commercial computer, but IBM immediately followed with its own 700 computer.  IBM's continued dominance did not, however, come from its 700 computer. It rested on the replacement of electric power by electronic technology for its punched-card tabulators. In 1954 came its 650 computer, powered by vacuum tubes (an invention at the end of World War I), followed by its 1400, powered by a transistor that was first licensed by AT&T in 1952. The 1400 was leased at $2,500 a month, the cost of a middle-sized punched-card tabulator. Its revenues of $2 billion helped to finance the commercializing of the world's most successful data processor, the System 360, a family of compatible computers.  In 1963, before the announcement of the System 360, IBM's computer revenues were $1.24 billion; Rand's were $145.5 million. Thomas Watson Jr., the executive most responsible for the change from data processing through electricity to electronics, noted: "While our great million dollar 700 got the publicity, the 650 became computing's Model T" (Inventing the Electronic Century, 87-8).  In the 1970s the System 360 all but ruled the world. The attempts of the two major U.S. companies, RCA and GE, to build a comparable family of mainframes failed, with large losses in funds and research time. The Japanese and European computer makers were even less successful. The most successful competitors were those that commercialized products on either side of IBM's 360's price and performance standards; these included the much smaller Digital Equipment, with its stripped-down minicomputer, and Control Data's supercomputer.  Then in 1970 Gene Amdahl, the designer of the System 360 and its successor, the System 370, left IBM to start his own enterprise, producing a plug-compatible System 370. Unable to raise the $40 million required to produce his system, he turned to Japan's Fujitsu, which received him with elation and, in turn, made his plug-compatible equipment available to other Japanese computer makers. With the acquisition of Amdahl's technology, Japan's industry quickly captured its own rapidly growing domestic market for computers. Then in the early 1980s, the four European computer producers turned to Fujitsu, Hitachi, and NEC to acquire plug-compatibles on an original equipment manufacturer (OEM) basis, that is, to be sold as products of the European companies.  With this sudden expansion of their market, the same three companies (with Toshiba and Mitsubishi Electric) concentrated on the mass production of a memory chip, which had been invented by Intel. In the briefest period of time during the early 1980s, the Japanese five knocked out the U.S. memory industry, forcing Intel and the four other major U.S. producers to shut down their memory chip plants.  The U.S. computer industry nevertheless recovered through the introduction at that moment of the microprocessor, and with it the personal computer. Here again IBM played the critical role. The personal computer had been initially commercialized by young hobbyists in the late 1970s. In 1980 IBM's managers set up a unit in Boca Raton, Florida, to mass-produce and mass-market a personal computer and to do so within a single year. The unit's revenues were $500 million at the close of the first year, close to Apple's $600 million. By 1983 and 1984 they had soared to $5.5 billion. Moreover, IBM's personal computer was an open system to be licensed by any applicant. Within a brief time 200 clones poured into the market. While few of the clones survived, the U.S.-produced and U.S.-mass-marketed personal computer transformed the computer industry. IBM itself gained little from its mass-produced personal computer. Indeed, it suffered heavy losses in its mainframe business. But IBM clones conquered world markets. And every clone had to use an Intel chip and a Microsoft operating system. The resulting advantages of scale and scope, plus Microsoft's control over applications, made these two the world's most powerful computer companies. By the early 1990s, Apple was the only major survivor of the pre-IBM producers that had its own proprietary operating systems.  Although the Japanese missed out on the personal computer revolution, the rapidly growing demand for computing power created by the swift expansion of local and wide area corporate computer networks (LANS and WANs) and the privatized Internet brought a second Japanese challenge in the early 1990s. Critical here was the development of the workstation using another microprocessor, the reduced instruction set computing (RISC) chip, which, with a UNIX operating system, became the primary competitor to the IBM personal computer clones. This technology—developed by Sun Microsystems and the U.S. makers of minicomputers—was quickly acquired by the Japanese computer companies.  By 1996, as recorded by Datamation, a leading trade journal, the four Japanese companies shared with IBM the revived market for large systems. In servers, the heirs of the workstation—IBM, Hewlett-Packard, and Compaq—led in revenues received, with NEC, Toshiba, Fujitsu, and Hitachi following. More surprising, in desktops, where IBM and Compaq were at the top, with close to the same revenues, three of these four Japanese companies followed. In software, IBM remained the world's leading revenue producer; then came Microsoft, followed by Hitachi, Fujitsu, and NEC. By 1996 the European industry had all but died. It had become a major outlet for the Japanese manufacturers.  Although the briefest facts of the computer and the broader information technology industry are no better known than those of consumer electronics, more has been written on the producers of computers than on those of audio and video. Kenneth Flamm of the Brookings Institution has published two excellent books, Targeting the Computer and Creating the Computer, which provide excellent brief reviews of the industry's story into the 1970s. He has done so by focusing on the computer-making enterprises and their national industries. Martin Campbell-Kelly and William Aspray, in Computer, focus on the early years of the industry, with only three short chapters on the microprocessor era. They do not mention the Japanese challenge, nor do two detailed books on Microsoft, nor does Paul Cerruzzi's excellent study on the evolution of computing technology. Martin Fransman, Japan's Computer and Communication Industry, which focuses on NEC, and Marie Anchordoguy, Computers, Inc., set the stage for Japan's challenge but say little about capturing the European market in mainframes and the industry's swift domination in memory chips. Very little has been written about the move of the core companies into the client server (RISC chips and UNIX operating systems) technology that permitted them to mount a major challenge to the United States in the early 1990s, when the marriage of the corporate WANs and the Internet completed the basic infrastructure of the new electronic-based century. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      What is Love???  A group of 4 to 8 year-olds was asked, "What does love mean?" The answers they gave were broader and deeper than anyone could have imagined.  Enjoy:  "When my grandmother got arthritis, she couldn't bend over and paint her toenails anymore. So my grandfather does it for her all the time, even when his hands got arthritis too. That’s love." Rebecca - age 8   "When someone loves you, the way they say your name is different. You know that your name is safe in their mouth." Billy - age 4   "Love is when you go out to eat and give somebody most of your French fries without making them give you any of theirs." Chrissy - age 6   "Love is what makes you smile when you're tired." Terri - age 4   "Love is when my mommy makes coffee for my daddy and she takes a sip before giving it to him, to make sure the taste is OK." Danny - age 7   "Love is when you tell a guy you like his shirt, then he wears   it everyday" Tina - age 7   "Love is like a little old woman and a little old man who are still friends even after they know each other so well." Tommy - age 6   "During my piano recital, I was on a stage and scared. I looked at all the people watching me and saw my daddy waving and smiling. He was the only one doing that. I wasn't scared anymore. That's love" Cindy - age 8  "My mommy loves me more than anybody. You don't see anyone else kissing me to sleep at night." Clare - Age 5     "Love is when mommy gives daddy the best piece of chicken." Elaine - age 5    "Love is when your puppy licks your face even after you left him alone all day." Mary Ann - age 4   "I know my older sister loves me because she gives me all her old clothes and has to go out and buy new ones." Lauren - age 4   "I let my big sister pick on me because my Mom says she only picks on me because she loves me. So I pick on my baby sister because I love her." Bethany - age 4  ........... Love............ is not only made for lovers.......  its also for friends who luv  each other sometimes better than lover.  Smile and spread some love today.    
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Ms. Gazo, a 33-year-old housewife who lives 600 miles south of Manila in Davao City, is one of more than 100,000 mobile phone users who re-sell Smart's cellular services through a new prepaid service called Smart Buddy e-Load. With a special, $20 chip for her mobile phone, Ms. Gazo can transfer bits of air time to her friends' and acquaintances' phones - as little as 30 pesos worth (about 55 cents)....   .....Prepaid customers buy a computer chip known as a SIM card for their phone that they register with the cellular network by entering a personal identification number. When the chip runs out of credit, they can buy a card with a new PIN allowing them to replenish their credit.   Smart was offering such cards in denominations of as little as 300 pesos. But even that was too much money for many potential rural customers to pay. Printing and distributing more cards to collect smaller sums, however, would add to the company's costs.   Smart discovered a solution in one of its most advanced services. In December 2000, the company introduced something called Smart Money, which enables customers to use their mobile phones to transfer cash from their bank accounts to a MasterCard cash card. Not only can they add funds to their own cash card over their mobile phones, but they can also transfer it to someone else. Husbands can transfer money to their wives without leaving the golf course. Teenagers can get their allowance without leaving the video-game arcade. Smart says 300,000 of its customers are now regular Smart Money users.   Smart then adapted the software for Smart Money to the same philosophy Unilever and Procter & Gamble have used for decades to break into rural markets in Africa and India. A villager might not be able to buy an entire bottle of shampoo or a box of laundry detergent, but he or she can afford to buy enough for a single washing. So Smart decided to miniaturize its prepaid cellular packages.   "This is telecommunications in sachets," said Ramon R. Isberto, Smart's head of public affairs.   Customers with $20 can buy a Smart Buddy SIM card that gives them 1,000 pesos worth of air time, or 100 minutes. They can then sell as little as 30 pesos worth of air time - 3 minutes - to other Smart prepaid customers just by messaging the network the phone number of their customer.    Smart will not say just how much air time is being bought over Smart Buddy, but analysts say more than a third of the companies' prepaid use is already being carried by the service. For Smart, that reduces the need to print and distribute prepaid cards for calls.   Best of all, Smart Buddy buyers can request such a card from re-sellers like Ms. Gazo by phone wherever they happen to be. Ms. Gazo says she has one customer who lives two hours away. Selling to distant customers means selling on credit, but Ms. Gazo said she did not mind.   "It's actually fun," she said.
     

    
</post>

<date>01,June,2004</date>
<post>


       
      The Merrill Lynch note urged the Santa Clara, Calif., company to drop some product lines and cut its costs by reducing headcount between 5,000 and 7,000. It also urged Sun to "focus on creating mission-critical computing systems where it can add value through innovation.   "That includes operating systems (Solaris, Orion), system architecture (blades), systems management (N1), and services."  The note also said it was "tempted to advise giving up on middleware, but the sedimentation trend of middleware moving into the OS [operating system] and the disruptive pricing of Orion (Java Development System) may be a hand worth playing though the selling model is unclear. Sun has made strides in improving its maintenance contract attach rates but may need to invest more in consulting/systems integration (though we don't see a major acquisition)."   It also urged the company to de-emphasize its SPARC processor system and save the roughly $200 million to $300 million that Sun spends on research and development for the processor systems.   "At the same time, Sun can't just dump SPARC outright given the size of the installed base and importance of migration paths," Milunovich wrote. "Customers must be assured that there will be multiple generations of SPARC to support their needs, especially high-end customers, much as HP has had to do with PA-RISC. The pure Solaris/SPARC story would be gone, but it already is."   The note was also hard-hitting about Sun's Java strategy, urging the company to spin off its Java division, asserting that "Java has been a technology success, a so-so branding effort, and a financial failure."  The note urged Sun to "return to its roots while looking to the future." Sun is an innovator, it continued, "but even IBM became selective about its R&D efforts when the red ink flowed. [Sun's]Solaris, Linux, Orion, Mad Hatter, N1, SPARC, x86, storage, Java-'The Network is the Computer' tent is bursting at the seams," he wrote of some of Sun's main product and services lines.  But Milunovich wrote that a large installed base of customers and strong balance sheet will only act as brakes on a "slippery slope to the bottom of [a] ravine filled with carcasses such as DEC, Data General, Compaq, and others."   Lets see how the episode goes. Any Gladiators on your side Sun ?   
     

    
</post>

<date>01,June,2004</date>
<post>


       
      The source of love is God The center of love is sacrifice The genesis of love is faith The power of love is prayer The seed of love is sincerity The laurel of love is trust The pulse of love is concern The test of love is adversity The beauty of love is growth The result of love is service The sparkle of love is hope The wisdom of love is patience The reality of love is fulfilment The sound of love is harmony The bliss of love is tranquility. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      There are moments in life when you miss someone so much that you just want to pick them from our dreams and hug them for real! When the door of happiness closes, another opens; but often times we look so long at the closed door that we don't see the one which has been opened for us.           Don't go for looks; they can deceive. Don't go for wealth; even that fades away. Go for someone who makes you smile, because it takes only a smile to make a dark day seem bright. Find the one that makes your heart smile.           Dream what you want to dream; go where you want to go; be what you want to be, because you have only one life and one chance to do all the things you want to do.           May you have enough happiness to make you sweet, enough trials to make you strong, enough sorrow to keep you human, enough hope to make you happy. The happiest of people don't necessarily have the best of  everything; they just make the most of everything that comes along  their way.           The brightest future will always be based on a forgotten past; you can't go forward in life until you  let go of your past failures and heartaches.           When you were born, you were crying and everyone   around you was smiling. Live your life so at the end,  you're the one who is smiling and everyone around you is crying.           love is god           
     

    
</post>

<date>01,June,2004</date>
<post>


       
      The following technology will be deployed during the Ironman events: * Dell Precision M60 mobile workstations for the Ironman video crew to edit video in real-time over Ironmanlive.com; * Dell Axim wireless-enabled handhelds for spectators to access racer split times from Dell volunteers at a help booth, and for emergency medical teams to access athlete information; * Dell PowerEdge servers to stream data on Ironmanlive.com Web site via Rackspace.com; * Dell Latitude D600 notebook computers for press booth; and * Dell OptiPlex desktops for Ironman Internet Café for athletes and spectators to communicate with friends and family. About Ironman Triathlon
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Monkeys that can move a robot arm with thoughts alone have brought the merger of mind and machine one step closer.  In experiments at Duke University, implants in the monkeys' brains picked up brain signals and sent them to a robotic arm, which carried out reaching and grasping movements on a computer screen driven only by the monkeys' thoughts.  The achievement is a significant advance in the continuing effort to devise thought-controlled machines that could be a great benefit for people who are paralyzed, or have lost control over their physical movements.  In previous experiments, some in the same laboratory at Duke, both humans and monkeys have had their brains wired so they could move cursors on computer screens just by thinking about it. And wired monkeys have moved robot arms by making a motion with their own arms. The new research, however, involves thought-controlled robotic action that does not depend on physical movement by the monkey and that involves the complex muscular activities of reaching and grasping.  The study is being published today in the inaugural issue of The Public Library of Science, a peer-reviewed scientific journal that makes articles available free of charge. The research team was led by Dr. Miguel A. L. Nicolelis, a neurobiology professor and co-director of the Center for Neuroengineering at Duke, in North Carolina. Dr. Nicolelis also did the earlier research on monkeys and robot arms at Duke.  While other laboratories have helped monkeys use thoughts to move robots, using different experimental designs, the Duke findings go furthest in the sense that their robots were mentally assimilated into the animals' brains.  "For nearly completely paralyzed people, this promises to be a fantastic boon," said Dr. Jon Kaas, a psychology professor at Vanderbilt University in Nashville, who is familiar with Dr. Nicolelis's research. "A person could control a computer or robot to do anything in real time, as fast as they can think."  While experts agree that thought-controlled personal robots are many years off, the Duke University team recently showed that humans produce brain signals like those of the experimental monkeys.   "Monkeys not only use their brain activity to control a robot," said Dr. John Chapin, a professor of physiology and pharmacology at the State University of New York Downstate Medical Center in Brooklyn. "They improve their performance with time. The stunning thing is that we can now see how this occurs, how neurons change their tuning as the monkey does different tasks."  Dr. Nicolelis implanted tiny probes called microwires into several brain regions of two rhesus monkeys. At first, each monkey learned to move a joystick that controlled a cursor on a computer screen. When a ball appeared, the animal had to move the cursor to the target to earn a drink of juice. Researchers collected electrical patterns from the monkey's brain as it performed the tasks.  After the monkey became skilled at the exercise, the scientists disconnected the joystick. At first, the monkey jiggled the stick and stared at the screen, Dr. Nicolelis said. Even though the joystick was not working, the monkey's reaching and grasping motor plans were being sent to a computer, which translated those signals into movements on screen.  There was an "incredible moment" when the monkey realized that it could guide the cursor and grasp an object on the screen just by thinking it, Dr. Nicolelis said. The arm dropped. Muscles no longer contracted.  The final step was to divert brain signals to a computer model that controlled the movements of a robot. The monkey continued to think the movements but in doing so it now moved the robot arm directly, without a joystick, which in turn directed movements of the cursor.  Controlling a shaky, jerky robot with thought is not easy, Dr. Nicolelis said. When the robot is first added, the monkey's performance degrades. It takes two days for the animal to learn the mechanical properties of the arm and to incorporate its delays into motor planning areas.  "By the end of training, I would say that these monkeys sensed they were reaching and grasping with their own arms instead of the robot arm," Dr. Nicolelis said. "Every time we use a tool to interact with our environment, such as a computer mouse, car or glasses, our brain assimilates properties of the tool into neuronal space. Tools are appendages which are incorporated into our body schema. As we develop new tools, we reshape our brains," he said. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      From the www.pcmag.com  This machine was adjudged Editors choice  The Dell PowerEdge 1750 is an example of great engineering in a 1U box.Aside from ingenious design,unique features,and comprehensive management tools,the Dell server performed admirably on our bench-mark tests,grabbing the EC for 1U servers. The PowerEdge 1750 has a lengthy list of remarkable features.Making the most of the inherent constraints of a 1U box,this machine and the HP ProLiant DL360 G3 are the only two servers in our roundup with dual redundant power supplies,yet only the PowerEdge 1750 offers two power supplies without taking up a PCI slot. The PowerEdge 1750 has seven user-replaceable cooling fans,though they tend to run a bit noisily.These fans,along with those in the IBM eServer xSeries 335,are highly reliable:When one fails,the others rev up to a higher speed to compensate. Although there isn ’t much room for innovation in 1U chassis design,the PowerEdge 1750 manages to stand apart from the rest with its unique chassis,whose top opens like a book to reveal the inner components —making it much installed software with version identification,enabling firmware and OS upgrades with one click.  OpenManage Array Manager lets you create virtual disks,volumes,and partitions,both locally and remotely,while Asset Information stores acquisition data as well as leasing information,depreciation values,maintenance statistics,and warranty information.Also included is Dell ’s IT Assistant software,which acts as a central console application for managing multiple servers,storage devices,and client systems across the network using SNMP,DMI,and CIM protocols,as well as Server Assistant, easier to service.The Active ID indicator lights takes servicing one step farther by including a blue locator light,which helps identify a specific server in a fully populated rack.  Every Dell server ships with the Dell OpenManage suite of management tools.OpenManage Server Administrator is a browser-based tool for managing individual servers.Server Administrator lets you schedule diagnostic tests on CPUs,the PCI bus,Ethernet and COM ports,hard drives,and RAID controllers. It monitors fan speeds,DIMM status(with failure history),temperature,and voltage status,and it provides a view of for simplified OS and driver installation,server setup,and RAID configuration. The Dell PowerEdge 1750 is a well loaded 1U box.It ’s a strong performer —highly reliable and easy to service and upgrade —and it comes with a solid suite of management tools.You can ’t go wrong with this excellent system.
     

    
</post>

<date>01,June,2004</date>
<post>


       
      This Larry's Oracle. Interesting CEO, who says, he is like bond. This is the Bio of Oracle CEO Larry Ellison: "Everyone Else Must Fail." The initial reviews of the book are enthusiastic: "Chock full of juicy rumors, detailed reporting and a brutal look at Oracle's past and present business practices, every Oracle customer-or competitor-should read this book." The reviews of Ellison and his management team at Oracle, though, are much harsher: "Oracle has moved from a team of B players led by A players to a team of C players led by B players." Other problems facing Oracle: alienated customers and a "conflict-ridden culture that sucks energy into the black hole of corporate politics." http://www.corante.com/ebusiness/redir/30974.html 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Recently, while I was hearing about Java Desktop Environment taking over Windows, now I hear the news of Sun all in crashes. Advertisement      Eric S. Raymond, writing at Newsforge, says "inside Sun, I hear that talent is bailing out of the company because they just don't believe the Solaris-will-prevail story management is peddling."   "The real question is twofold," he muses, then asks rhetorically: "Can OpenOffice.org survive without Sun, and where will Java land?"   Here is ESR's own answer". "Probably not at Microsoft; with C# in the picture, it is unlikely that Microsoft even wants to own Java any more. I have to guess that IBM is the most likely to shoulder both technologies, simply because nobody else is really positioned to do it. But that, of course, raises other worries -- is it really good for [the open source community] if IBM has a lead position in everything?"   Inquirer  urlLink Says: 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Nature deals with breakdowns in a complex system with evolution, and a very important part of evolution is the extinction of particular species. It's a sort of backtracking mechanism that corrects an evolutionary mistake. The Internet is an ecology, so if you build a species on it that is vulnerable to a certain pathogen, it can very well undergo extinction. By the way, the species that go extinct tend to have limited genetic diversity.                                                                                                                    Bill Joy  I am reading about Bill Joy, who was referred by the Fortune Magazine as the  "Edison of Internet"  Quite a good tag, for he has invented vi editor, csh Sell of Unix, Berkeley Unix distribution from which Free BSD and Net BSD came forth scientist behind Java and Jini Technology.  His article   "Why future doesn't need us"  shows his concern about unprecedented technological progress , where robots overtake humans.  I was surprised to hear that Bill left Sun Microsystems, a company which he cofounded 21 years ago.   Here is the excerpt of what I read in the Fortune mag  Bill Joy says:  I'd divide it into six chunks. As an undergraduate at the University of Michigan, I did numerical supercomputing and got to program some of the early Crays. Then I went to Berkeley and started working on Unix and building Internet protocols into it. My third stage was when we started Sun and built workstations and a distributed network file system and the Sparc microprocessor.   I was all set to leave Sun in 1987 when the company entered into a contract with AT&T—which actually owned Unix—and asked me to completely rewrite it in a modular way. But I couldn't find the right programming language, so my fourth career didn't really go anywhere. Then, after the San Francisco earthquake in 1989, I moved to Aspen and started a research lab for Sun called Smallworks, where I messed around some more with the Sparc chip and some other odds and ends.   In 1994, when a large block of ten-year options vested, I was thinking about leaving Sun again, but then the web came along and [CEO Scott McNealy] asked me to stick around a little longer. So I re-enlisted for the second time. That turned out to be the fifth stage, when I worked on the Java programming language, the Jini and JXTA concepts [networking and peer-to-peer technologies, respectively], Java chips for cellphones and smart cards—all that J-stuff. And finally, what really sucked up a lot of time the past couple of years was the aftermath of my Wired article, when I decided to try to expand it into a book that warns about why biotech, nanotechnology, and robotics have the power to render human beings extraneous. That is what I'd have to call the sixth phase of my career.   But I also did other things on the side. I had a gallery in San Francisco that sold the work of untrained, "primitive" artists. I was on the board of the Oregon Shakespeare Festival for four or five years. I'm also really into architecture and architectural modeling on computers. I've worked with Christopher Alexander [the renowned Berkeley professor, artist, and author of The Pattern Language] and Richard Meier [who designed the Getty Museum in Los Angeles]. Great architects are the last of the purists. What they do is not derivative.   When I think of my own work, most of it is built upon the efforts of others. The Unix work I did was derived from the work of Bell Labs and was more like a remodel than new construction. I'd really like to go and do something that's more like Java—that starts from a clean sheet and that isn't required by its compatibility with something else to be so complicated. Unfortunately, too few people get to do that in our industry.    Why, really, did you leave Sun? To become more involved in public issues like these?   There's no ideal time to leave a company, but I feel now that all the projects and strategies at Sun are in good hands. Sure, I could've found another project that needed incubation. We had one to design a new kind of network data-storage architecture that involved 20 or 30 people that I could've stayed involved in. It doesn't have a code name that starts with a J, though.   The problem with big projects like Java or rewriting Unix or designing the Sparc chip is that they require a five-year commitment. So when you come right down to it, I had to decide, "Do I want to push this big rock up a hill again?" Not this time.   Bill Gates faced a similar choice with his Longhorn project. He probably has a lot of great ideas and all these brilliant people, but he also has this antecedent condition he has to take into account—keeping it somewhat in sync with the old Windows. So the beautiful vision may fail because it has to be compatible. I've often wondered why they can't, for once, do something new. I mean really, really new? But then, when I asked myself that same question, that's when I knew I had to leave Sun.         
     

    
</post>

<date>01,June,2004</date>
<post>


       
       Consider a future device for individual use, which is a sort of mechanized private file and library. It needs a name, and, to coin one at random, "memex" will do. A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.    Vanevvar Bush July, 1945  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Bill Joy, is really an amazing man. I have read his "public penance" why future does't need us in the wired magazine some time earlier and I found it very much interesting and it gave me some glimpses of this great man. He has the same role model as I have - Michealangelo. For of this things, which he has said in this article are amazing. Like  Q:I mean, left at the company.  Joy:We're still there in spirit. There are an awful lot of good people at Sun who have been there a long time. Sometimes, founders leaving is a good thing.  You start to get in the way.   Q:Are you any more at peace with what you see coming? Joy:Not when the forces at play are so powerful that we have such strongly negative possible outcomes. Do we care whether we get a police state without civil liberties because the government's "protecting us" from terrorists? I think we do care. Are people paying enough attention to stop it? I don't think so.  Q:Meanwhile the markets continue to pour money into the fields that worry you - genomics, nanotechnology, and robotics. A:Because they don't have to pay the bill.  Q:You mean the damages if something goes wrong? A:Right. But I'm afraid we're not going to have this discussion until there's a really big accident, and maybe not even then. Assuming any of us are still around to have the discussion.  Q:Downsizing expectations? That's pretty pessimistic. A:I'm not a pessimist. Democracy is about individuals giving up the ability to do whatever they want so that everybody can have some rights. We may have to give up some of the power of high technology if we want to keep our civil liberties. And that is a choice, whether we realize it or not.  Q:Tell that to the happy workaholic hordes of Silicon Valley or Wall Street. A: Hey, I quit my job.  Q:All right, you win. What are you doing for fun these days? A:I'm figuring out a meditation wall for my apartment in New York. Eight feet high by 12 feet wide, with an array of overlapping rear projectors, each with a tiny Linux box and connected by gigabit Ethernet. I would love to get 72 dpi but will probably settle for less - about 30 megapixels for the whole thing. [Former Walt Disney Imagineering guru] Bran Ferren and Danny Hillis [inventor of massively parallel supercomputing] at Applied Minds are building it for me. It's very bright. Given that it's in an apartment, the main limitation will be power availability. I'll also need some great 30-megapixel images. Any ideas? I can always put a picture of stars on the wall. In Manhattan, you can't see them - except, of course, in a blackout.   ~~~ Is'nt it nice...   
     

    
</post>

<date>01,June,2004</date>
<post>


       
      RMS is one of the unique person I know about. Uncompromising and a Hardworker. Started GNU project, Emacs and GNU tools. Andy Tanenbauam - Minix. Linux Torvalds Linux. All have been the hackers of this era, whose works I have studied and used .  Here are few excerpts from  urlLink Free as in Freedom    Members of the tight-knit group called themselves " hackers." Over time, they extended the "hacker" description to Stallman as well. In the process of doing so, they inculcated Stallman in the ethical traditions of the "hacker ethic ." To be a hacker meant more than just writing programs, Stallman learned. It meant writing the best possible programs. It meant sitting at a terminal for 36 hours straight if that's what it took to write the best possible programs. Most importantly, it meant having access to the best possible machines and the most useful information at all times. Hackers spoke openly about changing the world through software, and Stallman learned the instinctual hacker disdain for any obstacle that prevented a hacker from fulfilling this noble cause. Chief among these obstacles were poor software, academic bureaucracy, and selfish behavior.   "I remember many sunrises seen from a car coming back from Chinatown," Stallman would recall nostalgically, 15 years after the fact in a speech at the Swedish Royal Technical Institute. "It was actually a very beautiful thing to see a sunrise, 'cause that's such a calm time of day. It's a wonderful time of day to get ready to go to bed. It's so nice to walk home with the light just brightening and the birds starting to chirp; you can get a real feeling of gentle satisfaction, of tranquility about the work that you have done that night."   The way I see it, any being that has power and abuses it deserves to have that power taken away.   "In India many people are interested in free software, because they see it as a way to build their computing infrastructure without spending a lot of money," Stallman says.   "Sometimes I think that perhaps one of the best things I could do with my life is find a gigantic pile of proprietary software that was a trade secret, and start handing out copies on a street corner so it wouldn't be a trade secret any more," said Stallman. "Perhaps that would be a much more efficient way for me to give people new free software than actually writing it myself; but everyone is too cowardly to even take it."    Dear Editors  Your Sep 27 article, When "Piracy" Funds Terrorism, violated a basic principle of journalism: not to make an accusation without positive grounds.  It accused unauthorized CD producers of funding terrorism, but cited no grounds for this except that some of them are in Pakistan.  I have no great sympathy for those who commercially copy music without paying the musicians.  (That includes the major record labels, since most of their musicians do not actually receive royalties for their records.)  But we must reject the attempts to demonize those who copy, whether it means saying they support terrorists or simply calling them "pirates", because this propaganda campaign doesn't stop with commercial copiers.  Its real target is you and me--anyone who sometimes copies a record.  The real terror campaign is being mounted by the record companies, which are suing hundreds and perhaps soon thousands of ordinary people in the US.  It aims to make people so frightened that they do not dare share with their friends.  Helping one's friends is part of human nature, so it takes a lot of fear to make people stop.  I hope that the citizens of India will insist on keeping India safe from record company terror.  Richard Stallman 545 Tech Square Cambridge, Massachusetts, USA     Stallman implored his fellow hackers to resist the lure of easy compromise.    
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Dells PE1750 machine running on National Center of Supercomputing Applications was ranked 4, in the  urlLink Top500   list.  This project is called  urlLink Tungsten .
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Sony, the commertializer of Technology seemed to have plunged according to  urlLink this article . It states the current CEO Idei was responsible, he is pushed back by the playstaion innovator. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Top 10 Implications of Microsoft Buying Google 10. Free worm with every search.  9. Google renamed "Microoosoooft."  8. Google becomes default search engine on all computers; attempts to use Yahoo! causes PCs to emit a pungent odor.  7. The search choices on Google will change from "Web," "Images," "Groups," "Directory," and "News" to "Web," "Shop Microsoft," "Buy From Microsoft Because You Have No Choice," and "Steve Ballmer Dancing."  6. Google Image Search will not show Microsoft's dark side.  5. All searches on the term "Steve Jobs" will return bios on Art Garfunkel, Ike Turner, Burt Ward (a.k.a. Robin), and other second fiddles.  4. Long speeches from Bill Gates about how "Smart Appliances" will allow owners to use Google to search for what's in their fridge. When pressed, Gates admits such appliances are only available in his mansion, and that he's never actually seen his refrigerator.  3. Microsoft uses Google technology to search for weapons of monopolistic destruction.  2. All searches on the term "Larry Ellison" will produce lists of garbage-collection websites.  1. "Bill Gates is richer than Warren Buffett" subliminally inserted into all search results. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      The other day, I blogged about  urlLink two MIT fellas making music sharing possible without being an outlaw . But these sheriffs woke up and are saying  urlLink someother thing . 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      The article was named  urlLink Everyone is a programmer  and introduced few giants to me; Charles Symonyi and Others who developed Uniform Modelling Language at Rational. Charles Symonyi is the guy who first developed the WYISWYG editor. It was called Bravo , Charles later joined Microsoft and developed Word and Excell. ( Know you can know the importance of the person)  in Software Industry. He became a billinoire in process and last year, he had quit MS to start his  urlLink Intentional Software .  Software are becoming complex and difficult to understand. We are keeping on adding extra to the existing thing and thus the base is getting lost or disfigured.  Charles Symonyi, plans to build a Tool which will make programming simpler. A Program to generate program, He says writing a program would be like doing a power point presentation.   But who will build that Programming Machine. Will it not be complexly written code?  See Doing a Powerpoint presentation is easy, but Developing a Power Point Program is not.   Is Symonyi not say a One universal software for all??? The needs are diverse and so should the programs be.  Joslings idea of  urlLink Jackpot   was good also.  Ok anyway, I would like join you guys.. Lets see what we can make.
     

    
</post>

<date>01,June,2004</date>
<post>


       
      This essay by Henry David Thoreau is a classic, which inspired Mahatma Gandhi to revolt against the Bristish rulers.  It reminds me, Swami vivekananda quote,  "Remember  it is the man who makes Laws, and Not the laws make men"  This essays says to follow your consciousness rather than anything else.    A wise man will not leave the right to the mercy of chance, nor wish it to prevail through the power of the majority.  urlLink  Henry David Thoreau   
     

    
</post>

<date>01,June,2004</date>
<post>


       
       MIT and Microsoft Research are in tie up exploring the Arena of Web Services.    They have come up with urlLink  iCampus  Where all the MIT Work and Study can be done through Internet. Like taking courses and giving an exam.   urlLink iLab  is the intiative to bring the Lab to the Desktop Enviroment so that students can do their experiments from their desktop. Few have even desgined a  urlLink Shuttle Track  at MIT where from the computers end, the students can know where their bus is and when should they go to the bus stop , instead of waiting.   Cool Indeed 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      A classic article by Time Magazine,providing some insights of legend in Computers (Science). You may say, that Software should be a Science as viewed by Stallman & co [ I included (at times)], but  urlLink Donald Knuth  himself says that it is very difficult to keep in pace with the scientific aspects of a field which is economically very  rewarding.   The Art of Computer Programming bear the quotes of Bill Gates.( 2 times I have noticed thus far glancing through 10 pages).  A very old  urlLink article . But still very interesting. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      “People think robots are utilitarian, but they can touch you on an emotional and aesthetic level, too,” says Breazeal of Robotic Life Group.  A bed of colorful robotic flowers that respond to human movement is causing a stir at the National Design Triennial at the Smithsonian’s Cooper-Hewitt National Design Museum in New York City. The  60-centimeter-high flowers, made of materials including translucent acrylic, aluminum, and copper, sit in a “soil” of metal shavings atop a table sheathed in brushed aluminum. At rest, the flowers pulse like a heartbeat, but when their built-in sensors detect a visitor approaching, they perk up and sway in response to the visitor’s movements.  “Children are dazzled by them, and adults are amazed,” says curator Donald Albrecht. “Then they want to know how they work.”   Two large cutouts in the table show the G4 Macintosh below that runs the exhibit, some of the 58 motors that power the flowers, and a huge tangle of cables.
     

    
</post>

<date>01,June,2004</date>
<post>


       
      I had been introduced to  urlLink Alice  by one of my friend  urlLink Kiruba . Since then, I am highly fascinated with these linguistic chatting entities. I found number of others like  urlLink Eliza  and lot many others having egos of some singer,actor like... But the best surprise came to me from  urlLink Alan . He is really interesting. Whomever I introduced him to, they are really fascinated about him. Alan is from a Isreali company. At Alicebot, you will many a times see about  urlLink Leobner Prize  . Infact, Alice has won Leobner Prize priviously. But I did not find such informations at Alans place and he is good a holding conversation better than ALICE. When Dr.Rich posted his observations regarding Leobner Prize, I raised a question:   If given a head-head competition between ALICE and ALAN, I am sure ALAN has better chances of winning.  The major difference I have found between ALICE and ALAN is that ALAN can continue with its sentences( it is one of the drawbacks as well, since it can to switch topics between continuing sentences) whereas ALICE is somewhat likea one-liner. I have always wondered, why ALAN is not featuring in Loebner Prize contests. If you ask these things to ALAN, you may get some "political responses"... I dont know what the actual story is.. but dont you all agree that ALAN is good than ALICE? What are the reasons? What are tradeoffs?   To me, the race is can ALICE match and beat ALAN in conversational skills.   These were answers which followed by      ( Dr.Rich) Someone posted a question about the ALAN chat bot so I went to have a look.  This bot evolved from Jason Hutchens' Megahal project, a former Loebner prize winner.  I must admit, when I engaged ALAN in a conversation about ALICE and the Loebner contest, the results were impressive.  That being said, it must be remembered that ALICE and AIML are free, open source technology and the company a-i.com is developing a proprietary "black box".  If you ask ALAN, "How do you work", he explains "My brain consists of a large content tree: A single 'agent file' and a set of 'handlers', which govern a variety of conversation topics."  He also admits, "My brain is still quite small: less than 350 handlers and a few hundred variables. But I'm still a young bot. Think how smart I'll be when I have thousands or more!"  Their web site is impressive and contains a lot of useful links, but there is nothing equivalent to the free AIML software that you can download and create your own bot.  (They have however announced a program that allows poeple to create so-called Private Virtual Personalities).  The ability of ALAN to stay "on topic" for (apparently) longer than ALICE is an illusion which could just as well be created using the   tag of AIML.  It happens that the ALICE brain has a wider range of "one-liner" responses and uses the   tag very little.  But there is no reason in principle that another AIML bot could carry on a conversation just like ALAN.    Has anyone tried teaching ALAN? It is really cool. Ask it "What is logical" and if there is no definition for it, it ask you to define it. So you reply "Logical is when the world blows up because it is run by chatter bots" and from then on that will be the definition. If you want to delete it say, "Forget logical" I tell you, you can have a lot of fun redefining a whole bunch of definitions :-)  I wonder if there is way to teach alice through talking to her?    can't imagine it would be too hard... hmm do any of the interpreters allow for variable expansion in the xml, i don't think that that would be normal xml though... i know with embedded javascript or embedded perl (ProgramV) it would be fairly simple to do that.   The problem is that random clients can't be trusted as teachers.  They can teach the robot all kinds of nonsense like, "The Indianapolis Speedway is located at the North Pole."  In principle you can teach a bot by talking to her, but you have to figure out which teachers you can trust first.  I always say it's like teaching a child language by telling him to go out on the street and believe everything he hears.  Sure, he may learn to talk, but without supervision he won't learn right answers from wrong ones.  I had a version of targeting working in SETL for a while that could pick out targets for the botmaster and carry on a natural language conversation like, "Someone said, 'how do fish swim?' and I said 'I didn't even know they could', what should I have said?"  And then the botmaster could write the new reply in natural language.  Unfortunately that code was lost and it hasn't been re-implemented in any of the other AIML software that I know of.     teach alice through  talking to her?    Sure, done before many times. Not many interpreters provide a mechanism for real-time learning. J-Alice is one of the few still supported interpreters that does. TinyAlice was perhaps the first to introduce a working version of the concept.  Visit http://j-alice.org for links to downloads and more information, etc.   For SETL  That sounds very cool Rich :) Unfortunately, J-Alice doesn't support targeting at all. But such a system much nicer alternative to training the bot. I would certainly love to see a re-implementation of this sometime in the near future ;-)    One possible idea is to use the net to verify or grade a response. I tried the sample question at www.answerbus.com . It returned "Question: Is The Indianapolis Speedway located at the north pole ? Possible answers:   XML   TXT Indianapolis Raceway Park - Located on the west side of Indianapolis Take I-465 to the west side of Indianapolis to the Speedway/Clermont exit; Take exit 16A and keep to the right lane. Indianapolis Motor Speedway - The speedway is a 2 1/2 mile, paved, oval located on the west side of Indianapolis."  No mention of "at the north pole". Hmmm, maybe we should ask the person about that. Maybe they moved it. Just because you find it on the net does not mean its right (yes, a billion people can be wrong), but if you can't find it, you may want some follow up.    Too bad you can't use telecommunication in the Loebner contest.  A bot in the contest couldn't access the info on answerbus.com becauase it would violate the no-telecommunications rule.  You'd have to figure out how to download all the info from answerbus onto the local machine.      Phew.. got to learn and experiment much. and BTW, I will make my   urlLink  phoe6    more smart ;)  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      My  urlLink sister  asked me, why every email client has the name  John Smith  as example.    Interesting indeed.I dunno. Lets post the question at usenet    In any Email Client, the example name is used as "John Smith". Who is this John Smith, and why are all the clients using this same name for example setups.     It leaves a better impression than "John Doe".    The Brit who established the first permanent British colony in North America. (1607, Jamestown, Virginia) An explorer, writer, soldier. It is simply used in America as a generic male name, like John Doe, who was the first man in America to be trampled to death by a female deer. Smith means common, and Doe means anonymous, to get technical. (John Doe was so messed up they couldn't identify him)  (see below)  (just kidding about the deer thing :-)     I have always assumed that it was because Smith was the most common "English" surname and John was the most common given name. Consequently, the example was "real" but could not be associated with an individual.    
     

    
</post>

<date>01,June,2004</date>
<post>


       
       I have read about this story before, but look at this story from Ray's point of view, gives to it a new dimention   To appreciate the implications of this (or any) geometric trend, it is useful to recall the legend of the inventor of chess and his patron, the emperor of China. The emperor had so fallen in love with his new game that he offered the inventor a reward of anything he wanted in the kingdom.  "Just one grain of rice on the first square, Your Majesty."  "Just one grain of rice?"  "Yes, Your Majesty, just one grain of rice on the first square, and two grains of rice on the second square."  "That's it -- one and two grains of rice?"  "Well, okay, and four grains on the third square, and so on."  The emperor immediately granted the inventor's seemingly humble request. One version of the story has the emperor going bankrupt because the doubling of grains of rice for each square ultimately equaled 18 million trillion grains of rice. At ten grains of rice per square inch, this requires rice fields covering twice the surface area of the Earth, oceans included.  The other version of the story has the inventor losing his head. It's not yet clear which outcome we're headed for.  But there is one thing we should note: It was fairly uneventful as the emperor and the inventor went through the first half of the chessboard. After thirty-two squares, the emperor had given the inventor about 4 billion grains of rice. That's a reasonable quantity -- about one large field's worth -- and the emperor did start to take notice.  But the emperor could still remain an emperor. And the inventor could still retain his head. It was as they headed into the second half of the chessboard that at least one of them got into trouble.  So where do we stand now? There have been about thirty-two doublings of speed and capacity since the first operating computers were built in the 1940s. Where we stand right now is that we have finished the first half of the chessboard. And, indeed, people are starting to take notice.  Now, as we head into the next century, we are heading into the second half of the chessboard. And this is where things start to get interesting.   ...    YOU NEVER DID GIVE THE ENDING TO THE EMPEROR STORY. SO DOES THE EMPEROR LOSE HIS EMPIRE, OR DOES THE INVENTOR LOSE HIS HEAD?  I have two endings, so I just can't say.  MAYBE THEY REACH A COMPROMISE SOLUTION. THE INVENTOR MIGHT BE HAPPY TO SETTLE FOR, SAY, JUST ONE PROVINCE OF CHINA.  Yes, that would be a good result. And maybe an even better parable for the twenty-first century. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
       I was reading the excerpt from "The Age of Spirtual Machines" by Ray Kurzweil. I find topic highly educative:   Technologies fight for survival, evolve, and undergo their own characteristic life cycle. We can identify seven distinct stages. During the precursor stage, the prerequisites of a technology exist, and dreamers may contemplate these elements coming together. We do not, however, regard dreaming to be the same as inventing, even if the dreams are written down. Leonardo da Vinci drew convincing pictures of airplanes and automobiles, but he is not considered to have invented either.  The next stage, one highly celebrated in our culture, is invention, a very brief stage, not dissimilar in some respects to the process of birth after an extended period of labor. Here the inventor blends curiosity, scientific skills, determination, and usually a measure of showmanship to combine methods in a new way to bring a new technology to life.  The next stage is development, during which the invention is protected and supported by doting guardians (which may include the original inventor). Often this stage is more crucial than invention and may involve additional creation that can have greater significance than the original invention. Many tinkerers had constructed finely hand-tuned horseless carriages, but it was Henry Ford's innovation of mass production that enabled the automobile to take root and flourish.  The fourth stage is maturity. Although continuing to evolve, the technology now has a life of its own and has become an independent and established part of the community. It may become so interwoven in the fabric of life that it appears to many observers that it will last forever. This creates an interesting drama when the next stage arrives, which I call the stage of the pretenders. Here an upstart threatens to eclipse the older technology. Its enthusiasts prematurely predict victory. While providing some distinct benefits, the newer technology is found on reflection to be missing some key element of functionality or quality. When it indeed fails to dislodge the established order, the technology conservatives take this as evidence that the original approach will indeed live forever.  This is usually a short-lived victory for the aging technology. Shortly thereafter, another new technology typically does succeed in rendering the original technology into the stage of obsolescence. In this part of the life cycle, the technology lives out its senior years in gradual decline, its original purpose and functionality now subsumed by a more spry competitor. This stage, which may comprise 5 to 10 percent of the life cycle, finally yields to antiquity (examples today: the horse and buggy, the harpsichord, the manual typewriter, and the electromechanical calculator).  To illustrate this, consider the phonograph record. In the mid-nineteenth century, there were several precursors, including Édouard-Léon Scott de Martinville's phonautograph, a device that recorded sound vibrations as a printed pattern. It was Thomas Edison, however, who in 1877 brought all of the elements together and invented the first device that could record and reproduce sound. Further refinements were necessary for the phonograph to become commercially viable. It became a fully mature technology in 1948 when Columbia introduced the 33 revolutions-per-minute (rpm) long-playing record (LP) and RCA Victor introduced the 45-rpm small disc. The pretender was the cassette tape, introduced in the 1960s and popularized during the 1970s. Early enthusiasts predicted that its small size and ability to be rerecorded would make the relatively bulky and scratchable record obsolete.  Despite these obvious benefits, cassettes lack random access (the ability to play selections in a desired order) and are prone to their own forms of distortion and lack of fidelity. In the late 1980s and early 1990s, the digital compact disc (CD) did deliver the mortal blow. With the CD providing both random access and a level of quality close to the limits of the human auditory system, the phonograph record entered the stage of obsolescence in the first half of the 1990s. Although still produced in small quantities, the technology that Edison gave birth to more than a century ago is now approaching antiquity.  Another example is the print book, a rather mature technology today. It is now in the stage of the pretenders, with the software-based "virtual" book as the pretender. Lacking the resolution, contrast, lack of flicker, and other visual qualities of paper and ink, the current generation of virtual book does not have the capability of displacing paper-based publications. Yet this victory of the paper-based book will be short-lived as future generations of computer displays succeed in providing a fully satisfactory alternative to paper.  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      IBM's  on demand  strategy has  caught it's research segment as well. The top leaders are ready to provide 3000 researchers working at Watson lab  on demand   They say: IBM so long found innovations in computers, but now they would putting this innovations to real world use.  Mixed feelings.  Not to a conclusion yet.  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      I dont know, how people are obeying a person like Bush.  I think, the soliders who went there to catch him and wage war. cared only  about their daily food/money and little fame. Not the heroics of war or bravery. Are american soliders waging the war at Iraq brave?   bullshit.  No one is standing up for himself sometimes I feel. The media is cheating us. I dont know what the truth is and who is right? It seems will take me ages to find what the problem is.   But what these few rich and powerful americans are demonstrating is, be powerful ( extremely powerful) and then do whatever you like because you are seeing the world and others are not seeing it.  On the day Saddam, was captured I took some refuge in writings of Noam Chomsky. But I dunno how and when these high/moral theories be translated to practicals. Is power only the practical game? If we look at things, most times, it seems that  , it is.      
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Thanks for Jeremy's  urlLink Blog  I located this useful  urlLink info .
     

    
</post>

<date>01,June,2004</date>
<post>


       
      We were intialially fascniated with the ideas of having some top companies doing some next gen works in our locales.. this is some what happening here. Google is seting up its R & D division in Bangalore . ( started man  ?) and Intel is developing the next generation 802.11g and wireless chipsets at Bangalore.  Now, these are not cost cutting moves.   The first news, I tried to /. but they did not find it much interesting to be posted and turned down. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      I am great fan of Google. I have always liked,appreciated and enjoyed the kind of  Intelligence these Developers have put to this tool.  But once, I had a chance of  catching the news alert tool wrong red-handed.   Below are few of the excerpts...    From: Senthil_OR Sent: 21 November 2003 14:10 To: mike.magee[at]theinquirer.net Subject: http://www.theinquirer.net/?article=727    http://www.theinquirer.net/?article=727   Why is this article on Friday November 21, 2003?   Can I know the reasons?     Google picked up this story - it's a glitch on their part I   think... it's   not resurrected on our pages...    Mike    Thank you Mike for your reply. I checked with the home page of theinquirer and made a search at page 2 of the inquirer and could not find this one.  I have sent a feedback to google news alert team.  Senthil    Yeah - don't know what the problem is... the news alerts are useful but oddly I don't have Dell in my news alert system so didn't notice it. Another reader did though - sent me a screen shot.  It's not there, and hasn't been there since 12799-727 stories ago...  Mike        Hi Senthil,  Thank you for bringing this to our attention. Google News is highly unusual in that it provides a news service compiled solely by computer algorithms without any intervention from human editors. Occasionally, this leads to an outdated article appearing in Google News and Google News Alerts. We are working to improve this, and the information you have provided will help us do so.  We appreciate your taking the time to provide feedback on Google News and hope you will contact us in the future with additional observations and suggestions.  Regards, The Google Team  Original Message Follows: ------------------------ From: Senthil_OR Subject: FW: http://www.theinquirer.net/?article=727 Date: Fri, 21 Nov 2003 10:15:12 -0600  Google News Alert Team, I got this link ( http://www.theinquirer.net/?article=727 ) for my Google news alert subscription of "Dell" on Nov-21-2003. It is a very old news and it was not featured in Friday November 21, 2003 edition of www.theinquirer.net  as per the author.  Kindly check this issue.  -Senthil   
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Thanks to Denis Howe for giving me an oppurtunity to (guest) edit some of the definitions at  urlLink The Free On-line Dictionary of Computing   Right now, I am checking with  urlLink 4GL  and  urlLink RPG-II   If you find these topics interesting and would like to share details, you could come in for a discussion with me.
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Have you ever felt the agony of being spammed. There seems to be no way out. A lot of reserch went as to how to stop this spam problem which threathning to extinct email  itself.  Recently many have come up with the Idea of charging the Spammers to stop these kindda of unsolicited and useless mails.   I dunno what these spammers gain by sending these  delete all  stuffs.    Taking Charge      I have switched on my Spam filter at  orsenthil [at] rediffmail [dot] com   - enabling it to accept only from my address book contacts. One disadvantage is someone mailing me fresh will usually be considered as a spammer ( too sorry) by my Mail box.  I have got another Mail ID,from Novell providing some innovative services.  Novell is getting interesting ya!  . At myrealbox.com  I signed their License agreement that if I use my ID for spamming then I would end up paying $10 for each spam mail sent out.Agreed! The best thing of Novell Mail Service is   It is Ad-Free and POP3,SMTP are available for Free!!!   So my new Mail ID is  ors [at] myrealbox [dot] com      Here is another way  Feel free to use this if you want. ( I used from someone else)   Unsolicited Bulk Email Senders, Please Note My " username l [at]  domainname   [dot] {com |org} "  Terms of Service (ToS) :   I am now so overwhelmed by the number of people who want me to look at their bulk email offerings that I am forced to charge Rs50 for each unsolicited bulk email  I receive, plus an additional Rs 50 fee per email for HTML or attachments, charged on your behalf  to your ISP if you do not pay within 30 days or, in the case of  US companies or individuals, to US embassy in India. I also charge Rs50 for testing your "unsubscribe" utility, which is a great bargain since so few of them work, and I know you want yours to operate properly.    Please don't claim I "opted in"  or "subscribed" to your bulk email. I didn't, and don't try to claim I did, because  we both know you're lying.  If you bought an "opt in" list with my email address on it, you got cheated. No one has permission to sell my personal information. Indeed,  if you sell/sold any of my personal information, including my email address, as part of a contact database without my written permission, you must pay me a Rs5 royalty per copy sold, minimum Rs500, and you should inform any email list vendor who sold you my contact information that they owe me money, too.   If you cannot abide by these Terms of Service, or you feel my fees are unfair, do not send me unsolicited bulk email or sell my personal information.  That's simple enough, isn't it?    
     

    
</post>

<date>01,June,2004</date>
<post>


       
        B2 d+ t- k- s u-- f i o x- e- l c-  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      In 1883, a creative engineer named John Roebling was inspired by an idea to build a spectacular bridge connecting New York with the Long Island. However bridge building experts throughout the world thought that this was an impossible feat and told Roebling to forget the idea. It just could not be done. It was not practical. It had never been done before.  Roebling could not ignore the vision he had in his mind of this bridge. He thought about it all the time and he knew deep in his heart that it could be done. He just had to share the dream with someone else. After much discussion and persuasion he managed to convince his son Washington, an up and coming engineer, that the bridge in fact could be built.  Working together for the first time, the father and son developed concepts of how it could be accomplished and how the obstacles could be overcome. With great excitement and inspiration, and the headiness of a wild challenge before them, they hired their crew and began to build their dream bridge.  The project started well, but when it was only a few months underway a tragic accident on the site took the life of John Roebling. Washington was injured and left with a certain amount of brain damage, which resulted in him not being able to walk or talk or even move.   "We told them so."   "Crazy men and their crazy dreams."   "It`s foolish to chase wild visions."   Everyone had a negative comment to make and felt that the project should be scrapped since the Roeblings were the only ones who knew how the bridge could be built. In spite of his handicap Washington was never discouraged and still had a burning desire to complete the bridge and his mind was still as sharp as ever.   He tried to inspire and pass on his enthusiasm to some of his friends, but they were too daunted by the task. As he lay on his bed in his hospital room, with the sunlight streaming through the windows, a gentle breeze blew the flimsy white curtains apart and he was able to see the sky and the tops of the trees outside for just a moment.   It seemed that there was a message for him not to give up. Suddenly an idea hit him. All he could do was move one finger and he decided to make the best use of it. By moving this, he slowly developed a code of communication with his wife.  He touched his wife's arm with that finger, indicating to her that he wanted her to call the engineers again. Then he used the same method of tapping her arm to tell the engineers what to do. It seemed foolish but the project was under way again.   For 13 years Washington tapped out his instructions with his finger on his wife's arm, until the bridge was finally completed. Today the spectacular Brooklyn Bridge stands in all its glory as a tribute to the triumph of one man's indomitable spirit and his determination not to be defeated by circumstances. It is also a tribute to the engineers and their team work, and to their faith in a man who was considered mad by half the world. It stands too as a tangible monument to the love and devotion of his wife who for 13 long years patiently decoded the messages of her husband and told the engineers what to do.   Perhaps this is one of the best examples of a never-say-die attitude that overcomes a terrible physical handicap and achieves an impossible goal.   Often when we face obstacles in our day-to-day life, our hurdles seem very small in comparison to what many others have to face. The Brooklyn Bridge shows us that dreams that seem impossible can be realised with determination and persistence, no matter what the odds are.   Even the most distant dream can be realized with determination and persistence.  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      It was exactly the ditto of what I had read in  urlLink FAIFZILLA  In the early and intial day ( when opensource, freesoftware,linux were same for me) I used to think of rms in comparision  with Mahatma Gandhi; one maths professor there ( at Cochi Unversity of Science and Technology) gave a breif introduction and said that he feels that rms is comparable to Mahatma Gandhi.  rms closed his eyes with shyness. then he came to mike and said, well he( rms) and his movement cannot be compared with Gandhi's coz. he ( rms) does not have the courage which Gandhi  had { I clapped here} and free software movement is not yet as sucessful as Gandhi's movement was.  The sucess of the movement depends upon you ( he pointed to us in the audience).  then it started with the explaination of Free Software Movement and the Gnu operating system. The ditto of the narration which you will find at faifzilla. There was a lot of Hackerish Humor in between, for eg:  rms said  " If you know how to swim; you see a person drowning in pool and it is not bush, would not you save him ?"    - this was w.r.t to helping your neighbour by sharing the software which is one of the freedoms provided by free software.  moving on to serious issues, rms said, " idealism is practical. The people who say that idealism is not possible, they are wrong. If you have long range goal , you either need to have an ideal or lots of money. it is because of idealism that nations like India and US exists"  The speech ended with  St.iGNUcious show , every one enjoyed that with good laugh.  The Q/A session followed. I had noted few points to be asked/discussed: 1) About Don Hopkins. 2) About MIT lab allowing rms to program and start the gnu after he had quit MIT. 3) About the Teaching profession instead of waiter ( he says in his speech) 4) Thanks for GNU Emacs 5) About Donald Knuth and his perception of CS, which is different.  6) Why not GNU HURD? Why is still not happening. 7) Argentina and Germany where lots of politics had crept with software. 8) Free Software having a Free Market 9) BASH  - the Best Shell 10) gnu c library was developed by a 17 year old - rms didnt name him in the speech, got to find links on it. 11)  which search engine do you use? 12) The Fun of CS is lost in preaching and politics of free software. -   urlLink read this thought.   so on.  I asked by starting, which search engine do you use.  rms said : none   I could not proceed further, I detailed when on cyberspace and you want to find something, which search engine do you use.   he said: I dont use any search engine, because it is software running a their computer on their server, I do not have any control on that.  - but me and lot of people use a search engine.  rms: its upon you, I have taken to schism of not using any proprietary software and using only free software.  - by search and internet only ,I could locate information like Open course ware provided by MIT  rms: opencourseware is not under free license  - but they improve the life of humanity  rms : yes, they improve the life of humanity,but they are not under free license.  later on it became that many more questions were to be asked by audiences.  Questions about HURD followed, and he said that they are technical questions and you should solve them.  After having got an autograph in the GNU GPL and Free Software, Free Society Book, I continued. - Do you still have contact with Don hopkins. rms: at times. - What is he doing now. rms: I dont know. - do you think that things like WSF, will be able to combat bush things. rms: I dont know, but I am just doing it. - According to an Indian Philosophy, you need stregth to combat strength. rms: ( agreeing to Indian thought) , yes there should be more people , why not you join in.  sometime later: - Hope you do more programming this year. Have fun with programming. rms: I need to talk on the free software philosiphy, I dont get time to program these days.  - but you are a guru , a master programmer. rms:nodding, yes but there are lot of people programming but very few talking on these matters.  - Happy Hacking.   ( he was involved with signing others and later I heard some continuation on wsf ,bush which on still on his mind)  ---- yes, it true, one will listen only if one hears about free software stuff from rms. like he practises what is says  - ( " one should be the change which he wants to see in this world") Does not use anything other than free software.  I dont know much abt myself. I am not completely as per his views, but I respect rms a lot.  Here are the news paper reports   urlLink  The Hindu (1)   urlLink The Hindu (2)   urlLink The Hindu (3)   - this (3) one was regarding the malayalam font released on that day, I just had brief talk with the person. The person was not completely a gnu/linux one.   urlLink This hindu article  is about Maddog visiting India at SGI premises, Matthew Scheulk having phone conversation with Prez APJ kalam and visiting India and RMS at WSF and kerala.    
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Why the future doesn't need us. By Bill Joy  From the moment I became involved in the creation of new technologies, their ethical dimensions have concerned me, but it was only in the autumn of 1998 that I became anxiously aware of how great are the dangers facing us in the 21st century. I can date the onset of my unease to the day I met Ray Kurzweil, the deservedly famous inventor of the first reading machine for the blind and many other amazing things.  Ray and I were both speakers at George Gilder's Telecosm conference, and I encountered him by chance in the bar of the hotel after both our sessions were over. I was sitting with John Searle, a Berkeley philosopher who studies consciousness. While we were talking, Ray approached and a conversation began, the subject of which haunts me to this day.  I had missed Ray's talk and the subsequent panel that Ray and John had been on, and they now picked right up where they'd left off, with Ray saying that the rate of improvement of technology was going to accelerate and that we were going to become robots or fuse with robots or something like that, and John countering that this couldn't happen, because the robots couldn't be conscious.  While I had heard such talk before, I had always felt sentient robots were in the realm of science fiction. But now, from someone I respected, I was hearing a strong argument that they were a near-term possibility. I was taken aback, especially given Ray's proven ability to imagine and create the future. I already knew that new technologies like genetic engineering and nanotechnology were giving us the power to remake the world, but a realistic and imminent scenario for intelligent robots surprised me.  It's easy to get jaded about such breakthroughs. We hear in the news almost every day of some kind of technological or scientific advance. Yet this was no ordinary prediction. In the hotel bar, Ray gave me a partial preprint of his then-forthcoming bookThe Age of Spiritual Machines, which outlined a utopia he foresaw - one in which humans gained near immortality by becoming one with robotic technology. On reading it, my sense of unease only intensified; I felt sure he had to be understating the dangers, understating the probability of a bad outcome along this path.  I found myself most troubled by a passage detailing adystopian scenario:  THE NEW LUDDITE CHALLENGE   First let us postulate that the computer scientists succeed in developing intelligent machines that can do all things better than human beings can do them. In that case presumably all work will be done by vast, highly organized systems of machines and no human effort will be necessary. Either of two cases might occur. The machines might be permitted to make all of their own decisions without human oversight, or else human control over the machines might be retained.   If the machines are permitted to make all their own decisions, we can't make any conjectures as to the results, because it is impossible to guess how such machines might behave. We only point out that the fate of the human race would be at the mercy of the machines. It might be argued that the human race would never be foolish enough to hand over all the power to the machines. But we are suggesting neither that the human race would voluntarily turn power over to the machines nor that the machines would willfully seize power. What we do suggest is that the human race might easily permit itself to drift into a position of such dependence on the machines that it would have no practical choice but to accept all of the machines' decisions. As society and the problems that face it become more and more complex and machines become more and more intelligent, people will let machines make more of their decisions for them, simply because machine-made decisions will bring better results than man-made ones. Eventually a stage may be reached at which the decisions necessary to keep the system running will be so complex that human beings will be incapable of making them intelligently. At that stage the machines will be in effective control. People won't be able to just turn the machines off, because they will be so dependent on them that turning them off would amount to suicide.   On the other hand it is possible that human control over the machines may be retained. In that case the average man may have control over certain private machines of his own, such as his car or his personal computer, but control over large systems of machines will be in the hands of a tiny elite - just as it is today, but with two differences. Due to improved techniques the elite will have greater control over the masses; and because human work will no longer be necessary the masses will be superfluous, a useless burden on the system. If the elite is ruthless they may simply decide to exterminate the mass of humanity. If they are humane they may use propaganda or other psychological or biological techniques to reduce the birth rate until the mass of humanity becomes extinct, leaving the world to the elite. Or, if the elite consists of soft-hearted liberals, they may decide to play the role of good shepherds to the rest of the human race. They will see to it that everyone's physical needs are satisfied, that all children are raised under psychologically hygienic conditions, that everyone has a wholesome hobby to keep him busy, and that anyone who may become dissatisfied undergoes "treatment" to cure his "problem." Of course, life will be so purposeless that people will have to be biologically or psychologically engineered either to remove their need for the power process or make them "sublimate" their drive for power into some harmless hobby. These engineered human beings may be happy in such a society, but they will most certainly not be free. They will have been reduced to the status of domestic animals.1    In the book, you don't discover until you turn the page that the author of this passage is Theodore Kaczynski - the Unabomber. I am no apologist for Kaczynski. His bombs killed three people during a 17-year terror campaign and wounded many others. One of his bombs gravely injured my friend David Gelernter, one of the most brilliant and visionary computer scientists of our time. Like many of my colleagues, I felt that I could easily have been the Unabomber's next target.  Kaczynski's actions were murderous and, in my view, criminally insane. He is clearly a Luddite, but simply saying this does not dismiss his argument; as difficult as it is for me to acknowledge, I saw some merit in the reasoning in this single passage. I felt compelled to confront it.  Kaczynski's dystopian vision describes unintended consequences, a well-known problem with the design and use of technology, and one that is clearly related to Murphy's law - "Anything that can go wrong, will." (Actually, this is Finagle's law, which in itself shows that Finagle was right.) Our overuse of antibiotics has led to what may be the biggest such problem so far: the emergence of antibiotic-resistant and much more dangerous bacteria. Similar things happened when attempts to eliminate malarial mosquitoes using DDT caused them to acquire DDT resistance; malarial parasites likewise acquired multi-drug-resistant genes.2  The cause of many such surprises seems clear: The systems involved are complex, involving interaction among and feedback between many parts. Any changes to such a system will cascade in ways that are difficult to predict; this is especially true when human actions are involved.  I started showing friends the Kaczynski quote fromThe Age of Spiritual Machines; I would hand them Kurzweil's book, let them read the quote, and then watch their reaction as they discovered who had written it. At around the same time, I found Hans Moravec's bookRobot: Mere Machine to Transcendent Mind. Moravec is one of the leaders in robotics research, and was a founder of the world's largest robotics research program, at Carnegie Mellon University.Robot gave me more material to try out on my friends - material surprisingly supportive of Kaczynski's argument. For example:   The Short Run (Early 2000s)   Biological species almost never survive encounters with superior competitors. Ten million years ago, South and North America were separated by a sunken Panama isthmus. South America, like Australia today, was populated by marsupial mammals, including pouched equivalents of rats, deers, and tigers. When the isthmus connecting North and South America rose, it took only a few thousand years for the northern placental species, with slightly more effective metabolisms and reproductive and nervous systems, to displace and eliminate almost all the southern marsupials.   In a completely free marketplace, superior robots would surely affect humans as North American placentals affected South American marsupials (and as humans have affected countless species). Robotic industries would compete vigorously among themselves for matter, energy, and space, incidentally driving their price beyond human reach. Unable to afford the necessities of life, biological humans would be squeezed out of existence.   There is probably some breathing room, because we do not live in a completely free marketplace. Government coerces nonmarket behavior, especially by collecting taxes. Judiciously applied, governmental coercion could support human populations in high style on the fruits of robot labor, perhaps for a long while.    A textbook dystopia - and Moravec is just getting wound up. He goes on to discuss how our main job in the 21st century will be "ensuring continued cooperation from the robot industries" by passing laws decreeing that they be "nice,"3 and to describe how seriously dangerous a human can be "once transformed into an unbounded superintelligent robot." Moravec's view is that the robots will eventually succeed us - that humans clearly face extinction.  I decided it was time to talk to my friend Danny Hillis. Danny became famous as the cofounder of Thinking Machines Corporation, which built a very powerful parallel supercomputer. Despite my current job title of Chief Scientist at Sun Microsystems, I am more a computer architect than a scientist, and I respect Danny's knowledge of the information and physical sciences more than that of any other single person I know. Danny is also a highly regarded futurist who thinks long-term - four years ago he started the Long Now Foundation, which is building a clock designed to last 10,000 years, in an attempt to draw attention to the pitifully short attention span of our society. (See "Test of Time,"Wired 8.03, page 78.)  So I flew to Los Angeles for the express purpose of having dinner with Danny and his wife, Pati. I went through my now-familiar routine, trotting out the ideas and passages that I found so disturbing. Danny's answer - directed specifically at Kurzweil's scenario of humans merging with robots - came swiftly, and quite surprised me. He said, simply, that the changes would come gradually, and that we would get used to them.  But I guess I wasn't totally surprised. I had seen a quote from Danny in Kurzweil's book in which he said, "I'm as fond of my body as anyone, but if I can be 200 with a body of silicon, I'll take it." It seemed that he was at peace with this process and its attendant risks, while I was not.  While talking and thinking about Kurzweil, Kaczynski, and Moravec, I suddenly remembered a novel I had read almost 20 years ago -The White Plague, by Frank Herbert - in which a molecular biologist is driven insane by the senseless murder of his family. To seek revenge he constructs and disseminates a new and highly contagious plague that kills widely but selectively. (We're lucky Kaczynski was a mathematician, not a molecular biologist.) I was also reminded of the Borg ofStar Trek, a hive of partly biological, partly robotic creatures with a strong destructive streak. Borg-like disasters are a staple of science fiction, so why hadn't I been more concerned about such robotic dystopias earlier? Why weren't other people more concerned about these nightmarish scenarios?  Part of the answer certainly lies in our attitude toward the new - in our bias toward instant familiarity and unquestioning acceptance. Accustomed to living with almost routine scientific breakthroughs, we have yet to come to terms with the fact that the most compelling 21st-century technologies - robotics, genetic engineering, and nanotechnology - pose a different threat than the technologies that have come before. Specifically, robots, engineered organisms, and nanobots share a dangerous amplifying factor: They can self-replicate. A bomb is blown up only once - but one bot can become many, and quickly get out of control.  Much of my work over the past 25 years has been on computer networking, where the sending and receiving of messages creates the opportunity for out-of-control replication. But while replication in a computer or a computer network can be a nuisance, at worst it disables a machine or takes down a network or network service. Uncontrolled self-replication in these newer technologies runs a much greater risk: a risk of substantial damage in the physical world.  Each of these technologies also offers untold promise: The vision of near immortality that Kurzweil sees in his robot dreams drives us forward; genetic engineering may soon provide treatments, if not outright cures, for most diseases; and nanotechnology and nanomedicine can address yet more ills. Together they could significantly extend our average life span and improve the quality of our lives. Yet, with each of these technologies, a sequence of small, individually sensible advances leads to an accumulation of great power and, concomitantly, great danger.  What was different in the 20th century? Certainly, the technologies underlying the weapons of mass destruction (WMD) - nuclear, biological, and chemical (NBC) - were powerful, and the weapons an enormous threat. But building nuclear weapons required, at least for a time, access to both rare - indeed, effectively unavailable - raw materials and highly protected information; biological and chemical weapons programs also tended to require large-scale activities.  The 21st-century technologies - genetics, nanotechnology, and robotics (GNR) - are so powerful that they can spawn whole new classes of accidents and abuses. Most dangerously, for the first time, these accidents and abuses are widely within the reach of individuals or small groups. They will not require large facilities or rare raw materials. Knowledge alone will enable the use of them.  Thus we have the possibility not just of weapons of mass destruction but of knowledge-enabled mass destruction (KMD), this destructiveness hugely amplified by the power of self-replication.  I think it is no exaggeration to say we are on the cusp of the further perfection of extreme evil, an evil whose possibility spreads well beyond that which weapons of mass destruction bequeathed to the nation-states, on to a surprising and terrible empowerment of extreme individuals.     Nothing about the way I got involved with computers suggested to me that I was going to be facing these kinds of issues.  My life has been driven by a deep need to ask questions and find answers. When I was 3, I was already reading, so my father took me to the elementary school, where I sat on the principal's lap and read him a story. I started school early, later skipped a grade, and escaped into books - I was incredibly motivated to learn. I asked lots of questions, often driving adults to distraction.  As a teenager I was very interested in science and technology. I wanted to be a ham radio operator but didn't have the money to buy the equipment. Ham radio was the Internet of its time: very addictive, and quite solitary. Money issues aside, my mother put her foot down - I was not to be a ham; I was antisocial enough already.  I may not have had many close friends, but I was awash in ideas. By high school, I had discovered the great science fiction writers. I remember especially Heinlein'sHave Spacesuit Will Travel and Asimov's I, Robot, with its Three Laws of Robotics. I was enchanted by the descriptions of space travel, and wanted to have a telescope to look at the stars; since I had no money to buy or make one, I checked books on telescope-making out of the library and read about making them instead. I soared in my imagination.  Thursday nights my parents went bowling, and we kids stayed home alone. It was the night of Gene Roddenberry's original Star Trek, and the program made a big impression on me. I came to accept its notion that humans had a future in space, Western-style, with big heroes and adventures. Roddenberry's vision of the centuries to come was one with strong moral values, embodied in codes like the Prime Directive: to not interfere in the development of less technologically advanced civilizations. This had an incredible appeal to me; ethical humans, not robots, dominated this future, and I took Roddenberry's dream as part of my own.  I excelled in mathematics in high school, and when I went to the University of Michigan as an undergraduate engineering student I took the advanced curriculum of the mathematics majors. Solving math problems was an exciting challenge, but when I discovered computers I found something much more interesting: a machine into which you could put a program that attempted to solve a problem, after which the machine quickly checked the solution. The computer had a clear notion of correct and incorrect, true and false. Were my ideas correct? The machine could tell me. This was very seductive.  I was lucky enough to get a job programming early supercomputers and discovered the amazing power of large machines to numerically simulate advanced designs. When I went to graduate school at UC Berkeley in the mid-1970s, I started staying up late, often all night, inventing new worlds inside the machines. Solving problems. Writing the code that argued so strongly to be written.  InThe Agony and the Ecstasy, Irving Stone's biographical novel of Michelangelo, Stone described vividly how Michelangelo released the statues from the stone, "breaking the marble spell," carving from the images in his mind.4 In my most ecstatic moments, the software in the computer emerged in the same way. Once I had imagined it in my mind I felt that it was already there in the machine, waiting to be released. Staying up all night seemed a small price to pay to free it - to give the ideas concrete form.  After a few years at Berkeley I started to send out some of the software I had written - an instructional Pascal system, Unix utilities, and a text editor called vi (which is still, to my surprise, widely used more than 20 years later) - to others who had similar small PDP-11 and VAX minicomputers. These adventures in software eventually turned into the Berkeley version of the Unix operating system, which became a personal "success disaster" - so many people wanted it that I never finished my PhD. Instead I got a job working for Darpa putting Berkeley Unix on the Internet and fixing it to be reliable and to run large research applications well. This was all great fun and very rewarding. And, frankly, I saw no robots here, or anywhere near.  Still, by the early 1980s, I was drowning. The Unix releases were very successful, and my little project of one soon had money and some staff, but the problem at Berkeley was always office space rather than money - there wasn't room for the help the project needed, so when the other founders of Sun Microsystems showed up I jumped at the chance to join them. At Sun, the long hours continued into the early days of workstations and personal computers, and I have enjoyed participating in the creation of advanced microprocessor technologies and Internet technologies such as Java and Jini.  From all this, I trust it is clear that I am not a Luddite. I have always, rather, had a strong belief in the value of the scientific search for truth and in the ability of great engineering to bring material progress. The Industrial Revolution has immeasurably improved everyone's life over the last couple hundred years, and I always expected my career to involve the building of worthwhile solutions to real problems, one problem at a time.  I have not been disappointed. My work has had more impact than I had ever hoped for and has been more widely used than I could have reasonably expected. I have spent the last 20 years still trying to figure out how to make computers as reliable as I want them to be (they are not nearly there yet) and how to make them simple to use (a goal that has met with even less relative success). Despite some progress, the problems that remain seem even more daunting.  But while I was aware of the moral dilemmas surrounding technology's consequences in fields like weapons research, I did not expect that I would confront such issues in my own field, or at least not so soon.     Perhaps it is always hard to see the bigger impact while you are in the vortex of a change. Failing to understand the consequences of our inventions while we are in the rapture of discovery and innovation seems to be a common fault of scientists and technologists; we have long been driven by the overarching desire to know that is the nature of science's quest, not stopping to notice that the progress to newer and more powerful technologies can take on a life of its own.  I have long realized that the big advances in information technology come not from the work of computer scientists, computer architects, or electrical engineers, but from that of physical scientists. The physicists Stephen Wolfram and Brosl Hasslacher introduced me, in the early 1980s, to chaos theory and nonlinear systems. In the 1990s, I learned about complex systems from conversations with Danny Hillis, the biologist Stuart Kauffman, the Nobel-laureate physicist Murray Gell-Mann, and others. Most recently, Hasslacher and the electrical engineer and device physicist Mark Reed have been giving me insight into the incredible possibilities of molecular electronics.  In my own work, as codesigner of three microprocessor architectures - SPARC, picoJava, and MAJC - and as the designer of several implementations thereof, I've been afforded a deep and firsthand acquaintance with Moore's law. For decades, Moore's law has correctly predicted the exponential rate of improvement of semiconductor technology. Until last year I believed that the rate of advances predicted by Moore's law might continue only until roughly 2010, when some physical limits would begin to be reached. It was not obvious to me that a new technology would arrive in time to keep performance advancing smoothly.  But because of the recent rapid and radical progress in molecular electronics - where individual atoms and molecules replace lithographically drawn transistors - and related nanoscale technologies, we should be able to meet or exceed the Moore's law rate of progress for another 30 years. By 2030, we are likely to be able to build machines, in quantity, a million times as powerful as the personal computers of today - sufficient to implement the dreams of Kurzweil and Moravec.  As this enormous computing power is combined with the manipulative advances of the physical sciences and the new, deep understandings in genetics, enormous transformative power is being unleashed. These combinations open up the opportunity to completely redesign the world, for better or worse: The replicating and evolving processes that have been confined to the natural world are about to become realms of human endeavor.  In designing software and microprocessors, I have never had the feeling that I was designing an intelligent machine. The software and hardware is so fragile and the capabilities of the machine to "think" so clearly absent that, even as a possibility, this has always seemed very far in the future.  But now, with the prospect of human-level computing power in about 30 years, a new idea suggests itself: that I may be working to create tools which will enable the construction of the technology that may replace our species. How do I feel about this? Very uncomfortable. Having struggled my entire career to build reliable software systems, it seems to me more than likely that this future will not work out as well as some people may imagine. My personal experience suggests we tend to overestimate our design abilities.  Given the incredible power of these new technologies, shouldn't we be asking how we can best coexist with them? And if our own extinction is a likely, or even possible, outcome of our technological development, shouldn't we proceed with great caution?     The dream of robotics is, first, that intelligent machines can do our work for us, allowing us lives of leisure, restoring us to Eden. Yet in his history of such ideas,Darwin Among the Machines, George Dyson warns: "In the game of life and evolution there are three players at the table: human beings, nature, and machines. I am firmly on the side of nature. But nature, I suspect, is on the side of the machines." As we have seen, Moravec agrees, believing we may well not survive the encounter with the superior robot species.  How soon could such an intelligent robot be built? The coming advances in computing power seem to make it possible by 2030. And once an intelligent robot exists, it is only a small step to a robot species - to an intelligent robot that can make evolved copies of itself.  A second dream of robotics is that we will gradually replace ourselves with our robotic technology, achieving near immortality by downloading our consciousnesses; it is this process that Danny Hillis thinks we will gradually get used to and that Ray Kurzweil elegantly details inThe Age of Spiritual Machines. (We are beginning to see intimations of this in the implantation of computer devices into the human body, as illustrated on thecover ofWired 8.02.)  But if we are downloaded into our technology, what are the chances that we will thereafter be ourselves or even human? It seems to me far more likely that a robotic existence would not be like a human one in any sense that we understand, that the robots would in no sense be our children, that on this path our humanity may well be lost.  Genetic engineering promises to revolutionize agriculture by increasing crop yields while reducing the use of pesticides; to create tens of thousands of novel species of bacteria, plants, viruses, and animals; to replace reproduction, or supplement it, with cloning; to create cures for many diseases, increasing our life span and our quality of life; and much, much more. We now know with certainty that these profound changes in the biological sciences are imminent and will challenge all our notions of what life is.  Technologies such as human cloning have in particular raised our awareness of the profound ethical and moral issues we face. If, for example, we were to reengineer ourselves into several separate and unequal species using the power of genetic engineering, then we would threaten the notion of equality that is the very cornerstone of our democracy.  Given the incredible power of genetic engineering, it's no surprise that there are significant safety issues in its use. My friend Amory Lovins recently cowrote, along with Hunter Lovins, an editorial that provides an ecological view of some of these dangers. Among their concerns: that "the new botany aligns the development of plants with their economic, not evolutionary, success." (See "A Tale of Two Botanies," page 247.) Amory's long career has been focused on energy and resource efficiency by taking a whole-system view of human-made systems; such a whole-system view often finds simple, smart solutions to otherwise seemingly difficult problems, and is usefully applied here as well.  After reading the Lovins' editorial, I saw an op-ed by Gregg Easterbrook inThe New York Times (November 19, 1999) about genetically engineered crops, under the headline: "Food for the Future: Someday, rice will have built-in vitamin A. Unless the Luddites win."  Are Amory and Hunter Lovins Luddites? Certainly not. I believe we all would agree that golden rice, with its built-in vitamin A, is probably a good thing, if developed with proper care and respect for the likely dangers in moving genes across species boundaries.  Awareness of the dangers inherent in genetic engineering is beginning to grow, as reflected in the Lovins' editorial. The general public is aware of, and uneasy about, genetically modified foods, and seems to be rejecting the notion that such foods should be permitted to be unlabeled.  But genetic engineering technology is already very far along. As the Lovins note, the USDA has already approved about 50 genetically engineered crops for unlimited release; more than half of the world's soybeans and a third of its corn now contain genes spliced in from other forms of life.  While there are many important issues here, my own major concern with genetic engineering is narrower: that it gives the power - whether militarily, accidentally, or in a deliberate terrorist act - to create a White Plague.  The many wonders of nanotechnology were first imagined by the Nobel-laureate physicist Richard Feynman in a speech he gave in 1959, subsequently published under the title "There's Plenty of Room at the Bottom." The book that made a big impression on me, in the mid-'80s, was Eric Drexler'sEngines of Creation, in which he described beautifully how manipulation of matter at the atomic level could create a utopian future of abundance, where just about everything could be made cheaply, and almost any imaginable disease or physical problem could be solved using nanotechnology and artificial intelligences.  A subsequent book,Unbounding the Future: The Nanotechnology Revolution, which Drexler cowrote, imagines some of the changes that might take place in a world where we had molecular-level "assemblers." Assemblers could make possible incredibly low-cost solar power, cures for cancer and the common cold by augmentation of the human immune system, essentially complete cleanup of the environment, incredibly inexpensive pocket supercomputers - in fact, any product would be manufacturable by assemblers at a cost no greater than that of wood - spaceflight more accessible than transoceanic travel today, and restoration of extinct species.  I remember feeling good about nanotechnology after readingEngines of Creation. As a technologist, it gave me a sense of calm - that is, nanotechnology showed us that incredible progress was possible, and indeed perhaps inevitable. If nanotechnology was our future, then I didn't feel pressed to solve so many problems in the present. I would get to Drexler's utopian future in due time; I might as well enjoy life more in the here and now. It didn't make sense, given his vision, to stay up all night, all the time.  Drexler's vision also led to a lot of good fun. I would occasionally get to describe the wonders of nanotechnology to others who had not heard of it. After teasing them with all the things Drexler described I would give a homework assignment of my own: "Use nanotechnology to create a vampire; for extra credit create an antidote."  With these wonders came clear dangers, of which I was acutely aware. As I said at a nanotechnology conference in 1989, "We can't simply do our science and not worry about these ethical issues."5 But my subsequent conversations with physicists convinced me that nanotechnology might not even work - or, at least, it wouldn't work anytime soon. Shortly thereafter I moved to Colorado, to a skunk works I had set up, and the focus of my work shifted to software for the Internet, specifically on ideas that became Java and Jini.  Then, last summer, Brosl Hasslacher told me that nanoscale molecular electronics was now practical. This wasnew news, at least to me, and I think to many people - and it radically changed my opinion about nanotechnology. It sent me back toEngines of Creation. Rereading Drexler's work after more than 10 years, I was dismayed to realize how little I had remembered of its lengthy section called "Dangers and Hopes," including a discussion of how nanotechnologies can become "engines of destruction." Indeed, in my rereading of this cautionary material today, I am struck by how naive some of Drexler's safeguard proposals seem, and how much greater I judge the dangers to be now than even he seemed to then. (Having anticipated and described many technical and political problems with nanotechnology, Drexler started the Foresight Institute in the late 1980s "to help prepare society for anticipated advanced technologies" - most important, nanotechnology.)  The enabling breakthrough to assemblers seems quite likely within the next 20 years. Molecular electronics - the new subfield of nanotechnology where individual molecules are circuit elements - should mature quickly and become enormously lucrative within this decade, causing a large incremental investment in all nanotechnologies.  Unfortunately, as with nuclear technology, it is far easier to create destructive uses for nanotechnology than constructive ones. Nanotechnology has clear military and terrorist uses, and you need not be suicidal to release a massively destructive nanotechnological device - such devices can be built to be selectively destructive, affecting, for example, only a certain geographical area or a group of people who are genetically distinct.  An immediate consequence of the Faustian bargain in obtaining the great power of nanotechnology is that we run a grave risk - the risk that we might destroy the biosphere on which all life depends.  As Drexler explained:  "Plants" with "leaves" no more efficient than today's solar cells could out-compete real plants, crowding the biosphere with an inedible foliage. Tough omnivorous "bacteria" could out-compete real bacteria: They could spread like blowing pollen, replicate swiftly, and reduce the biosphere to dust in a matter of days. Dangerous replicators could easily be too tough, small, and rapidly spreading to stop - at least if we make no preparation. We have trouble enough controlling viruses and fruit flies.   Among the cognoscenti of nanotechnology, this threat has become known as the "gray goo problem." Though masses of uncontrolled replicators need not be gray or gooey, the term "gray goo" emphasizes that replicators able to obliterate life might be less inspiring than a single species of crabgrass. They might be superior in an evolutionary sense, but this need not make them valuable.   The gray goo threat makes one thing perfectly clear: We cannot afford certain kinds of accidents with replicating assemblers.    Gray goo would surely be a depressing ending to our human adventure on Earth, far worse than mere fire or ice, and one that could stem from a simple laboratory accident.6 Oops.     It is most of all the power of destructive self-replication in genetics, nanotechnology, and robotics (GNR) that should give us pause. Self-replication is the modus operandi of genetic engineering, which uses the machinery of the cell to replicate its designs, and the prime danger underlying gray goo in nanotechnology. Stories of run-amok robots like the Borg, replicating or mutating to escape from the ethical constraints imposed on them by their creators, are well established in our science fiction books and movies. It is even possible that self-replication may be more fundamental than we thought, and hence harder - or even impossible - to control. A recent article by Stuart Kauffman inNature titled "Self-Replication: Even Peptides Do It" discusses the discovery that a 32-amino-acid peptide can "autocatalyse its own synthesis." We don't know how widespread this ability is, but Kauffman notes that it may hint at "a route to self-reproducing molecular systems on a basis far wider than Watson-Crick base-pairing."7  In truth, we have had in hand for years clear warnings of the dangers inherent in widespread knowledge of GNR technologies - of the possibility of knowledge alone enabling mass destruction. But these warnings haven't been widely publicized; the public discussions have been clearly inadequate. There is no profit in publicizing the dangers.  The nuclear, biological, and chemical (NBC) technologies used in 20th-century weapons of mass destruction were and are largely military, developed in government laboratories. In sharp contrast, the 21st-century GNR technologies have clear commercial uses and are being developed almost exclusively by corporate enterprises. In this age of triumphant commercialism, technology - with science as its handmaiden - is delivering a series of almost magical inventions that are the most phenomenally lucrative ever seen. We are aggressively pursuing the promises of these new technologies within the now-unchallenged system of global capitalism and its manifold financial incentives and competitive pressures.  This is the first moment in the history of our planet when any species, by its own voluntary actions, has become a danger to itself - as well as to vast numbers of others.   It might be a familiar progression, transpiring on many worlds - a planet, newly formed, placidly revolves around its star; life slowly forms; a kaleidoscopic procession of creatures evolves; intelligence emerges which, at least up to a point, confers enormous survival value; and then technology is invented. It dawns on them that there are such things as laws of Nature, that these laws can be revealed by experiment, and that knowledge of these laws can be made both to save and to take lives, both on unprecedented scales. Science, they recognize, grants immense powers. In a flash, they create world-altering contrivances. Some planetary civilizations see their way through, place limits on what may and what must not be done, and safely pass through the time of perils. Others, not so lucky or so prudent, perish.    That is Carl Sagan, writing in 1994, inPale Blue Dot, a book describing his vision of the human future in space. I am only now realizing how deep his insight was, and how sorely I miss, and will miss, his voice. For all its eloquence, Sagan's contribution was not least that of simple common sense - an attribute that, along with humility, many of the leading advocates of the 21st-century technologies seem to lack.  I remember from my childhood that my grandmother was strongly against the overuse of antibiotics. She had worked since before the first World War as a nurse and had a commonsense attitude that taking antibiotics, unless they were absolutely necessary, was bad for you.  It is not that she was an enemy of progress. She saw much progress in an almost 70-year nursing career; my grandfather, a diabetic, benefited greatly from the improved treatments that became available in his lifetime. But she, like many levelheaded people, would probably think it greatly arrogant for us, now, to be designing a robotic "replacement species," when we obviously have so much trouble making relatively simple things work, and so much trouble managing - or even understanding - ourselves.  I realize now that she had an awareness of the nature of the order of life, and of the necessity of living with and respecting that order. With this respect comes a necessary humility that we, with our early-21st-century chutzpah, lack at our peril. The commonsense view, grounded in this respect, is often right, in advance of the scientific evidence. The clear fragility and inefficiencies of the human-made systems we have built should give us all pause; the fragility of the systems I have worked on certainly humbles me.  We should have learned a lesson from the making of the first atomic bomb and the resulting arms race. We didn't do well then, and the parallels to our current situation are troubling.  The effort to build the first atomic bomb was led by the brilliant physicist J. Robert Oppenheimer. Oppenheimer was not naturally interested in politics but became painfully aware of what he perceived as the grave threat to Western civilization from the Third Reich, a threat surely grave because of the possibility that Hitler might obtain nuclear weapons. Energized by this concern, he brought his strong intellect, passion for physics, and charismatic leadership skills to Los Alamos and led a rapid and successful effort by an incredible collection of great minds to quickly invent the bomb.  What is striking is how this effort continued so naturally after the initial impetus was removed. In a meeting shortly after V-E Day with some physicists who felt that perhaps the effort should stop, Oppenheimer argued to continue. His stated reason seems a bit strange: not because of the fear of large casualties from an invasion of Japan, but because the United Nations, which was soon to be formed, should have foreknowledge of atomic weapons. A more likely reason the project continued is the momentum that had built up - the first atomic test, Trinity, was nearly at hand.  We know that in preparing this first atomic test the physicists proceeded despite a large number of possible dangers. They were initially worried, based on a calculation by Edward Teller, that an atomic explosion might set fire to the atmosphere. A revised calculation reduced the danger of destroying the world to a three-in-a-million chance. (Teller says he was later able to dismiss the prospect of atmospheric ignition entirely.) Oppenheimer, though, was sufficiently concerned about the result of Trinity that he arranged for a possible evacuation of the southwest part of the state of New Mexico. And, of course, there was the clear danger of starting a nuclear arms race.  Within a month of that first, successful test, two atomic bombs destroyed Hiroshima and Nagasaki. Some scientists had suggested that the bomb simply be demonstrated, rather than dropped on Japanese cities - saying that this would greatly improve the chances for arms control after the war - but to no avail. With the tragedy of Pearl Harbor still fresh in Americans' minds, it would have been very difficult for President Truman to order a demonstration of the weapons rather than use them as he did - the desire to quickly end the war and save the lives that would have been lost in any invasion of Japan was very strong. Yet the overriding truth was probably very simple: As the physicist Freeman Dyson later said, "The reason that it was dropped was just that nobody had the courage or the foresight to say no."  It's important to realize how shocked the physicists were in the aftermath of the bombing of Hiroshima, on August 6, 1945. They describe a series of waves of emotion: first, a sense of fulfillment that the bomb worked, then horror at all the people that had been killed, and then a convincing feeling that on no account should another bomb be dropped. Yet of course another bomb was dropped, on Nagasaki, only three days after the bombing of Hiroshima.  In November 1945, three months after the atomic bombings, Oppenheimer stood firmly behind the scientific attitude, saying, "It is not possible to be a scientist unless you believe that the knowledge of the world, and the power which this gives, is a thing which is of intrinsic value to humanity, and that you are using it to help in the spread of knowledge and are willing to take the consequences."  Oppenheimer went on to work, with others, on the Acheson-Lilienthal report, which, as Richard Rhodes says in his recent bookVisions of Technology, "found a way to prevent a clandestine nuclear arms race without resorting to armed world government"; their suggestion was a form of relinquishment of nuclear weapons work by nation-states to an international agency.  This proposal led to the Baruch Plan, which was submitted to the United Nations in June 1946 but never adopted (perhaps because, as Rhodes suggests, Bernard Baruch had "insisted on burdening the plan with conventional sanctions," thereby inevitably dooming it, even though it would "almost certainly have been rejected by Stalinist Russia anyway"). Other efforts to promote sensible steps toward internationalizing nuclear power to prevent an arms race ran afoul either of US politics and internal distrust, or distrust by the Soviets. The opportunity to avoid the arms race was lost, and very quickly.  Two years later, in 1948, Oppenheimer seemed to have reached another stage in his thinking, saying, "In some sort of crude sense which no vulgarity, no humor, no overstatement can quite extinguish, the physicists have known sin; and this is a knowledge they cannot lose."  In 1949, the Soviets exploded an atom bomb. By 1955, both the US and the Soviet Union had tested hydrogen bombs suitable for delivery by aircraft. And so the nuclear arms race began.  Nearly 20 years ago, in the documentaryThe Day After Trinity, Freeman Dyson summarized the scientific attitudes that brought us to the nuclear precipice:  "I have felt it myself. The glitter of nuclear weapons. It is irresistible if you come to them as a scientist. To feel it's there in your hands, to release this energy that fuels the stars, to let it do your bidding. To perform these miracles, to lift a million tons of rock into the sky. It is something that gives people an illusion of illimitable power, and it is, in some ways, responsible for all our troubles - this, what you might call technical arrogance, that overcomes people when they see what they can do with their minds."8  Now, as then, we are creators of new technologies and stars of the imagined future, driven - this time by great financial rewards and global competition - despite the clear dangers, hardly evaluating what it may be like to try to live in a world that is the realistic outcome of what we are creating and imagining.     In 1947,The Bulletin of the Atomic Scientists began putting a Doomsday Clock on its cover. For more than 50 years, it has shown an estimate of the relative nuclear danger we have faced, reflecting the changing international conditions. The hands on the clock have moved 15 times and today, standing at nine minutes to midnight, reflect continuing and real danger from nuclear weapons. The recent addition of India and Pakistan to the list of nuclear powers has increased the threat of failure of the nonproliferation goal, and this danger was reflected by moving the hands closer to midnight in 1998.  In our time, how much danger do we face, not just from nuclear weapons, but from all of these technologies? How high are the extinction risks?  The philosopher John Leslie has studied this question and concluded that the risk of human extinction is at least 30 percent,9 while Ray Kurzweil believes we have "a better than even chance of making it through," with the caveat that he has "always been accused of being an optimist." Not only are these estimates not encouraging, but they do not include the probability of many horrid outcomes that lie short of extinction.  Faced with such assessments, some serious people are already suggesting that we simply move beyond Earth as quickly as possible. We would colonize the galaxy using von Neumann probes, which hop from star system to star system, replicating as they go. This step will almost certainly be necessary 5 billion years from now (or sooner if our solar system is disastrously impacted by the impending collision of our galaxy with the Andromeda galaxy within the next 3 billion years), but if we take Kurzweil and Moravec at their word it might be necessary by the middle of this century.  What are the moral implications here? If we must move beyond Earth this quickly in order for the species to survive, who accepts the responsibility for the fate of those (most of us, after all) who are left behind? And even if we scatter to the stars, isn't it likely that we may take our problems with us or find, later, that they have followed us? The fate of our species on Earth and our fate in the galaxy seem inextricably linked.  Another idea is to erect a series of shields to defend against each of the dangerous technologies. The Strategic Defense Initiative, proposed by the Reagan administration, was an attempt to design such a shield against the threat of a nuclear attack from the Soviet Union. But as Arthur C. Clarke, who was privy to discussions about the project, observed: "Though it might be possible, at vast expense, to construct local defense systems that would 'only' let through a few percent of ballistic missiles, the much touted idea of a national umbrella was nonsense. Luis Alvarez, perhaps the greatest experimental physicist of this century, remarked to me that the advocates of such schemes were 'very bright guys with no common sense.'"  Clarke continued: "Looking into my often cloudy crystal ball, I suspect that a total defense might indeed be possible in a century or so. But the technology involved would produce, as a by-product, weapons so terrible that no one would bother with anything as primitive as ballistic missiles." 10  InEngines of Creation, Eric Drexler proposed that we build an active nanotechnological shield - a form of immune system for the biosphere - to defend against dangerous replicators of all kinds that might escape from laboratories or otherwise be maliciously created. But the shield he proposed would itself be extremely dangerous - nothing could prevent it from developing autoimmune problems and attacking the biosphere itself. 11  Similar difficulties apply to the construction of shields against robotics and genetic engineering. These technologies are too powerful to be shielded against in the time frame of interest; even if it were possible to implement defensive shields, the side effects of their development would be at least as dangerous as the technologies we are trying to protect against.  These possibilities are all thus either undesirable or unachievable or both. The only realistic alternative I see is relinquishment: to limit development of the technologies that are too dangerous, by limiting our pursuit of certain kinds of knowledge.  Yes, I know, knowledge is good, as is the search for new truths. We have been seeking knowledge since ancient times. Aristotle opened his Metaphysics with the simple statement: "All men by nature desire to know." We have, as a bedrock value in our society, long agreed on the value of open access to information, and recognize the problems that arise with attempts to restrict access to and development of knowledge. In recent times, we have come to revere scientific knowledge.  But despite the strong historical precedents, if open access to and unlimited development of knowledge henceforth puts us all in clear danger of extinction, then common sense demands that we reexamine even these basic, long-held beliefs.  It was Nietzsche who warned us, at the end of the 19th century, not only that God is dead but that "faith in science, which after all exists undeniably, cannot owe its origin to a calculus of utility; it must have originated in spite of the fact that the disutility and dangerousness of the 'will to truth,' of 'truth at any price' is proved to it constantly." It is this further danger that we now fully face - the consequences of our truth-seeking. The truth that science seeks can certainly be considered a dangerous substitute for God if it is likely to lead to our extinction.  If we could agree, as a species, what we wanted, where we were headed, and why, then we would make our future much less dangerous - then we might understand what we can and should relinquish. Otherwise, we can easily imagine an arms race developing over GNR technologies, as it did with the NBC technologies in the 20th century. This is perhaps the greatest risk, for once such a race begins, it's very hard to end it. This time - unlike during the Manhattan Project - we aren't in a war, facing an implacable enemy that is threatening our civilization; we are driven, instead, by our habits, our desires, our economic system, and our competitive need to know.  I believe that we all wish our course could be determined by our collective values, ethics, and morals. If we had gained more collective wisdom over the past few thousand years, then a dialogue to this end would be more practical, and the incredible powers we are about to unleash would not be nearly so troubling.  One would think we might be driven to such a dialogue by our instinct for self-preservation. Individuals clearly have this desire, yet as a species our behavior seems to be not in our favor. In dealing with the nuclear threat, we often spoke dishonestly to ourselves and to each other, thereby greatly increasing the risks. Whether this was politically motivated, or because we chose not to think ahead, or because when faced with such grave threats we acted irrationally out of fear, I do not know, but it does not bode well.  The new Pandora's boxes of genetics, nanotechnology, and robotics are almost open, yet we seem hardly to have noticed. Ideas can't be put back in a box; unlike uranium or plutonium, they don't need to be mined and refined, and they can be freely copied. Once they are out, they are out. Churchill remarked, in a famous left-handed compliment, that the American people and their leaders "invariably do the right thing, after they have examined every other alternative." In this case, however, we must act more presciently, as to do the right thing only at last may be to lose the chance to do it at all.     As Thoreau said, "We do not ride on the railroad; it rides upon us"; and this is what we must fight, in our time. The question is, indeed, Which is to be master? Will we survive our technologies?  We are being propelled into this new century with no plan, no control, no brakes. Have we already gone too far down the path to alter course? I don't believe so, but we aren't trying yet, and the last chance to assert control - the fail-safe point - is rapidly approaching. We have our first pet robots, as well as commercially available genetic engineering techniques, and our nanoscale techniques are advancing rapidly. While the development of these technologies proceeds through a number of steps, it isn't necessarily the case - as happened in the Manhattan Project and the Trinity test - that the last step in proving a technology is large and hard. The breakthrough to wild self-replication in robotics, genetic engineering, or nanotechnology could come suddenly, reprising the surprise we felt when we learned of the cloning of a mammal.  And yet I believe we do have a strong and solid basis for hope. Our attempts to deal with weapons of mass destruction in the last century provide a shining example of relinquishment for us to consider: the unilateral US abandonment, without preconditions, of the development of biological weapons. This relinquishment stemmed from the realization that while it would take an enormous effort to create these terrible weapons, they could from then on easily be duplicated and fall into the hands of rogue nations or terrorist groups.  The clear conclusion was that we would create additional threats to ourselves by pursuing these weapons, and that we would be more secure if we did not pursue them. We have embodied our relinquishment of biological and chemical weapons in the 1972 Biological Weapons Convention (BWC) and the 1993 Chemical Weapons Convention (CWC).12  As for the continuing sizable threat from nuclear weapons, which we have lived with now for more than 50 years, the US Senate's recent rejection of the Comprehensive Test Ban Treaty makes it clear relinquishing nuclear weapons will not be politically easy. But we have a unique opportunity, with the end of the Cold War, to avert a multipolar arms race. Building on the BWC and CWC relinquishments, successful abolition of nuclear weapons could help us build toward a habit of relinquishing dangerous technologies. (Actually, by getting rid of all but 100 nuclear weapons worldwide - roughly the total destructive power of World War II and a considerably easier task - we could eliminate this extinction threat. 13)  Verifying relinquishment will be a difficult problem, but not an unsolvable one. We are fortunate to have already done a lot of relevant work in the context of the BWC and other treaties. Our major task will be to apply this to technologies that are naturally much more commercial than military. The substantial need here is for transparency, as difficulty of verification is directly proportional to the difficulty of distinguishing relinquished from legitimate activities.  I frankly believe that the situation in 1945 was simpler than the one we now face: The nuclear technologies were reasonably separable into commercial and military uses, and monitoring was aided by the nature of atomic tests and the ease with which radioactivity could be measured. Research on military applications could be performed at national laboratories such as Los Alamos, with the results kept secret as long as possible.  The GNR technologies do not divide clearly into commercial and military uses; given their potential in the market, it's hard to imagine pursuing them only in national laboratories. With their widespread commercial pursuit, enforcing relinquishment will require a verification regime similar to that for biological weapons, but on an unprecedented scale. This, inevitably, will raise tensions between our individual privacy and desire for proprietary information, and the need for verification to protect us all. We will undoubtedly encounter strong resistance to this loss of privacy and freedom of action.  Verifying the relinquishment of certain GNR technologies will have to occur in cyberspace as well as at physical facilities. The critical issue will be to make the necessary transparency acceptable in a world of proprietary information, presumably by providing new forms of protection for intellectual property.  Verifying compliance will also require that scientists and engineers adopt a strong code of ethical conduct, resembling the Hippocratic oath, and that they have the courage to whistleblow as necessary, even at high personal cost. This would answer the call - 50 years after Hiroshima - by the Nobel laureate Hans Bethe, one of the most senior of the surviving members of the Manhattan Project, that all scientists "cease and desist from work creating, developing, improving, and manufacturing nuclear weapons and other weapons of potential mass destruction."14 In the 21st century, this requires vigilance and personal responsibility by those who would work on both NBC and GNR technologies to avoid implementing weapons of mass destruction and knowledge-enabled mass destruction.     Thoreau also said that we will be "rich in proportion to the number of things which we can afford to let alone." We each seek to be happy, but it would seem worthwhile to question whether we need to take such a high risk of total destruction to gain yet more knowledge and yet more things; common sense says that there is a limit to our material needs - and that certain knowledge is too dangerous and is best forgone.  Neither should we pursue near immortality without considering the costs, without considering the commensurate increase in the risk of extinction. Immortality, while perhaps the original, is certainly not the only possible utopian dream.  I recently had the good fortune to meet the distinguished author and scholar Jacques Attali, whose bookLignes d'horizons (Millennium, in the English translation) helped inspire the Java and Jini approach to the coming age of pervasive computing, as previously described in this magazine. In his new bookFraternités, Attali describes how our dreams of utopia have changed over time:  "At the dawn of societies, men saw their passage on Earth as nothing more than a labyrinth of pain, at the end of which stood a door leading, via their death, to the company of gods and toEternity. With the Hebrews and then the Greeks, some men dared free themselves from theological demands and dream of an ideal City whereLiberty would flourish. Others, noting the evolution of the market society, understood that the liberty of some would entail the alienation of others, and they soughtEquality."  Jacques helped me understand how these three different utopian goals exist in tension in our society today. He goes on to describe a fourth utopia,Fraternity, whose foundation is altruism. Fraternity alone associates individual happiness with the happiness of others, affording the promise of self-sustainment.  This crystallized for me my problem with Kurzweil's dream. A technological approach to Eternity - near immortality through robotics - may not be the most desirable utopia, and its pursuit brings clear dangers. Maybe we should rethink our utopian choices.  Where can we look for a new ethical basis to set our course? I have found the ideas in the book Ethics for the New Millennium, by the Dalai Lama, to be very helpful. As is perhaps well known but little heeded, the Dalai Lama argues that the most important thing is for us to conduct our lives with love and compassion for others, and that our societies need to develop a stronger notion of universal responsibility and of our interdependency; he proposes a standard of positive ethical conduct for individuals and societies that seems consonant with Attali's Fraternity utopia.  The Dalai Lama further argues that we must understand what it is that makes people happy, and acknowledge the strong evidence that neither material progress nor the pursuit of the power of knowledge is the key - that there are limits to what science and the scientific pursuit alone can do.  Our Western notion of happiness seems to come from the Greeks, who defined it as "the exercise of vital powers along lines of excellence in a life affording them scope." 15  Clearly, we need to find meaningful challenges and sufficient scope in our lives if we are to be happy in whatever is to come. But I believe we must find alternative outlets for our creative forces, beyond the culture of perpetual economic growth; this growth has largely been a blessing for several hundred years, but it has not brought us unalloyed happiness, and we must now choose between the pursuit of unrestricted and undirected growth through science and technology and the clear accompanying dangers.     It is now more than a year since my first encounter with Ray Kurzweil and John Searle. I see around me cause for hope in the voices for caution and relinquishment and in those people I have discovered who are as concerned as I am about our current predicament. I feel, too, a deepened sense of personal responsibility - not for the work I have already done, but for the work that I might yet do, at the confluence of the sciences.  But many other people who know about the dangers still seem strangely silent. When pressed, they trot out the "this is nothing new" riposte - as if awareness of what could happen is response enough. They tell me, There are universities filled with bioethicists who study this stuff all day long. They say, All this has been written about before, and by experts. They complain, Your worries and your arguments are already old hat.  I don't know where these people hide their fear. As an architect of complex systems I enter this arena as a generalist. But should this diminish my concerns? I am aware of how much has been written about, talked about, and lectured about so authoritatively. But does this mean it has reached people? Does this mean we can discount the dangers before us?  Knowing is not a rationale for not acting. Can we doubt that knowledge has become a weapon we wield against ourselves?  The experiences of the atomic scientists clearly show the need to take personal responsibility, the danger that things will move too fast, and the way in which a process can take on a life of its own. We can, as they did, create insurmountable problems in almost no time flat. We must do more thinking up front if we are not to be similarly surprised and shocked by the consequences of our inventions.  My continuing professional work is on improving the reliability of software. Software is a tool, and as a toolbuilder I must struggle with the uses to which the tools I make are put. I have always believed that making software more reliable, given its many uses, will make the world a safer and better place; if I were to come to believe the opposite, then I would be morally obligated to stop this work. I can now imagine such a day may come.  This all leaves me not angry but at least a bit melancholic. Henceforth, for me, progress will be somewhat bittersweet.     Do you remember the beautiful penultimate scene in Manhattan where Woody Allen is lying on his couch and talking into a tape recorder? He is writing a short story about people who are creating unnecessary, neurotic problems for themselves, because it keeps them from dealing with more unsolvable, terrifying problems about the universe.  He leads himself to the question, "Why is life worth living?" and to consider what makes it worthwhile for him: Groucho Marx, Willie Mays, the second movement of the Jupiter Symphony, Louis Armstrong's recording of "Potato Head Blues," Swedish movies, Flaubert's Sentimental Education, Marlon Brando, Frank Sinatra, the apples and pears by Cézanne, the crabs at Sam Wo's, and, finally, the showstopper: his love Tracy's face.  Each of us has our precious things, and as we care for them we locate the essence of our humanity. In the end, it is because of our great capacity for caring that I remain optimistic we will confront the dangerous issues now before us.  My immediate hope is to participate in a much larger discussion of the issues raised here, with people from many different backgrounds, in settings not predisposed to fear or favor technology for its own sake.  As a start, I have twice raised many of these issues at events sponsored by the Aspen Institute and have separately proposed that the American Academy of Arts and Sciences take them up as an extension of its work with the Pugwash Conferences. (These have been held since 1957 to discuss arms control, especially of nuclear weapons, and to formulate workable policies.)  It's unfortunate that the Pugwash meetings started only well after the nuclear genie was out of the bottle - roughly 15 years too late. We are also getting a belated start on seriously addressing the issues around 21st-century technologies - the prevention of knowledge-enabled mass destruction - and further delay seems unacceptable.  So I'm still searching; there are many more things to learn. Whether we are to succeed or fail, to survive or fall victim to these technologies, is not yet decided. I'm up late again - it's almost 6 am. I'm trying to imagine some better answers, to break the spell and free them from the stone.    --------------------------------------------------------------------------------    1 The passage Kurzweil quotes is from Kaczynski's Unabomber Manifesto, which was published jointly, under duress, byThe New York Times and The Washington Post to attempt to bring his campaign of terror to an end. I agree with David Gelernter, who said about their decision:   "It was a tough call for the newspapers. To say yes would be giving in to terrorism, and for all they knew he was lying anyway. On the other hand, to say yes might stop the killing. There was also a chance that someone would read the tract and get a hunch about the author; and that is exactly what happened. The suspect's brother read it, and it rang a bell.   "I would have told them not to publish. I'm glad they didn't ask me. I guess."   (Drawing Life: Surviving the Unabomber. Free Press, 1997: 120.)     2 Garrett, Laurie.The Coming Plague: Newly Emerging Diseases in a World Out of Balance. Penguin, 1994: 47-52, 414, 419, 452.     3 Isaac Asimov described what became the most famous view of ethical rules for robot behavior in his bookI, Robot in 1950, in his Three Laws of Robotics: 1. A robot may not injure a human being, or, through inaction, allow a human being to come to harm. 2. A robot must obey the orders given it by human beings, except where such orders would conflict with the First Law. 3. A robot must protect its own existence, as long as such protection does not conflict with the First or Second Law.     4 Michelangelo wrote a sonnet that begins:   Non ha l' ottimo artista alcun concetto  Ch' un marmo solo in sè non circonscriva  Col suo soverchio; e solo a quello arriva  La man che ubbidisce all' intelleto.   Stone translates this as:   The best of artists hath no thought to show  which the rough stone in its superfluous shell  doth not include; to break the marble spell  is all the hand that serves the brain can do.   Stone describes the process: "He was not working from his drawings or clay models; they had all been put away. He was carving from the images in his mind. His eyes and hands knew where every line, curve, mass must emerge, and at what depth in the heart of the stone to create the low relief."   (The Agony and the Ecstasy. Doubleday, 1961: 6, 144.)     5 First Foresight Conference on Nanotechnology in October 1989, a talk titled "The Future of Computation." Published in Crandall, B. C. and James Lewis, editors.Nanotechnology: Research and Perspectives. MIT Press, 1992: 269. See alsowww.foresight.org/Conferences/MNT01/Nano1.html.     6 In his 1963 novelCat's Cradle, Kurt Vonnegut imagined a gray-goo-like accident where a form of ice called ice-nine, which becomes solid at a much higher temperature, freezes the oceans.   7 Kauffman, Stuart. "Self-replication: Even Peptides Do It." Nature, 382, August 8, 1996: 496. Seewww.santafe.edu/sfi/People/kauffman/sak-peptides.html.     8 Else, Jon.The Day After Trinity: J. Robert Oppenheimer and The Atomic Bomb (available at www.pyramiddirect.com).     9 This estimate is in Leslie's bookThe End of the World: The Science and Ethics of Human Extinction, where he notes that the probability of extinction is substantially higher if we accept Brandon Carter's Doomsday Argument, which is, briefly, that "we ought to have some reluctance to believe that we are very exceptionally early, for instance in the earliest 0.001 percent, among all humans who will ever have lived. This would be some reason for thinking that humankind will not survive for many more centuries, let alone colonize the galaxy. Carter's doomsday argument doesn't generate any risk estimates just by itself. It is an argument forrevising the estimates which we generate when we consider various possible dangers." (Routledge, 1996: 1, 3, 145.)     10 Clarke, Arthur C. "Presidents, Experts, and Asteroids."Science, June 5, 1998. Reprinted as "Science and Society" inGreetings, Carbon-Based Bipeds! Collected Essays, 1934-1998. St. Martin's Press, 1999: 526.     11 And, as David Forrest suggests in his paper "Regulating Nanotechnology Development," available atwww.foresight.org/NanoRev/Forrest1989.html, "If we used strict liability as an alternative to regulation it would be impossible for any developer to internalize the cost of the risk (destruction of the biosphere), so theoretically the activity of developing nanotechnology should never be undertaken." Forrest's analysis leaves us with only government regulation to protect us - not a comforting thought.     12 Meselson, Matthew. "The Problem of Biological Weapons." Presentation to the 1,818th Stated Meeting of the American Academy of Arts and Sciences, January 13, 1999. (minerva.amacad.org/archive/bulletin4.htm)     13 Doty, Paul. "The Forgotten Menace: Nuclear Weapons Stockpiles Still Represent the Biggest Threat to Civilization."Nature, 402, December 9, 1999: 583.     14 See also Hans Bethe's 1997 letter to President Clinton, at www.fas.org/bethecr.htm.     15 Hamilton, Edith.The Greek Way. W. W. Norton & Co., 1942: 35.     --------------------------------------------------------------------------------  Bill Joy, cofounder and Chief Scientist of Sun Microsystems, was cochair of the presidential commission on the future of IT research, and is coauthor ofThe Java Language Specification. His work on theJini pervasive computing technology was featured inWired 6.08.  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ It is same with your dreams and nightmares, you have to feed them to keep them alive.  John.F.Nash- A Beautiful Mind  
     

    
</post>

<date>01,June,2004</date>
<post>


       
       MIT's Opencourseware. - Thanking you.   This book is dedicated, in respect and admiration, to the spirit that lives in the computer.  ``I think that it's extraordinarily important that we in computer science keep fun in computing. When it started out, it was an awful lot of fun. Of course, the paying customers got shafted every now and then, and after a while we began to take their complaints seriously. We began to feel as if we really were responsible for the successful, error-free perfect use of these machines. I don't think we are. I think we're responsible for stretching them, setting them off in new directions, and keeping fun in the house. I hope the field of computer science never loses its sense of fun. Above all, I hope we don't become missionaries. Don't feel as if you're Bible salesmen. The world has too many of those already. What you know about computing other people will learn. Don't feel as if the key to successful computing is only in your hands. What's in your hands, I think and hope, is intelligence: the ability to see the machine as more than when you were first led up to it, that you can make it more.''  Alan J. Perlis (April 1, 1922-February 7, 1990) 
     

    
</post>

<date>01,June,2004</date>
<post>


       
       urlLink http://geocities.com/help_gayathri/  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      This discussion on BSD and the way it is different from Linux was good.  http://www.over-yonder.net/~fullermd/rants/bsd4linux/bsd4linux1.php  For anyone who has at anytime just got a look and feel of BSD would make a good read. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
        Time and again, we have felt the persona of Waugh urlLink  .   urlLink  .   urlLink  .     Good Bye to a Real Hero!     
     

    
</post>

<date>01,June,2004</date>
<post>


       
      ls -t will sort the files present in the directory  with timeline- recently accessed files first.  But, following  urlLink TUPE , I created a file called  -t  using  ed . I am try to ls that  -t  file. even tried  ls '-t' ls \-t but it is not working.   trying to find a way to get around the option -t and list the file  -t    Got the Answer from a friend     ls -- -t  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      January is named after a Roman god Janus, an appropriate personification of the start of the new year. This particular Roman god had two faces so that he could look ahead toward the future and back at the past at the same time.    and a computer geek would wish his mate as    May this New Year Bring to you -  Independence of Java  Power of Unix  Popularity of Windows  Extensibility of J2EE  Luxury of .Net  Efficiency of C  Ease of VB  Robustness of Oracle  Vision of UML  Simplicity of HTML  Style of Mac  Dexterity of Photoshop  Enormity of 3D Max  Vastness of Internet  Compactness of JPG  Richness of BMP  Coverage as Yahoo  Reach of Google  Prudence of Froogle  Security of Norton & McAfee  Intelligence of Unreal  Realism of Max Payne  Speed of NFS  Fun of RoadRash  Intelligence of Chessmaster  Impression of Quake3  And the goodness of all software that comes for free...
     

    
</post>

<date>01,June,2004</date>
<post>


       
      When posted this, many people came with worst for iTunes, OSDN personals,SCO,Longhorn. Segway transporter,Weapons of Mass distruction and list followed.  iTunes had no effect on me. so I would not rate it best. For me the Best Technological innovation came from google's many appls like deskbar..blogger and variety of functions in search. For Worst... the insufficiency in dealing with spammers in the Internet.   But enjoy this.. for a fella saying the worst Technology feature is OSDN personals  I dated that girl. She knew C, enjoyed D&D, swore by Emacs, and she was one of the most beautiful people I have ever known. I met her at a LUG meeting. After the meeting, I went up and started talking to her. We went out for drinks. A relationship developed. We were a couple in perfect harmony. We finished each other's #defines. But there were irreconcileable differences. She used KDE, I used Gnome. We fought over it. One day, it got so intense that we agreed it would be better if we parted. I haven't seen her in over a year, but I still think about her almost daily. So Crystal, if you are out there and you miss me, send me an email.  Mike  "Software is like sex, its better when its free." -- Attributed to Linus Torvalds  
     

    
</post>

<date>01,June,2004</date>
<post>


       
          The Book Execution was really good. Larry Bosidy says that you got to work hard at all the times. Do your Best and  Get the Best  in the prevailing circumstances.  Not just completing the Job,but the harvest can be done, go ahead and go further than just completing the Job. It provided good information about various companies and their strategies as well. Nice book to read. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      I presented a Tech Ed today at Office on Longhorn.  Had been hearing much about this. Interesting concepts indeed from Microsoft. Everyone now knows about WinFS, the Hype which it has created around.  Came to know that there will be Inbuilit CD/DVD burner software with OS. Sparkle a Flash Killer application. Integrated Search for Computer as well as Internet. 3D desktop. Lots of Jargons were there.  urlLink Longhorn at MSDN  and  urlLink Longhorn SDK Docs provided a good information. Also got a Monad Presentation which was delivered in the PDC. ( Monad is a ksh/sh Object Oriented Shell)  But look out for  urlLink Fritz Chip  - the Cyrptoprocessor , Hardware level security features coming up.    Hope this information does not serve any marketing purpose but rather interests you in the concepts.  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Found this new Atom thing and now my Blogs can be  viewed by Newsreaders as well.  The RSS is  http://usr.blogspot.com/atom.xml   I use a Webenabled Newsreader called  urlLink BlogLines , it is quite good.   urlLink Feedster  helped me to find my Atom RSS url :-)
     

    
</post>

<date>01,June,2004</date>
<post>


       
      #!/bin/sh function factorial() {         N=$1         A=1         while test $N -gt 0  ; do                 A = `expr $A '*' $N`                 N = `expr $N - 1`         done         echo $A }  factorial  It is giving me a message that unary operator is expected at line 6 which is while test $N - gt 0 ; do 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      This should be helpful in understanding this issue:  urlLink http://www.chzsoft.com.ar/855patch.html  - shall try it out and post the results later.  Had the following discussion, which helped me get to this.  --] You are now talking on #linux --- Topic for #linux is Linux discussion.  Read lwn.net.  cut-n-paste != code reuse --- Topic for #linux set by mdomsch_hm at Sat Jan 31 03:28:46 [senthil_or] Hi posted at linux-testers.[drm:i830_wait_ring] *ERROR* lockup --] JerryDinh (JerryDinh@33f0f94f.23fa4838.amer.dell.com) has joined #linux [mdomsch] senthil_or: saw it, no idea [mdomsch] I had something similar on [= RHL8 with my Latitude C400 [mdomsch] but was fixed with newer DRM and XFree in RHL9 [mdomsch] RHEL3 should be fine in that regard. [senthil_or] Nope, I am using RH3 but still getting it constantly [senthil_or] Matt,one more thing. This is another problem X11 , posted here http://snipurl.com/4ax0 [senthil_or] have got any pointers ? [mdomsch] Red Hat XFree86 SRPM? [mdomsch] They may well have something in there to address this which hasn't made it upstream yet. [senthil_or] the question at google groups ( usenet) related to Xfree86 latest failing when used with Freebsd but Fedora/RH worked fine [senthil_or] agp0: trying to bind to stolen memory was the error, unable to figure out how to resolve [mdomsch] yeah, I read it. [mdomsch] "stolen memory" is system RAM being used by the video adapter as video memory [mdomsch] some is reserved by the BIOS at boot time, but generally only 1MB [mdomsch] which isn't enough for most video modes [mdomsch] So the i830 X driver needs to "steal" more (16MB or more) [mebrown] 855patch.tar.gz [mdomsch] in the SRPM? [senthil_or] ok..i see [mebrown] i have D500. google for 855patch. it lets you set the amount of ram to use. [senthil_or] Yeah, got the link - It offers good explaination of whats happend. Shall Try that. Thanks Matt!! :) [mdomsch] thank mebrown
     

    
</post>

<date>01,June,2004</date>
<post>


       
      I read a article on MIT Technology review. It focussed on Indian Brain Drain from IITs. But for me It provided the stars in Indian Enterprenuers. Here is the whole story.   For years, it was talked about as India’s “Brain Drain.” Smart young students would take one of the most competitive entrance examinations in the world, get a bachelor's degree in engineering—and promptly go West.  Every year, out of the 250,000 students who appear for the entrance exams conducted by the seven Indian Institutes of Technology (located in Mumbai, Delhi, Kharagpur, Kanpur, Chennai, Guwahati, and Roorke), only 3,000 or so get selected. IIT professors estimate that about 25 to 30 percent graduates leave India right after the graduation. Many others gain some work experience before going abroad for a master's degree in either business or science, or on an H1B visa that enables them to work in the United States.  The result: IIT graduates are scattered across the high-tech world and can be seen aplenty in Silicon Valley and in organizations such as NASA, Microsoft, and IBM. The roster includes ace venture capitalist Vinod Khosla of Kleiner Perkins; Arun Netravali, former president of Lucent Technologies' Bell Labs; Rajat Gupta, former managing director of McKinsey; Gururaj Deshpande, founder of Sycamore Networks; Arun Sarin, CEO of Vodafone; Victor Menezes, senior vice chairman of Citigroup; and Raghuram Rajan, chief economist of the International Monetary Fund. Even the comic strip character Asok, from Dilbert, comes from IIT. No wonder Businessweek called IIT graduates one of the “hottest exports India has ever produced.”  That wasn't the plan, exactly. Established by India’s first Prime Minister, Jawaharlal Nehru, and initially modeled on MIT, the IITs were supposed to lead India into the modern age. Instead, however, there has been an outflow of students to greener pastures (read: dollar salaries).  Three years ago, in an effort to slow this exodus, IIT Bombay (it retains the name Bombay even though the city is now called Mumbai) set up an information technology incubator at the Kanwal Rekhi School of Information Technology, or KReSIT. The incubator is slowly building a culture of entrepreneurship by encouraging IIT's best and brightest to stay in India. The IIT connection ensures that these startups concentrate on high-value areas such as building intellectual property and products. This stands in contrast to the many Indian companies that focus on software services and business process outsourcing—the activities that are causing a U.S. backlash against India. The startups at the incubator work in a variety of technical fields, including routing technology, information security, robotics, business activity monitoring, electronic design automation, and decision support systems for financial institutions.  D. B. Phatak, the IIT Bombay professor who started KReSIT, says that the incubator was a fundamental part of the vision for KReSIT. “Capable people will move from low-opportunity areas to high-opportunity areas. Therefore we have to create opportunities in India and promote people who are young and ambitious.” As part of KReSIT’s charter of pursuing applied research, the incubator was set up to commercialize technologies and create a challenging environment that would encourage the IITians to stay in India.  The IIT Bombay incubator is the first one to be set up at the IITs and the seeds of entrepreneurship, are slowly taking root. The incubator takes a stake of three percent of the equity of the incubated companies. Of the 13 companies spawned at KReSIT, four have moved out of the incubator. Three of these companies have raised their first round of funding, and one is already self-sustaining.  Several government departments have stepped forward to support the initiative. As a result, the incubator is expanding from its tiny 28-square-meter area into a separate building with 900 square meters of space, and has raised approximately $1 million to fund its activities. Following IIT Bombay’s example, IIT Delhi and IIT Kanpur have set up their own business incubators and IIT Madras (located in Chennai) is in the process of setting one up. Despite the downturn that followed the dotcom bust, interest remains high, and many students vie for each place in the incubator.  While it is too early to draw conclusions, there are initial indications that the incubator has been successful in persuading IITians to take up entrepreneurship as a career option. Take, for example, Vishal Gupta. Once the entrepreneurship bug bit him, Gupta turned down full scholarships from Cornell and the University of Southern California; instead, he turned his thesis on rules-based event engines for fingerprinting into the core technology of a startup he launched at IIT Bombay called Herald Logic. The company develops software that enables companies to monitor key parameters of their business. For example, if a fund manager’s performance falls below that of the market index, Herald Logic’s software can trigger alerts to those responsible for running the organization.  The company was launched with $2,000 from Gupta’s father’s retirement money; it now sustains itself on its own revenues. Gupta says that the IIT Bombay incubator was a huge help in the initial cash-strapped days; the exchange of “war stories” with other incubated companies, he says, accelerated his learning process. Moreover, he adds, the IIT branding opens doors that would otherwise be closed to most startups and is an invaluable help in marketing the company and its products.  Another graduate who refused the blandishments of the corporate world is Reapan Tikoo, co-founder of Powai Labs, a startup that builds simulation accelerators for the electronic design automation industry. Tikoo, who earned a master's degree in management from IIT, was offered a high-paying job as a production executive in the entertainment industry. He opted instead to convert the business plan he wrote for his degree into a real-life enterprise.  The incubator would not have been possible in the past, Phatak says, due to cultural reasons. He cites "a change in the mind-set" in India toward a more entrepreneurial outlook: "wealth generation is considered good," Phatak says. "These conditions did not exist in India during the 1980s."  Kanwal Rekhi, the IIT Bombay benefactor after whom KReSIT is named, adds that the phenomenal growth of Indian information technology services companies such as Infosys and Wipro—which have come within striking distance of the billion-dollar mark by riding the outsourcing wave—has also made an impact on the Indian psyche. Entrepreneurship, especially in the technology industry, is now firmly established as a viable career option in India.  Both Rekhi and Phatak agree that the incubator has a long way to go before it replicates the entrepreneurial record of U.S universities like MIT and Stanford. However, several successful entrepreneurs and professionals who made it big in the west are now helping out by contributing time and money. Anna Lee Saxenian, a professor of city and regional planning at the University of California, Berkeley who studied immigrant Indian and Chinese entrepreneurs in the high-tech industry, calls this phenomenon “brain circulation.”  Rekhi himself is the former CTO of Novell and founder of The Indus Entrepreneurs, an organization that promotes entrepreneurship. Other mentors to the incubator include Nandan Nilekani, CEO of Infosys, one of India’s most successful software companies; Rakesh Mathur, who sold the e-commerce company Junglee to Amazon.com; and Suhas Patil, co-founder of Cirrus Logic.   The IIT brand name may give its students a head start but obstacles still remain. The venture capital ecosystem in India is still nascent. Indeed, says Phatak, venture capital in India is more of a glorified loan rather than a true risk. “The nature of VC funding in India does not match the ethos of funding in the United States, where VCs are involved in mentoring, giving ideas, and helping hire CEOs and CTOs," he says. In India, he explains, venture capitalists "make you feel they are more interested in protecting their capital than in taking a true risk.” Gupta of Herald Logic says that building a product company in India is difficult because technology adoption within Indian companies is slow. “You are in the midst of laggards, trying to find early adopters,” he says.  Despite these challenges, R. K. Lagu, the IIT Bombay electrical engineering professor who is in charge of the incubator, says that there is great interest. “The incubator started as an IT incubator but now faculty and students from other disciplines have also become interested,” says Lagu. He adds that initially, most business plans were from final year undergraduate students but in the last two cases it has been a faculty-student combination.  Powai Labs’ Tikoo sums up the attitude of IIT Bombay’s entrepreneurs when he says, “There was a time when India did exports through cheap labor, but that time has gone. You cannot build an Infosys today with any amount of capital. The next ten years will belong to technology R&D based product companies out of India.”  Tikoo adds that 20 years ago, the United States was the place to be for technological entrepreneurs. But now, he says, growth and investments are happening in India. “This is where the action is. It would be foolish to miss this opportunity by being out of India." 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      The URL is   urlLink http://puggy.symonds.net/~senthil/C/    
     

    
</post>

<date>01,June,2004</date>
<post>


       
        Let no one ever come to you,without leaving better and happier    Mother Teresa    urlLink      I had just read this book on Mother Teresa. The author is a friend of mother and has described in detail about mother,work and mission from the heart.    Mother Teresa lived with God.          Let me know,if you would like to read the book.                                                      
     

    
</post>

<date>01,June,2004</date>
<post>


       
      The following data structures are a complete list for almost all practical programs:      array     linked list     hash table     binary tree   Of course, you must also be prepared to collect these into compound data structures.  Rob Pike 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      char p[]={0x4F,0x20,0x52,0x20,0x53,0x65,0x6E,0x74,0x68,0x69,0x6C,0x20,0x4B,0x75,0x6D,0x61,0x72,0x61,0x6E,0x00}; used man ascii and learnt some...
     

    
</post>

<date>01,June,2004</date>
<post>


       
      BY contrast, Apple says it developed the iPod in just six months, faster than any major product in the company's history. The hand-held device, which contains more computing power than an early Macintosh, was put together starting in 2001 by hardware designers led by Tony Fadell, a young engineer who had worked at the Apple spinoff General Magic, at Philips Electronics and briefly at RealNetworks, led by Rob Glaser, who has developed the Rhapsody music service.  In the late 1990's, Mr. Fadell tried to start his own Silicon Valley company, Fuse, designing consumer electronics products, including some related to digital music. When Fuse failed to get financing, he went to Apple, first as a contractor in February 2001, and then in April that year as the senior director of the iPod and other special projects.  He would eventually build a 35-member team of engineers from Apple and other companies. Using a version of a microprocessor that powers most cellphones, the group brought the iPod together rapidly by relying on software licensed from a small start-up, Pixo, a cellphone software company founded by Paul Mercer, another former Apple engineer.   urlLink NYTimes Article  
     

    
</post>

<date>01,June,2004</date>
<post>


       
       lucubration  \loo-kyoo-BRAY-shun; loo-kuh-\, noun:   1.  The  act  of  studying  by  candlelight;  nocturnal study;   meditation.  2.  That which is composed by night; that which is produced by   meditation   in   retirement;  hence  (loosely)  any  literary   composition. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
          "The primary purpose of the DATA statement is to give names to constants; instead of referring to pi as 3.141592653589793 at every appearance, the variable PI can be given that value with a DATA statement and used instead of the longer form of the constant. This also simplifies modifying the program, should the value of pi change." - FORTRAN manual for Xerox computers   "Programming graphics in X is like finding sqrt(pi) using Roman numerals." - Henry Spencer   Real programmers are surprised when the odometers in their cars don't turn from 99,999 to A0000.    Any sufficiently advanced bug is indistinguishable from a feature.   COBOL programmers understand why women hate periods.   Computer interfaces and user interfaces are as different as night and 1.   If God had intended humans to program, we would be born with serial I/O ports.   There are two ways to write error-free programs; only the third one works.    
     

    
</post>

<date>01,June,2004</date>
<post>


       
      "Everyday life is like programming, I guess,If you love something you can put beauty into it."  urlLink Donald E Knuth      The art of Don E. Knuth "I've never been a good estimator of how long things are going to take," he says. Coming from someone who's been writing one book on and off for the past quarter-century, this seems a bit of an understatement. But when you consider that most of Knuth's work has been devoted to just that -- figuring out how much time things like computer programs take -- and the statement takes on new (and slightly disingenuous) meanings.  Read a good  urlLink Salon article .  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Thats a good amount $$$$ Hearing for the first time the actual prediction and don't know what might be the real. Need to check how much the other IPO's have gone for ( Netscape ( Famous for IPO), M$, Yahoo, Dell, IBM,Sun,hp ... etc). Any other I am missing with my limited knowledge!    
     

    
</post>

<date>01,June,2004</date>
<post>


       
                urlLink Koolio  is a traveling autonomous refrigerator robot -- Picture a cross between  R2D2 and a vending machine.  It is designed for use on the 3rd floor of Benton Hall at the University of Florida .  This floors houses the Machine Intelligence Lab (MIL) as well class rooms and professor’s offices. Around here professors are always working  diligently in their office and sometimes can’t even find the time to get up and get a drink.  Now they don’t have to worry.  Here is the sequence of event.    Dr. Nechyba is in an important meeting with a future client and has a sudden urge for a Diet Coke.  He logs on to the network and tells Koolio that he would like a Diet Coke.   Koolio receives this signal through its wireless card and determines the room number needed.  It leaves its docking station at the MIL lab and proceeds to the hallway.  It navigates the hall way with a verity of instrument  Sonar for accurate long distance coverage.  IR for close obstacle avoidance.  Shaft encoding for accurate navigation once location is determined.  Web cameras for reading room numbers off the wall.   It locates room 326 and delivers Dr. Nechyba his Diet Coke thus averting disaster.  It leaves the room and returns to its docking station to recharge and wait for the next call.     Although Koolio  is initially designed for Benton Hall it could be applied in an any office, school, or even your own home.  It is completely autonomous and has numerous sensors to accommodate  even the trickiest environments.  Imagine watching a movie with someone and you get thirsty.  It is an inconvenience to everyone watching the movie to stop the movie, but at the same time you don’t want to miss anything important.  Koolio could solve your problem.  Or say you are watching a boxing match with your buddies and need another beer.  Instead of missing that knock out punch Koolio could deliver you, and all your friends, beer all night long.  You have this luxury 24 hours a day, 7 days a week, 365 days a year.   Find a human to do that.  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      When the Software industry  had badly gone down, three giants Sun,> SCO(UNIX) and Microsoft started producing condoms and named them Java-condo, CondomiX , and MS-Condome  respectively.  A customer using Java-condo complained to Sun that the  condom doesn't fit correctly. Sun replied: "Wait till we get the ISO  standard". They boasted that it will fit to any size irrespective of  underlying structure.  Well, the customer switched to CondomiX and  found that by the time he finshes reading the instructions, given along with  CondomiX, his wife was sleeping and he himself forgetting why he is  using CondomiX.  Finally he swiched to MS-Condome.To his surprise  it was so good...and comfortable!. He used it happily. Six months  later he found that his wife was pregnant. He got angry and complained to  Micrsoft. He got his reply from  Microsoft:  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---  ---   ---  A  PATCH IS COMING SOON...!
     

    
</post>

<date>01,June,2004</date>
<post>


       
      From the Blogger page,was led to the sign up of gmail. I use myrealbox.com with usename ors for personal mails.  Now signed up  @gmail.com  with username= orsenthil  . You can contact me anywhere you like!
     

    
</post>

<date>01,June,2004</date>
<post>


       
      This seems to be  Old Programming School Practise. Came to know from  urlLink Ariya .    a ^=b; b ^=a; a ^=b;    In C Programming, ^ stands for Exclusive OR.  Lets expand this expression and denote  an  and  bn  for new values of a and b then,   bn = b xor ( a xor b) = b xor a xor b = a an = a xor ( b xor a) = a xor b xor a = b    
     

    
</post>

<date>01,June,2004</date>
<post>


       
       urlLink ESR 's HOWTO on  urlLink Unix and Internet Fundamentals  is very good. It was crisp and effective. Will give a good start for any beginner and also may bridge the gaps when you have a rough idea. This was not just about Unix and Internet, but about Computers, OS and all the basic things one needs to be aware of.  Read the  urlLink Linux Reading List HOWTO  also by the same author.   
     

    
</post>

<date>01,June,2004</date>
<post>


       
      "The most exciting phrase to hear in science, the one that heralds new discoveries, is not 'Eureka!' (I've found it!), but 'That's funny...'"    -Isaac Asimov.  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      A good acount of Tracing a Cracker and of Computer Security. Interestingly I had read a Crack version of it, I got from one of my friend.  How the Cuckoo Laid Its Egg.  The hacker had become a super-user. He was like a cuckoo bird. The cuckoo is a nesting parasite that lays her eggs in other birds' nests: some other bird will raise her young. The survival of cuckoo chicks depends on the ignorance of other species. Our mysterious visitor had laid an egg-program into our computer, letting the system hatch it and feed it privileges.  This is written by Clifford Stoll, who traced the Hacker Markus Hess, who logged in to most of the Top Security sites of America.
     

    
</post>

<date>01,June,2004</date>
<post>


       
      In brief  Hard Work  , and here' what Michael Dell says:       You don't have to be a genius,or a visionary,or even a college graduate to think unconventionally.You just need a framework and a dream.   Dell has developed its most critical competitive advantage:becoming a virtually integrated organization.   I also learned that if you've got a good idea,it pays to do something aboutit.   Sometimes it's better not to ask - or to listen - when people tell you something can't be done. I didn't ask for permission or approval. I just went ahead and did it.   Of course,performance and time to market are huge differentiators, and highlighted the efficiency of what we were doing.   We had to do something dramatically different.   1986     Its always been our nature to think about what is possible and what is achievable, and set stretch goals accordingly.   ...the goal was realistic. All we had to do was figure out how to achieve it.   "thinking unconventionally" and " not listening to people who tell you something can't be done"   The lesson is: Believe in what you're doing. If you've got an idea that's really powerful, you've just got to ignore the people who tell you it won't work, and hire people who embrace your vision.   It's fun to do things that people don't think are possible or likely. It's also exciting to achieve the unexpected.Our competitors didn't consider us a threat for a long time,providing us with even a greater oppurtunity to surprise them with our success.   Finally,we learned to be oppurtunistic.   We would soon be faced with challenges that would threaten the very existence of our company.   To our stunned disbelief, we had quickly become known as a company with the inventory problem.   We weren't then - and we certainly aren't now - in the business of convincing people to buy something they did'nt want,so we canceled Olympic before it ever saw the light of the day, and admitted we made a mistake.We had gone ahead and created a product that was,for all intents and purposes,technology for technology's sake,rather than technology for the customer's sake.   And it's incredibly gratifying to watch a brilliant technologist grow to understand all aspects of the way our business works.   It's not like there are sirens going off or people running through the hallways saying "You're growing too fast!Stop!" In fact,while it's happening,it seems to happen in slow motion.   We knew there was such a thing as growing too fast,but we also knew that if we did'nt do it,we might not live to tell about it.   We were making decisions based on emotions and opinions. In leadership, it's important to be intuitive,but not at the expense of facts.   Facts are your friend.   A company's success should always be defined by its strategy and its ideas - and it should not be limited by the abilities of the people who are running it.   Plan or Die   Data is the engine that keeps us on track.   I would rather overkill a great idea than underexploit it.   Even if something seems to be working, it can always be improved.   Today's leaders are voracious learners.   If something can be improved,someone will figure out a way to do it. No matter what your business, that someone had better be you.   It comes from being willing to follow our convictions.It comes from an innate fascination with eliminating unneccessary steps.     These were...Direct from Dell         
     

    
</post>

<date>01,June,2004</date>
<post>


       
      FOLDOC entry to edit: [Yuanqing Zhu [drinking201ATyahoo.com.cn]: clause]   This is a logic programming thing.  Needs a bit more explanation I think.  ------- Start of forwarded message ------- From: Yuanqing Zhu [drinking201ATyahoo.com.cn] To: Denis Howe [dbhATdoc.ic.ac.uk], Yuanqing Zhu [rinking201ATyahoo.com.cn] Subject: clause X-Host: X-Url: sendnew X-Browser: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0) Sender: Denis Howe   Date: Fri, 26 Dec 2003 07:31:53 +0000  clause  definition of clause:the disconjuction form of literals reference:Microsoft Computer Dictionary  ------- End of forwarded message -------  --  Denis Howe   Free On-Line Dictionary of Computing http://www.foldoc.org/   Need to Complete this by  7 th of May.  As before,  you are welcome for discussion on this topic.   
     

    
</post>

<date>01,June,2004</date>
<post>


       
      I was given a oppurtunity to  urlLink edit  a  urlLink FOLDOC  entry. I am happy to see it published now. You may enjoy reading the definition  about  urlLink 4GL .   Here is format I submitted sometime December last week of 2003.( It was during /on Christmas)  fourth generation language   (4GL) An "application specific" language, one with built-in knowledge of an application domain, similar to {SQL} having the built-in knowledge of database domain.  The term was invented by Jim Martin to refer to {non-procedural}{ high level languages} built around {database} systems. The fourth generation language (4GL) was designed to be very close to natural language to ease the programming process and with the idea that certain applications  could be generalised by adding limited programming ability to them.  The fourth generation language are called Report Generated languages, because when fed a description of data format and the report to generate,they turned that into a {COBOL} ( or another 3GL,which actually contained the commands to read and process the data) and placed the results on the page.  Some examples of 4GL are : {SQL}; {Focus}, {Metafont}, 	{PostScript}, {S}, {IDL-PV/WAVE}, {Gauss}, 	{Mathematica}; and {data-stream languages} such as {AVS}, 	{APE}, {Iris Explorer}.   Enjoy the Formal entry here:  urlLink 4GL  Thanks! BTW, FOLDOC really great and helping me improve my knowledge in Computer Science. Thanks Denis Howe!
     

    
</post>

<date>01,June,2004</date>
<post>


       
      After seeing the title  urlLink Sun and Microsoft Settle Litigation , I thought ( On 2 April), heck! these guys are still playing the April Fool Joke Around and Just passed by. Few minutes later returned, wondered whats happening?
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Thank you for contacting Google about our  urlLink Copernicus Research Center .  We've received an overwhelming response to this opportunity and are not currently accepting additional resumes. We will, however, keep your information on file should we have an opening in the future. At the current staffing levels, we anticipate that we may need additional applicants on or around April Fool's Day in 2104.  Until then, we appreciate your interest in Google and your taking the time to write us.   Sincerely,  The Googlunar Recruiting Team 
     

    
</post>

<date>01,June,2004</date>
<post>


       
       Josephson junction     A Josephson junction is a type of electronic circuit capable of switching at very high speeds when operated at temperatures approaching absolute zero. Named for the British physicist who designed it, a Josephson junction exploits the phenomenon of superconductivity, the ability of certain materials to conduct electric current with practically zero resistance. Josephson junctions are used in certain specialized instruments such as highly-sensitive microwave detectors, magnetometers, and QUIDs.  A Josephson junction is made up of two superconductors, separated by a nonsuperconducting layer so thin that electrons can cross through the insulating barrier. The flow of current between the superconductors in the absence of an applied voltage is called a Josephson current, and the movement of electrons across the barrier is known as Josephson tunneling. Two or more junctions joined by superconducting paths form what is called a Josephson interferometer.  While researching superconductivity, Brian David Josephson studied the properties of a junction between two superconductors. Following up on earlier work by Leo Esaki and Ivar Giaever, he demonstrated that in a situation when there is electron flow between two superconductors through an insulating layer (in the absence of an applied voltage), and a voltage is applied, the current stops flowing and oscillates at a high frequency.  The Josephson effect is influenced by magnetic fields in the vicinity, a capacity that enables the Josephson junction to be used in devices that measure extremely weak magnetic fields, such as superconducting quantum interference devices (SQUIDs). For their efforts, Josephson, Esaki, and Giaever shared the Nobel Prize for Physics in 1973.     A superfast switch used in many computers. It consists of a thin layer of insulating material sandwiched between layers of superconducting material.     Consists of two superconductors separated by a thin insulating barrier. Is in fast electronic switches or sensitive magnetometers.     As before, welcome for discussion.   So need to Submit by June 28 to  urlLink FOLDOC   (But,I guess I would be sooner this time, since... I would like to :) 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Read  urlLink this article  about Extensible programming for 21st century. It gives a programming model of the future, while analysing the so far existant programming models. this was a good read to know more abt these models. Will visit again some day.
     

    
</post>

<date>01,June,2004</date>
<post>


       
      My Sisters have their WebPages Up! Here is  urlLink Ashwini's  and here is  urlLink Vaishnavis . Lets see who goes to google first. :)
     

    
</post>

<date>01,June,2004</date>
<post>


       
      If the builders built building the way programmers wrote programs, then the first woodpecker to come along would destroy civilization.
     

    
</post>

<date>01,June,2004</date>
<post>


       
      I am still styding the C Programming Language Book. My aim is to complete the book ( with a good number of iterations) and solve all the problems in the book. At the end of it, I have to write a small editor. After this is completed, I have to give my RHCE exam.  For the 11th hour preparation for RHCE, I came across  urlLink thid link  today,posted in our internal groups. But,priority to the C Learning First.
     

    
</post>

<date>01,June,2004</date>
<post>


       
      in his characteristic way, has some interesting thing  urlLink here . And here is a  urlLink followup  equally amusing.
     

    
</post>

<date>01,June,2004</date>
<post>


       
        The W3C Workshop on Web Applications and Compound Documents -  urlLink Papers     urlLink DomainKeys    IETF-  urlLink INTERNET DRAFT    At May 22,00:11, I stand finished reading them. :) (IETF Draft, I quickly glanced through)
     

    
</post>

<date>01,June,2004</date>
<post>


       
        clause	         1.   A {logical formula} in {conjunctive normal form},          which has the {schema} 			 p1 ^ ...^ pm => q1 V ... V qn. 	or,equivalently 			~p1 V ... V ~pn V q1 V ... V qn, 	where pi and qi are {atoms}. 	The operators ~,^,V,=> are connectives,where ~ stands for a	                                     {negation},^ for a {conjunction},V for a {disjunction} and =>  	for an {implication}.  	2.   A part of a {SQL} statement that does not	constitute         a full statement; for e.g. a "WHERE clause". 	(2004-5-15)   Was sent to me on  urlLink Apr 7 , took a little extra time to edit.  Thanks  urlLink FOLDOC  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Now, this was stupid. I had not figured out how to do copy-paste in vim editor. At last now, I got it. Thanks to google and  urlLink Linux Novice  HOWTO Copy and Paste in VIM Editor. These Steps Assume you are in the Command Mode. Step 1: Move your Cursor to the Starting Point of the text tobe Copied . Step 2: Press  v . The Mode changes to Visual. Step 3: Move your Cursor now to the Ending point of the text tobe Copied. Step 4: Press  y   This will copy the text in the buffer and lead to the starting point again. Step 5: Move your Cursor to the position where you want to paste the text. Step 6: Press  p     YO!  
     

    
</post>

<date>01,June,2004</date>
<post>


       
          Last year,the folks at Dell ran for  urlLink Freedom Foundation . It was good.The same is being repeated this year.  Would you like to Participate?  For Freedom Foundation, it is like running and contributing money to thesuffering children.The participation fee is Rs150/-  Will get a T-Shirt and a Cap, but that150/- will go to the Freedom Foundation.   Details: Date : 6 June 2003  Time: 8:30 PMVenue: Police Parade Grounds, on MG Road. Distance: 5 KM Fee : Rs 150/Participant.   Do let me know if you are interested (by 14th May as I have to register the details.
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Most real innovation is done by crazy people doing crazy things. The keys are:   Learn all you can before you go adventuring.  Don't be afraid to make mistakes.  Only make new mistakes.  Keep your eyes open.  Don't just look straight ahead: develop your peripheral vision.  It's the things that go in unexpected directions are the most important.    urlLink James Gosling  in his  urlLink Java Blog 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Geek humor: If you take a close look at the form Google filed with the Securities and Exchange Commission, the exact value of its planned offering is $2,718,281,828 dollars, which some would immediately recognize as the mathematical constant  urlLink e .  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      -----Forwarded Message----- From: O.R.Senthil Kumaran   To: Denis Howe   Cc: charlesmarie88@hotmail.com Subject: Re: FOLDOC entry to edit: [Charles T. Marie  : Josephson device (i.e. Josephson junction)] Date: Mon, 28 Jun 2004 22:40:37 +0530  Josepshson Junction           A type of {electronic} {circuit} capable of 	switching at very high speeds when operated at temperatures 	approaching {absolute zero}.Its low power dissipation factor 	makes it useful in {high density computer circuits}.                                                                                                   (2004-06-28)  ---- Hi Denis,               Thanks again for providing me the oppurtunity to edit FOLDOC. I have edited the Josephson Junction entry submitted to me on May 28,2004.If you have any comments on it,kindly let me know.  I am ready to take up the next entry and contribute to FOLDOC.  Warm Regards, Senthil     On Fri, 2004-05-28 at 06:01, Denis Howe wrote: > ------- Start of forwarded message ------- > From: Charles T. Marie   > To: Denis Howe  , Charles T. Marie >       > Subject: Josephson device (i.e. Josephson junction) > X-Host: 66-187-167-179.internetprovidersinc.com > X-Url: > http://whatis.techtarget.com/definition/0,,sid9_gci815055,00.html > X-Browser: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; MSN 6.1; >     MSNbMSFT; MSNmen-us; MSNc00; v5m) > Sender: Denis Howe   > Date: Mon, 29 Dec 2003 20:05:03 +0000 >  > *** EOOH *** > From: Charles T. Marie   > To: Denis Howe  , Charles T. Marie >       > Subject: Josephson device (i.e. Josephson junction) > X-Host: 66-187-167-179.internetprovidersinc.com > X-Url: > http://whatis.techtarget.com/definition/0,,sid9_gci815055,00.html > X-Browser: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; MSN 6.1; >     MSNbMSFT; MSNmen-us; MSNc00; v5m) > Sender: Denis Howe   > Date: Mon, 29 Dec 2003 20:05:03 +0000 >  > Josephson device (i.e. Josephson junction) >  > Josephson junction (i.e. Josephson device - my comment) >  > A Josephson junction is a type of electronic circuit capable of > switching at very high speeds when operated at temperatures > approaching absolute zero. Named for the British physicist who > designed it, a Josephson junction exploits the phenomenon of > superconductivity, the ability of certain materials to conduct > electric current with practically zero resistance. Josephson junctions > are used in certain specialized instruments such as highly-sensitive > microwave detectors, magnetometers, and QUIDs.  A Josephson junction > is made up of two superconductors, separated by a nonsuperconducting > layer so thin that electrons can cross through the insulating > barrier. The flow of current between the superconductors in the > absence of an applied voltage is called a Josephson current, and the > movement of electrons across the barrier is known as Josephson > tunneling. Two or more junctions joined by superconducting paths form > what is called a Josephson interferometer. >  > While researching superconductivity, Brian David Josephson studied the > properties of a junction between two superconductors. Following up on > earlier work by Leo Esaki and Ivar Giaever, he demonstrated that in a > situation when there is electron flow between two superconductors > through an insulating layer (in the absence of an applied voltage), > and a voltage is applied, the current stops flowing and oscillates at > a high frequency. >  > The Josephson effect is influenced by magnetic fields in the vicinity, > a capacity that enables the Josephson junction to be used in devices > that measure extremely weak magnetic fields, such as superconducting > quantum interference devices (SQUIDs). For their efforts, Josephson, > Esaki, and Giaever shared the Nobel Prize for Physics in 1973. > ------- End of forwarded message ------- >  > --  > Denis Howe   > Free On-Line Dictionary of Computing > http://www.foldoc.org/ Put your brain in gear before starting your mouth in motion. Rocky's Lemma of Innovation Prevention: Unless the results are known in advance, funding agencies will reject the proposal.  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Josepshson Junction              A type of {electronic} {circuit} capable of switching at very high speeds when operated at temperatures approaching {absolute zero}.Its low power dissipation factor makes ituseful in {high density computer circuits}.                                                                                                 (2004-06-28)  Submitted my next FOLDOC entry Today to Denis Howe.  Thanks again to  urlLink FOLDOC .
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Had planned for a demo and missed that completely. I think that could be a unreason of un-satisfaction , together with still not an expertise in Linux.  Here are the  urlLink slides .    
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Pain builds character. (Sometimes it builds products, too.) -  urlLink Jwz 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      In the misc section:  Sending binary data files in ASCII-armored format without encryption or signature (using pgp command line package.)      pgp -a      This command instructs PGP to produce an ASCII-armored file called filename.asc . The recipient uses the -p option to unwrap the message and restore the sender s original filename.pgp -p filename.asc You can then store the files in a email message or database table in text format.  http://unixtips.org/index.php3?catList=8 ---  This is Incorrect.  This does not work with GPG version 1.2.1 For gpg command to act, it requires an option as well as a command. -a is an option and no command is being given.  Kindly remove this tip.   Regards, Senthil --   ( I have included the Original Author of tips Id, If I am wrong in understanding kindly let me know, but this tip failed in my box)  --  ~~~                                                                               ~~~ If you think before you speak, the other guy gets his joke in first. ~~~                                                                               ~~~ 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      As am Preparing for Blug meet, (I am really nervous,butI am keeping the basic rules in my mind) . Just to check the concepts, I downloaded  urlLink Phip Zimmermann 's  urlLink PGP Keys  and imported to my Keyring and verified the Fingerprint ( Just for the sake of it). :)
     

    
</post>

<date>01,June,2004</date>
<post>


       
       Need Gmail Invitation?   Let me know, I shall send the invitation to your email id.   
     

    
</post>

<date>01,June,2004</date>
<post>


       
       urlLink Bangalore Kidney Foundation  is conducting a  urlLink program  on 26th and 27th June and its for good for all. Few of us have decided to voluteer for various activities.  If you are interested, then be there at  dhwani sur mansur  and support the BKF Intiatives.   urlLink   
     

    
</post>

<date>01,June,2004</date>
<post>


       
       "What ever you do will be insignificant, but is very important that you do it."   -Mahatma Gandhi 
     

    
</post>

<date>01,June,2004</date>
<post>


       
        urlLink     There were many things useful.   The general stuffs led me to read more HOWTos.   just about gnuplot   a fine intro on firewalls and a bit details in iptables and ipchains.   Then there was in Hyperthreading concepts of Intel P4 w.r.t Kernel 2.6.x and concurrency concepts.( Just read and need to come back again to logically understand completely)   some news and fun!
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Bill Joy again, a great man living admist us.He says us to Proceed with caution.        urlLink nytimes article . Bill Joy finished his 21-year career at Sun Microsystems, the huge technology company he helped create and build, early last September. At that time, Sun sent out a press release to announce that its chief scientist was leaving and had no definite plans. This was true: Joy had only vague notions of what he would do next.  As he departed, he told a reporter he wanted to sit down and relax with a glass of wine and a blank sheet of paper. This was perhaps not so true: it's hard to imagine Joy with a blank sheet of paper. Thoughts and ideas pile up furiously in his notebooks; diagrams and scribbles and equations issue forth at all hours of the day and night.  When Joy, 49, finally moved out of a small Sun office in downtown Aspen, where he had worked since the late 1980's, he had to archive 900 pounds of professional papers. Then it took him months to go through his personal records, which filled 20 fireproof file cabinets.   Over the past few months, Joy has been splitting his time between a new house in Marin County, Calif., and his main home in Aspen, where his two children go to school. ''I'm in an in-between thing,'' he says. Indeed, there are at least two Bill Joys. Start with Joy the computer scientist -- or computer architect, as he often describes himself -- a Silicon Valley deity, generally regarded as one of the most gifted engineers ever to have negotiated freeway traffic. Much of the present-day Internet is built upon the Unix code Joy wrote in the 1970's when he was a grad student at Berkeley; after that, his signature inventions shaped the Java programming language and the Sun computer servers that power a good portion of the Web. This Bill Joy recently flirted with the idea of taking a job at Google and continues to entertain pitches from friends and strangers eager to have him join their start-ups. The challenge is to find the right one.  As Joy says, he wants to do something in technology again, but only something new, something different, ''something that isn't going to happen anyway'' without his help.   The other Bill Joy, however, would very much like to prevent the inevitable from happening. Four years ago in an article he wrote for Wired magazine, Joy declared that the headlong race in biotechnology and nanotechnology might prove catastrophic. In the time since, he has continued to explore and advance this concern. Joy says he thinks the probability of a ''civilization-changing event'' is most likely in the double digits, perhaps as high as 50 percent. He doesn't merely ascribe these odds to terrorism; he suggests a pandemic disease might arise from a sudden accident or as a consequence of cutting-edge research. For disquieting evidence, he points out that a couple of years ago scientists assembled polio in a lab. That in late 2002 J. Craig Venter, the founder of Celera Genomics, announced plans to create organisms from scratch. That only a few months ago scientists were tinkering with deadly strains of bird flu in less-than-top-security labs.  That the genomic sequence for the plague is now on the Web for anyone to see or make use of.    Joy calls the bird-flu experimentation ''insane.'' But he is fixated less on whether scientists committed ethical breaches in this case than on whether the larger scientific community can temper the pace of innovation before it's too late. He's not exactly optimistic, predicting that public awareness will most likely come only after an actual accident at a company or a university. Until then, he says, speed -- the mad rush for patents and market share and money -- will trump caution. Regulatory agencies are structured to catch shady C.F.O.'s, not reckless private-sector technologists. And markets are ill equipped to play traffic cop. ''Markets are extremely good at go,'' Joy says. ''They're not very good at stop. And I think we need a little bit of stop right now. Or else we're not going to like the outcome.''    One of Joy's favorite coffee shops in Aspen is Zele, a bustling downtown hangout where he has been a regular for half a decade. On a sunny morning in March, I waited at a window seat and watched him arrive from half a block away. There was no mistaking him. Lanky and loose-limbed and well over six feet, Joy wore faded sweat pants and an old sweatshirt; his hair, always tousled in photographs, actually stood vertically on end.  Joy seemed so utterly lost in concentration as he crossed the street that for a moment I worried he might be hit by a passing car.  Then he tuned in, came inside and introduced himself with a big grin and a handshake. He took off his watch, unplugged his headphones, unstrapped an iPod mini from his upper arm and dumped the gadgetry, along with a Treo already chiming with incoming e-mail, on the table.   Over the past few years, Joy has often been characterized in the press and on the Web as haunted by grim possible futures and tortured by whatever role he might have played in leading us to them. In person he comes across as a serene technology enthusiast, working through his talking points without anxiety or heat. Unlike Bill McKibben or Wendell Berry, two authors Joy admires who have written with passion about how technology threatens to devalue our day-to-day lives,  Joy has made a point of not pursuing the moral, religious or political dimensions of his argument. ''I'm focused on truly catastrophic danger, where we must act,''  he wrote to me in a late-night e-mail message.   Joy began writing a book about his concerns several years ago. The result was a 50,000-word manuscript, written between projects at Sun and never published, that he said he hoped would awaken a lay public to the potential dangers of genetic engineering, robotics and nanotechnology. In September 2001, Joy took the manuscript with him to New York, where he rented two adjoining rooms -- one for writing and one for sleeping -- in the Mercer Hotel in SoHo. On the night of Sept. 10, he stayed up past 4 a.m. sorting hundreds of books on terrorism, flu epidemics, plagues and bioweapons. He had just shipped them in from Aspen. Joy says: ''I woke up four hours later to smoke and sirens, with this manuscript about using technology to do terrorism right next to me. Spooky.''   He scrapped the idea of a book as a wake-up call. He began an ill-fated second draft with a sociological theme (cultural responses to death) and then tried a third draft, relating scientific research to capital markets and the law. Joy is adamant that modern technology has wreaked a huge imbalance of power. A lethal virus fashioned in a lab or a genetic experiment gone awry is far worse than the dynamite that Alfred Nobel invented nearly 140 years ago, because the self-replicating nature of these dangers leaves an inordinate amount of responsibility in the hands of a single scientist or corporation. In Joy's view, the legal system is now powerless to stop a rogue scientist or a negligent technologist.   He is likewise sure that the financial markets do not acknowledge the true hazards of certain kinds of science. To Joy this is a hugely important point.  He isn't keen on regulation, since he considers it far less effective than market forces.  (A millionaire many times over from his shares in Sun and other tech start-ups, Joy knows the fruits of the market firsthand.) Yet he does think we now need to ''manage'' the system somewhat. He says he believes that businesses doing research in areas deemed risky by their peers should be forced to take out insurance against catastrophes. He also says that science guilds should have the authority to limit access to potentially dangerous ideas. ''Perhaps some knowledge won't be made public,'' Joy says. ''Perhaps there would be secrets. You know, you couldn't just get the code to the plague or the flu if you wanted it.'' In this model, any given firm could be refused genomic information by a guild, or bankrupted by insurance costs, or rejected by venture capitalists or investors frightened away by potential expenses and liabilities.   These notions made it into his book's final draft. But that draft will never be published. ''The book got away from me,'' Joy says. He wasn't satisfied he had come up with a comprehensive set of solutions, and by mutual agreement with his publisher he pulled the plug. He finished paying back a six-figure contract advance a few months ago. I asked if it was harder to write software systems than to write a book. ''With code, the computer tells you if it understands what you write,'' he says. ''It's much harder to write prose. That is, if you want to be understood.''    If there is a key to understanding Joy's point of view, a code to his code, it is recognizing his preoccupation with risk instead of fear. Making us think about potential ''bad outcomes'' is his goal; scaring the hell out of us is not. Joy often uses the free market as an example of a system where any outcome, good or bad, is possible. At the moment, he argues, the same potential for good and bad outcomes exists in various kinds of private-sector research. The problem, though, is that a single bad biotech outcome may quickly become epidemic, unstoppable and irreversible. ''Markets can take us places we don't want to go,'' he says, ''and science, unchallenged and uninhibited, will take us places we don't want to go.'' As a contrast, Joy brings up the example of the operating systems he writes so well, where information is carefully architected and vast numbers of computational outcomes are anticipated. Even the freakish occurrence, the wildest mistake, is taken into account.   The theme of risk runs through Joy's other work. He characterizes his efforts at Berkeley and Sun as a 25-year project to make computing simpler, more reliable and more secure. A longtime foe of Microsoft, he is particularly scathing when it comes to the bugs and vulnerabilities of that software company's operating system. Windows creates risk; Windows tempts a bad outcome. It's a logic Joy applies to his personal life as well. At his house in Aspen one afternoon, Joy went downstairs to help his 8-year-old daughter choose a movie to watch. Joy is a film buff, and he recently outfitted his basement with a spectacular home entertainment system. He also happens to be a bibliophile, so he bought three handbooks -- ''Halliwell's Film Guide,'' Pauline Kael's ''5001 Nights at the Movies'' and the ''Time Out Film Guide'' -- to compare reviews. ''I was going through the books and found out there are only about 2,000 movies in history in which there's critical consensus that they're really good,'' he told me. ''So I bought 600 of them.'' No bad movies, fewer possible bad outcomes.   Of course, genetic engineering isn't ''Seven Brides for Seven Brothers,'' which Joy urged upon his daughter. But his focus on consequences stays constant. Nanotechnology and biotech are supposed to save lives and brighten the future; Joy isn't sure we can bank on that. We should instead question the bedrock assumption that good science equals beneficial science. Good science, he says, is the discovery of truth -- for example, an experiment that yields an accurate result and that is repeatable. But science may not be good for us anymore if it yields a bad outcome. ''The Greeks knew better,'' Joy says. ''Oedipus was destroyed by truth. He looked like he had a happy life until he learned one too many things. That's the cautionary tale.''   A few years ago, a friend gave Joy a paperback copy of the autobiography of Bertrand Russell. Russell began his career in mathematics, like Joy, before switching to what would become his better known work in logic and philosophy. Like Joy, Russell spent much of his later career agitating for change and speaking out against the perils of technology -- in that era, nuclear weapons. When Joy brought up Russell's book in conversation with me, he found a copy in his living room and began reading in a pleasant monotone:  ''I thought that people would not like the prospect of being fried with their families and their neighbors and every living person that they had heard of. I thought it would only be necessary to make the danger known and that, when this had been done, men of all parties would unite to restore previous safety. I found that this was a mistake. There is a motive which is stronger than self-preservation: it is the desire to get the better of the other fellow.''  When he finished reading, Joy closed the book and said, ''I understand exactly where he's coming from.''   Indeed, while Joy defines himself as a realist, never as a pessimist, he sometimes betrays a frustration akin to Russell's. He is encouraged, for instance, that weapons of mass destruction have become water-cooler talk and that his speeches and writings have sparked a certain amount of debate in the scientific community. (Joy has been especially provocative on the threat of tiny self-replicating ''nanobots'' reducing all earthly matter -- us included -- to dust. This is what's known among theoreticians as the ''gray goo'' problem.) On the other hand, there have been scientists in the public and private sectors who have characterized Joy as a neo-Luddite. Or who regard him as an outlier, a software writer unqualified to speak authoritatively about the complexities of biotechnology.   Joy admits such insults and marginalization have been difficult. Still, he claims to be more daunted by the complacency of human nature. ''It's hard for people to accept that something will happen if it hasn't happened yet,'' he says. In recent months, he's come to the conclusion that getting further involved in public policy might be the most sensible next step. Joy served on a technology commission during the Clinton presidency and has ties to a number of politicians in Washington. He doesn't consider the Bush administration receptive to his ideas on market restraint, and in the event of its re-election, he would try a fourth time to write a book. In the event of a Kerry administration, he says, he would work his contacts and go to D.C. More than anything, he wants to start a public debate. ''You only get one shot with this kind of thing,'' Joy told me.   And yet when scientists issue their warnings in some far-off corner of Congress, does anybody hear them? I asked Joy about this in various ways over the past few months. When I wrote him one morning to say that the fact that we couldn't yet cure baldness suggests that the risks he describes are far, far away, he wrote back around midnight, sending me a link to some research that suggests baldness might soon be cured. Then he added that regrettably it is easier to destroy than to create, and nature provides a ready-made toolbox of pathogens to adapt from. The issue is how widespread and easy we are going to make the ability for individuals to do this. That we can accidentally create fatal new pathogens is not a theoretical possibility but a practical actuality.   In an earlier conversation, Joy had singled out James Watson, who discovered with Francis Crick the double helix of DNA, as a scientist who rejects Joy's arguments for weighing benefits against bad outcomes. Watson has indeed said that it makes little sense to stop research on account of unspecified risks or ''evil.'' In the name of science and discovery, he says, we are ethically obliged to go forward. ''That position has to be wrong,'' Joy insists. In his view, we are ethically bound to slow down.   This is not an easy case to make. In his last unpublished manuscript, speaking of those in the science and business communities who dispute the catastrophic potential of their work, Joy said, ''I wonder where these people hide their fear.'' I asked Joy about this in another e-mail message. Where is the fear today? A few days after, at 2:10 a.m., Joy wrote: ''I think our lack of pandemic experience is the key. We don't know what it feels like. It seems like Hollywood. It isn't real enough to us that we demand action, we just file the catastrophic thought and tolerate its dissonance and that it conflicts with other things we believe. As humans we do this so well. And, of course, there is Bertrand Russell's experience. . . . But I hope that we can transcend his bitter experience, somehow, now. We must try.''   Some weeks later, he sent a note to point out that a SARS research lab in China had been shut down that day. A safety breach had led to several infections. ''This is the reality of handling such dangerous things,'' he wrote.  He had wanted to remind me. Accidents will happen.    Jon Gertner is a contributing writer for the magazine. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
           To you, a robot is just a robot. But you haven't worked with them. You don't know them. They're a cleaner, better breed than we are.  When Earth is ruled by master-machines... when robots are more human than humankind.    urlLink Dr.Susan Calvin .  I was spellbound by the genius of  urlLink ASIMOV ,when I read his  urlLink I,ROBOT . As a Mark of Respect for that Great man,here are they again...   Three Laws of Robotics      A robot may not injure a human being or, through inaction, allow a human being to come to harm.  A robot must obey orders given it by human beings except where such orders would conflict with the First Law.  A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. 
     

    
</post>

<date>01,June,2004</date>
<post>


       
       urlLink Bugzilla  Bug 253551   Let me see how this goes through. Btw, this is my first bug raised in  mozilla.   Mozilla-Bugzilla:                                                                                 - read the bug-writing guidelines:   http://www.mozilla.org/quality/bug-writing-guidelines.html                                                                                   - look at the list of most frequently reported bugs to make sure your   problem hasn't already been reported:   http://www.mozilla.org/quality/most-frequent-bugs/                                                                                   To submit a bug report, go to: http://bugzilla.mozilla.org/enter_bug.cgi                                                                                   We encourage you to to get involved in the Mozilla project. There are many ways to help out -- see our Get Involved page for ideas: http://www.mozilla.org/get-involved.html  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      If you are using Mozilla mail over IMAP and have wondered that it is not  Highlighting New Messages on the folders and subfolders of IMAP except  INBOX, then here is thing you got to do.  * Go to each Folder - Right Click- Properties and Select the Option  Check for New Messages in the folder.  There is a Bug Open  urlLink 18266    for this kind of behaviour and a patch supplied.  Not tried with the Patch yet. But hope that this feature/features  discussed in the bug becomes available with future Mozilla releases as  default.   Hey, its very much possible for a user to expect that new mails will  be checked in all folders and subfolders of IMAP and he should say so  explicitly.     
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Quickly read and completed the   urlLink LinuxGazatte #104 .  Linux on Low End systems had suggestions for trying different kinds of window managers. some discussion on linux on assembly. There were lots of chat like discussions educating users,but I guess they would be more informative only when read more patiently. kgpg was of help and I sent a mail to the author.  hope to check more and even contribute some.     
     

    
</post>

<date>01,June,2004</date>
<post>


       
       This   about dmr and Bell Lab days is very interesting.                         [dmr]                 ...    Another factor helped the duo of C and Unix to spread much faster than they otherwise would have. AT&amp;T was required under the terms of a 1958 court order in an antitrust case to license its non-telephone-related technology to anyone who asked. And so Unix and C were distributed, mostly to universities, for only a nominal fee. When one considers the ineptness of AT&amp;T's later attempts to commercialise Unix&#8212;after the court order ceased to be applicable because of another antitrust case which broke up AT&amp;T in 1984&#8212;this restriction, an accidental boost to what would later become known as the open-source movement, becomes even more crucial.    ...    
     

    
</post>

<date>01,June,2004</date>
<post>


       
      On Friday I called home and Ashwini picked up the phone and immediately I recognized the good mood and she said Vaishnavi got placed at Futuresoft.  Thats very nice of her and Congratulations Vaish. Cheers!  btw, her home page is  href="http://geocities.com/orvaishnavi/">here .  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      These are some Income Tax stuffs which I have not yet figured out yet.Poor that I dont know anything about these. But got those forms few weeks back from the payroll folks, put them in my desk,locked. Got a mail that I need to submit it again. After a couple of days, I went a submitted it to the Ken Consulting fella here. Yeah, one job over. But I dont understand it yet. :)  Btw, for my reference my PAN Card number is   AAGPO1627H   ( hope next time I need it, I should be able to google for it)  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Bangalore seems to have got a good Internet System. Just completed the Registration at kar.nic.in/passport (Downloaded the Forms also from there)  The Internet Reference No. : I02257    
     

    
</post>

<date>01,June,2004</date>
<post>


       
                           I just checked with few resources and could come up  with this.  > *man system *: >   >       The value returned is -1 on error (e.g. fork failed),  and  the   > return >       status  of  the command otherwise.  This latter return status is  > in the >       format specified in wait(2).  Thus, the exit code of the   > command  will >       be  WEXITSTATUS(status).   In  case  /bin/sh could not be  > executed, the >       exit status will be that of a command that does exit(127). >   > > The return status of *wait(2)  *consists of 2 parts, packed into a  > single 16 bit unsigned integer. > a) a Signal Status that indicates why process terminated. > b) the process termination value as returned by main or given to exit. > > So exit 1  in the script, which executed normally, *might* be  a) 0x00  > b) as 0x01 >            0000 0001 0000 0000  which is 256 >     exit 2  will be like  0000 0010 0000 0000   which is 512 > etc... > > So, the System call in the C Program is giving us the returnstatus of  > wait(2) for the script executed. > > But as the manual says, the exit code of the command will be  > WEXITSTATUS(status). > > So, > #include  > > int retcode = WEXITSTATUS(system(cmdline)); > > gives the values returned by exit in the script. > Regards, > Senthil   > >> If I write a c program which calls a shell script, I'm noticing that  >> the return code is a multiple of 256.  Any idea why? >> >> #include   >> int main () >> { >>     char cmdline[50] = "source myscript.sh"; >>     int retcode = system(cmdline); >>     printf("Returned %d\n", retcode); >>     return 0; >> } >> >> #!/bin/bash >> exit 0          --> returns 0 in the c code >> >> #!/bin/bash >> exit 1          --> returns 256 in the c code >> >> #!/bin/bash >> exit 2          --> returns 512 in the c code >> >> #!/bin/bash >> exit 13         --> returns 3328 in the c code >> >>   >> > > > > > > > > > > > > > > >  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Suppose you have 1.c 2.c 3.c To create tar file of name *codes.tar*, the command is tar -cf codes.tar 1.c 2.c 3.c  c option stands for create. f for file  tar -czf codes.tgz 1.c 2.c 3.c will create a compressed tarred file.  Now for extracting these is this command tar xvf codes.tar tar xvzf codes.tgz  f should go along with filename.     
     

    
</post>

<date>01,June,2004</date>
<post>


       
       urlLink Uthcode  is back and it going to take a all new shape.   Thanks to  urlLink Sarovar.org  for approving the project request. Its going to take a hell lot of work now and I am going to enjoy every bit of it. :)     Uthcode is platform for the young developers to cut their teeth. It provides the snippets of codes for a budding programmer to get started with the programming and enjoy it.     
     

    
</post>

<date>01,June,2004</date>
<post>


       
       You asked:          ``Tell me what I should do in order to keep strong feelings         towards C.'' Permit me to babble out a few random thoughts ...  Write programs for other people.  Where possible, openly publish your code, perhaps on your web site. The http://freshmeat.net/ site is a good place to announce new code. (It is a strange web site name, but they are legit and THE place to announce new code). Support the code that you publish.  Start small and work your way up ... Some of my more popular code is actually small code fragments that are used by others over and over again.  Don't be discouraged if nobody (or if only a few people) uses some of your early programs. What is important is to practice writing quality, well commented code ... to maintain and fix that code. .....  I found this really useful and inspirational once again. http://www.isthe.com/chongo/tech/comp/c/expert.html  Thanks Chongo!  I take it again.  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      ------------ From:	Denis Howe   Sent:	Sunday, July 11, 2004 5:22 AM To:	OR, Senthil Subject:	FOLDOC entry to edit: new definition]  ------- Start of forwarded message ------- From: Matt  To: Denis Howe  Subject: new definition X-Host: user-24-214-169-13.knology.net X-Url: editing X-Browser: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1) Sender: Denis Howe   Date: Mon, 05 Jan 2004 20:20:24 +0000  new definition  Linux Loader (or Lilo)  I have found a new definition. http://www.acm.uiuc.edu/workshops/linux_install/lilo.html   LILO - The Linux Loader >From the LILO README:  LILO is a versatile boot loader for Linux.  It does not depend on a specific file system, can boot Linux kernel images from floppy disks and hard disks, and can even boot other operating systems.  One of up to sixteen differernt images can be selected at boot time.  Various parameters, such as the root device, can be set indepenantly for each kernel.  LILO can even be used as the master boot record.  ------- End of forwarded message -------
     

    
</post>

<date>01,June,2004</date>
<post>


       
      If you have a  urlLink Mozilla  (not using? Kindly comeout of stoneage, I welcome  urlLink you )then try  about:mozilla  in your address bar.   And so at last the beast fell and the unbelievers rejoiced. But all was not lost, for from the ash rose a great bird. The bird gazed down upon the unbelievers and cast fire and thunder upon them. For the beast had been reborn with its strength renewed, and the followers of Mammon cowered in horror.from                                                                                   The Book of Mozilla, 7:15  
     

    
</post>

<date>01,June,2004</date>
<post>


       
         The sycophan t  type of troll is based on a quote from the newscaster character Kent   Brockman of  The   Simpsons . In one episode of the show (episode number 96,  Deep Space Homer ), Brockman watches a video broadcast from a space capsule, within which a number of ants have accidentally been released. The ants appear huge because they float directly in front of the broadcast camera and close to the lens. He mistakenly assumes that alien insects are invading Earth and attempts to ingratiate himself to them by broadcasting propaganda: &quot;I, for one, welcome our new insect overlords.&quot;    Subsequent to this  Simpsons  episode, variants of the phrase came into common use in 2002-2003, generally used to suggest that whatever party referred to as the new overlords is engaging in Orwellian behavior.    --        Any fool can tell the truth, but it requires a man of sense to know how to lie well. -- Samuel Butler        
     

    
</post>

<date>01,June,2004</date>
<post>


       
      I had installed FC2 on the Dell Latitude CPx which I have received to use for few days. When not connected to network,the boot asked me about the xircom card module and I replied to remove it. Later I did not know how to load the module back. There is this command  urlLink modprobe ,but I dont know how to use it. Whom should I ask?? :( The Laptop lied inside the desk for a couple of days coz I would not be able to go online with out activating the card. Trials for restarting the network services gave:  system-config network: xirc2ps_cs device eth0 does not seem to be  present, delaying initialization.  Then decided to just Google it and came across this  urlLink Bug 123734 . Wondered! The same problem has been faced by some other guy as well! Checked with the details and lead me an already opened bug, But the last section said.   ok, after further testing it appears that by removing the ether nete  configuration in Network configuration and rebooting the laptop. then  creating a new network interface definition repairs this.  note:  rebooting without removing the existing device profile does  not repair this, the current device profile must be deleted before  rebooting, then add a new profile.    Did this and Yo! Problem solved. :)    but need to analyze more has this is just a simple and a not so efficient way 
     

    
</post>

<date>01,June,2004</date>
<post>


       
          The match was very interesting and the interesting part was the joy and the talks of the new Champion. They were very much self-depreciating and humourous and really enjoyable.
     

    
</post>

<date>01,June,2004</date>
<post>


       
          Key ID :  4C88D59C    &nbsp;     Key fingerprint =  6C8E 0A4A 64BF 9C70  6034&nbsp; FD5A 1931 DE09 4C88 D59C    &nbsp;     Key Server :  pgp.mit.edu    &nbsp;        &nbsp;        &nbsp;     &nbsp; 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      When I finish my work quickly, I am usually finding that I get more interest and I devote my self more to the work. I should take a hint here.  O.R.Senthil Kumaran  
     

    
</post>

<date>01,June,2004</date>
<post>


       
      If you have been using  urlLink Google Deskbar , you ofcourse know, how useful and how handy it is. Three Cheers to Google again. And as Linux user, I have always wanted such thing for my box.  when looking for some muttrc files, stumbled upon this excellent home page of  urlLink Telsa ,her config files are really cool and helpful a lot. Therein I got the link to   urlLink Googlizer   written by her husband Alan Cox.  Googlizer is very easy to set up and when on X, if you have to google for any term, just highlight using your mouse and and click on the Googlizer in the panel, your default browser will openup with the search.  Mozilla/Firefox already have a right button menu for searching Google for the highlighed term (Good thing), but Googlizer extends this to all applications under X, for example I open a man page over gnome-terminal and would like search google for some term in it, then I would just highlight then and press Googlizer :)  Yo 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Was hearing about this  urlLink Project Looking Glass  for sometime. The idea itself was facsinating and just imagine how it would have been when actually   urlLink tried   on the first day I got the  urlLink news . Sun is inviting the community at java.net for this  urlLink project  and the instructions for  urlLink getting  started was really easy.  urlLink     
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Mutt + Sendmail + Mail resulted in this :)  ---  -----Original Message----- From: Bill  Sent: Wednesday, August 04, 2004 1:47 AM To: OR, Senthil Subject: Invitation.   Dear Senthil,               We are really impressed by your activities. Instead of losing you to Open Source and Free Software Folks, we would like to  embrace you with the good fortune for working with US.  Hope you consider our proposal.  Regards, Bill 
     

    
</post>

<date>01,June,2004</date>
<post>


       
      Phone Number is  2235 5555   Engineers Study Circle Phone Number is 2559 5849  
     

    
</post>


</Blog>