<Blog>

<date>05,August,2004</date>
<post>

	 
       And When was Data Ever Correct? 
 Ironic - I was thinking about how to trust online data, and there goes Rob beating me to the punch! But seriously, how do we know that any data is correct? I mean, I BELIEVED the encyclopedia was correct when I was a kid, but now I understand that encyclopedia writers are just as human as the rest of us.  Case in point, I was listening to one of Stephen Hawking's latest books (I'm a book on tape fiend), and he started explaining current theories of space-time. Then he blantantly said (I'm paraphrasing here), "So what you learned in school was wrong - sorry..." 
 So it makes me think and/or wonder if the Internet, when taken in total, might be a bit MORE reliable (I know, imagine that). Here's what I mean:  basic journalism tells you to always find at least two confirmations before writing it down. But when doing research in a traditional-book-mode, it becomes cumbersome to do this with every little fact. So you trust that the author(s) did the checking for you, footnote the source in your paper, and go on your merry way. But with the Internet and current search engines, I can often find many sources by typing in a few keywords. Now since I know that each individual source may or may not be reliable (see Rob's note below), I'm going to spend a few minutes checking the other sources for confirmation of information. In that way we all become active consumers of knowledge, testing facts against each other rather than accepting them. Interesting, no? 
 Of course, the cynic in me just assumes that we'll be lazy and just get it wrong all the time, but oh well... 
 And on a side note, I too hope the Internet never replaces written media; somehow curling into an easy chair by the fire with my laptop and a good web page isn't the same visceral experience. And I could never have imagined getting through the brand new Harry Potter book online - that site would have crashed in a heartbeat! 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Is that precious Data Correct? 
 Another issue that has to be spoken of while on the subject of finding data online is the validity of the data. How do you know that the information that you find on the internet is any more real then what Al Jazira used to show the Iraqi people? When you would go to the library you know that the information that you find in the Encyclopedia Britannica is going to be correct. When you do a search on google you have no way of know that the information found is going to be correct. Quality over quantity. Knowledge has always been accessible. I believe internet in some ways 'dumbs up' our society, it promotes people to be lazy. Why would you want to go out and research information when you can sit at home in your start-wars pajama pants and look on the internet and find the same information. But is that information correct? 
 
 Now looking at it from the other side. 
 The internet did not, and will not ever (that I can say with confidence, though I'm sure MANY will disagree) replace written media. It is however another very strong resource for finding information and more importantly sharing information. There are things on the internet that you wouldn't find in a library. It is a completely different type of information in many cases. I don't think that knowledge is valued less at all due to the internet. The shear fact that people do use the internet to research subjects and find answers proves that the knowledge is valued. Heck, we have seen a 20% increase in book circulation EVERY year since 1988. We also have 35 public internet computer that are always filled with people, 9am to 8pm. Many times people will come in, research a subject on the internet, and then check out books on the same subject. It's like getting multiple opinions from the doctor. But if lightning caught a library on fire, much like a hard drive, years and years of information would be destroyed. You can't run a backup on a library. 
 
 
 


If you don't repeat the actions of your own success
you won't be successful
You gotta know your own formula, your own ingredients
What made you, YOU.. 

     

    
</post>

<date>05,August,2004</date>
<post>

	 
       How Precious is Your Data? 
 Quick thought today - is data (information, knowledge, insert synonym here) less or more precious in the digital age? I mean, we've always asserted that "Knowledge is Power," but now we have the possibility of having so much knowledge at our fingertips that we might take it for granted. I remember when I was a kid in school, going to the library to research was sancrosanct. You'd go in, consult the card catalogs, pull down tomes of encylopedias, and take notes inn a silent room. I mean, could it be more like a church? For goodness sakes, when you graduated you put on priestly robes! 
 But now, I can find the same information ten times over with a quick jaunt to Google. This happens while my iTunes are blaring Public Enemy, my email is getting my correspondence, and a game of Sims is in the background. Very pedistrian. And I must have as much data sitting on my hard drive as that encylopedia once encompassed (of course, who can say how much of it is "worthwhile" data). All it would take is one good lightning blast and years of notes, projects, art, etc. are gone like ashes. Transient (unless I do better on my back-ups, but who does that consistently?). So when you add this all up, does it mean that we value knowledge less because it is too easy, too transient, and/or too much? 
 P.S. - in-joke for Rob:   K nowledge  R eigns  S upreme  O ver  N early  E verybody... 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Phone-bia 
 I realized something yesterday - phones scare me. I mean, if I have any other way to interact with someone (email, go see them, send carrier pigeon, whatever) over calling them, I'm all over it. Strange, no? 
 I think this comes from two personal tendancies. The first is that I'm an introvert and hate interacting with people, especially people I don't really know (okay, one may now ask why an introvert is starting a blog, but I'm sure that's worth another separate entry and years of therapy!). But the second reason, I think, is more interesting. I find talking to people on the phone incredibly, incredibly awkward. I mean, how many times have you called someone and then got stuck with pregnant pauses, unsure of what to say next? You know, you wish your Dad a happy Father's Day or call an old friend to wish them a Happy Birthday, and the moment you have said, "Happy Birthday" you're hemming and haughing on what to say next? Not that every call is like that - sometimes you can sit and talk with someone intensly for hours. But the awkward times are  so awkward  as to turn me off from the experience. And now with email, instant messaging, and the like, I can still converse with people, just not with actual sound. 
 So my question is, is this just a personal quirk, or is this something endemic to the phone as a medium? Does the phone itself encourage or create the awkwardness, with all of us standing around listening and waiting for someone else to say something? The phone demands so much of us when we use it (focus on conversation, shutting out of distractions) that each pregnant pause is an eternity; the same pause at the dinner table is just a good reason to move on to dessert. Or do I have a phone phobia? 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Harry Potter - no Digerati 
 Okay, I admit it, I was one of the geeky millions that pre-ordered the latest installment of the Harry Potter saga; right now I'm halfway through and enjoying the guilty pleasure immensely. But it struck me - not a single computer in the whole Harry Potter world! What's that about? 
 So it got me thinking of the popular films and books currently bopping about, and they seem to come in two distinct classes: 
 
  Technology is Evil  - from  Matrix Reloaded  to  The Incredible Hulk , we see nothing but technology screwing with our brains, our bodies, and whatever is left.  Very dystopian, very dark, very anti-tech... 
  Technology?  What's That?  - Harry Potter, Lord of the Rings, numerous upcoming period pieces, you see a lot of books and movies coming out/resurging that our based in our pre-electronic past. I mean, I haven't seen Frodo whip out his cell phone yet, have you? 
 
 So is this just chance? Or do we have a collective need to avoid or fear technological change. I mean, how more relevant could  Frankenstein  be in a world of genetic testing and biotech engineering? And it makes me wonder: has any author/director/artist articulated a plausible, enjoyable, and  positive  view of a technological future? Or are technological futures just banal - much more fun to run from Agent Smiths... 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Analog Habits Die Hard 
 Part of my job as the web designer for  urlLink Virginia Homes  is to travel each week to every home being built and photograph it.  I then take these photos back to the office and upload them to the web, where home owners can see their new house under construction. Nice gig, and gets me out of the office on a regular basis. Now often, I will encounter the different tradesmen and subcontractors while photographing, and they inevitably (and politely) strive to stay "out of the frame" - much like how people jump out of view when tourists start taking snapshots. 
 So here's the weird thing - I'm using a digital camera to do this. In the past, jumping out of the way showed consideration to the photographer; each shot that person took was one less piece of actual film being used. A photographer had only 24 or 36 shots (on average) on a roll, and it cost hard money to develop each and every one of those shots. So jumping out of the way was not only polite - it saved the photographer money and other resources. But now that I'm using a digital camera, this isn't the case. Three button presses and the photograph is deleted into the void, and with current memory cards I can store hundreds of shots before having to swap media. So while I may not want every Tom, Dick, and Harry wandering into my shots, it's not a big deal anymore. 
 Yet this is behavior which I don't see changing, at least not for a long, long time. I mean, it's habit - we've all grown up with traditional cameras, and I'm sure the majority of cameras still in use employ actual film. Still, I forsee film being used less and less every year - it won't be too long till most cameras are digital in nature. So will we hold onto this polite dance we do when someone pulls out a camera? Will we continue to treat the camera as an analog medium, though the new generation of cameras have few of the limits their predecessors had? How hard is it to change memorized manners and behaviors? 
 Note - I probably should state that I am not advocating  not  getting out of the way of photographers; it's still a polite and considerate thing to do. I certainly plan to continue the behavior. It just seems anachronistic, that's all. 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Deus Ex Machina 
 I've been listening lately to Stephen Hawking's book  Universe in a Nutshell , and today he began to muse about the growing complexity of our own evolution and the evolution of our "electronic" creations. It made me realize once again that there is a great fear that our machines (computers, robots, refrigerators, what have you) will overthrow their creators (again, us) and rule the world. Now most of the time this is a proposition, that while ominous, which is jolly fun; just look at the Matrix and Terminator movie franchises if you think otherwise! But in musing today, I now wonder if this is just an irrational fear. The argument, to me, seems to fall into three flavors: 
 
  I'm an Idiot Theory  - It seems that most people assume that humans, in general, are idiots. That we make things with no regard to their meanings, implications, or even results. And that eventually we will stumble across making a machine with enough intelligence to clone itself, overtake us, and either enslave or kill humanity outright. Now, I'm not one to defend the intelligence (or lack of) of the human race; certainly we can all come up with examples, either individually or as a group, of people being idiots. But to project that upon every member of society seems an overgeneralization. I've got to believe that some of the people who are not idiots are also some of the people involved with A.I., computer design, and programming. Hopefully they'll be intelligent enough to pull the plug? This idea is also a bit contradictory - it states that while we may be smart enough to build a complex, thinking machine, we're not complex enough to protect ourselves from them. Call me crazy, but the protection instinct seems WAY less complicated than a microchip - all I need to take out my laptop is a sturdy hammer... 
  Fear of Creation Theory  - Let's call this my Frankenstein Theory. It works a little off the above "I'm an Idiot Theory" and looks something like this:  Man makes creation, creation gets pissed that it's creator isn't perfect, creation rips creator to shreds (either literally or metaphorically). So why are we so scared of our own creations? Is it a fear of the imperfections of ourselves? I mean, if we actually met "God" (in whatever incarnation you prefer), which would we do:  a) fall down in a mixture of awe, disbelief, and/or praise that we'd met our creator, or b) rip him to shreds because we found our Junior High years SO imperfect? So then why do we think that our creations would be so pissed at us that they'd eradicate us or our way of life? Sounds a bit Freudian to me... 
  Logic is God Theory  - This one comes straight from the Enlightenment, the idea that the universe is some expression of supreme logic and order (whether divine or otherwise). And since we build computers and such in the spirit of mathematical logic, they will eventually replace us because they, as it were, are closer to the language of God. And that they'll just create even smarter computers - we won't stand a chance and go the way of the Cro-magnon. Okay, haven't we learned already that it's hard, very hard, to create something from scratch that is better than you in every way? When's the last time you've had a philisophical discussion with your computer?  Or composed poetry with it? Or romance? Sure, my computer whips me every time we play chess, but let me tell you that that's not too hard! The only creations we make that can sometimes surpass us are our children, and even then it's a crap shoot. And we didn't use blindingly elegant logic, it had to do with a messy thing called sex and years of trial and error, not to mention millions of years of evolution. Yet we seem to fear that once the computer is given free reign, not only will they comprehend themselves to clone themselves but that they'll improve on the design and replace us... 
 
 Now, I'm as paranoid as the next body, and loved having my wits scared by those squidy things in the Matrix. But it does seem that imminent predictions of our demise by the machines sells human potentiality short. Still, I might consider keeping a wooden bat next to my CPU, just in case. And watch out if they co-opt our pets in the cause; then we'll really be in trouble! 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Suprised?  Well.... 
 Well, the news that the web design world is waking up to (unless, like me, you're really a geek and check the news over the weekends) is that Microsoft has decided to discontinue development of the Internet Explorer Browser for the Macintosh. Now as I'm a dedicated Mac Geek, this should be affecting me.  However, I long ago ceased using IE on any platform, Mac or otherwise. The truth is that while IE had been a groundbreaking browser, especially on the last Mac release, it has long since been eclipsed by other third parties (Safari for the Mac, the Mozilla Project, Opera, you take your pick). So when Microsoft says that they think Mac users will be better served by something like Safari, I wholeheartedly agree with them (hard to admit that I'd agree with MS, but there you go). 
 What I find more disconcerting is that this seems to fit into a series of recent predatory moves that Microsoft has taken.  Let me list a few... 
 
 Microsoft discontinues the development of Macintosh Internet Explorer, further isolating the Mac Platform from mainstream software and acceptance.  MS does promise to continue development of Office for the Mac, however... 
 Microsoft buys the leading Windows Emulation software company for the Mac (makers of Virtual PC); for the time being they promise to continue development... 
 Microsoft buys a leading Linux developer whose main product is Linux Anti-Viral Software, than plans to discontinue the AV software 
 Microsoft begins paying licensing fees to SCO for Unix, strengthing SCO's claims of ownership of the Unix code and thereby the threat of legal action against users and distributors of Linux (which SCO claims incorporates copyrighted SCO Unix code).  Many folks see this as a way to help SCO make it's current legal case against IBM and in general weaken the position of Linux and Open Source projects in the world at large. 
 Microsoft begins slashing prices on software which has competition with Linux, including Developers SQL Server (by over $400 dollars).  They also establish a special fund to use in discounting software when signing contracts with large enterprises (governments, etc.).  All of this follows a memo where Steve Balmer specifically indentified Linux as a threat to MS. 
 And the capper, rumor has it that MS will soon discontinue the development of a stand-alone browser for the Windows (that's right, Windows) platform.  If true, it means the ultimately the only way to get a new browser would be to buy a new copy of Windows itself. 
 
 Now, taken separately many of the actions seem either justified or benign. But together, it appears to show the same pattern of predatory practices that landed MS in the courts before, only this time there seems to be no push by the government to respond. Now, for legal reasons, please don't trust or quote this - go to the source.  Most of this info has been gleaned from  urlLink C|net  and other online sources. And if you find something that appears to be misinterpretted or in error, please  urlLink email me  and I'll do what I can.  Now the point of this is not to sound like a raving paranoid, but I do have to admit that it all smells kind of fishy... 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       June 15 - Fathers' Day 
 Good morning all, and a Happy Fathers' Day to you. This being my first Fathers' Day (my first child was born June 2), it got me thinking about how one of the most interesting digital things I created didn't require a computer or special training at all.  For what is DNA but sets of digital bits, replicable and and with error sum checks?  So that while we still see the world in analog terms, our basis is digital in nature. Now, a former instructor of mine asserted that a human being isn't truely human until they come of age and awareness. Certainly seems to mean that I have a lot more work and training ahead of me; time to work on the upgrades... 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Greetings to the Blog... 
 Hello to the blogging world from myself, Jeff Stevens.  I hope to start this weblog as a diary of my trials, tribulations, setbacks and successes as I begin converting myself from a traditional to a digital fine artist - keep your fingers crossed! 
 As for background, I am an artist, web designer, and educator in Columbus, Ohio.  My current interests range from creating 2-D collage pieces to interactive Flash art/applications (whatever that might mean).  To see more from my "analog past," see my work at  urlLink http://home.earthlink.net/~jfstevens . 
 I also hope to invite others to participate in this little experiment, sharing their experiences in creating art through new online media - if you would like to chime in or have a participant suggestion, let me know! 


     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Moving Day Blues 
 If there's anything I hate more than moving from an old location to a new location (home, office, etc.), I can't think of it.  So you can imagine what fun I've had over the last two weeks as my company,  urlLink Virginia Homes  has moved to a new office in Downtown Columbus. And while I like the new digs (get my own office and everything, woo-hoo!), I hate all the bother of moving: packing, unpacking, cleaning, customizing a space, etc. 
 But the worst of it has been getting our computer system up and running. While it's nice to have new computers (very, very nice in fact), I still can't believe all of the little things one does to a system, and that I now have to redo. Why can't I have some kind of tech genie how wiggles his/her nose and boom, there it is? Even worse, I had to buy a new PDA last week, and boy did I have to jump through hoops to get that back to normal. I mean, I wish you could just do some kind of electronic Vulcan Mind Meld (probably can, just don't know how yet!).  Guess it's back to hacking school for me! 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Is Film Too Easy? 
 As many who know me know, I'm becoming a DVD junkie. This is not necessarily because the picture quality is better (though I love that most are in widescreen format) but rather because of the numerous director's commentaries running through the movies. I have learned more through listening to directors talk about their craft then I could have imagined, and it's much cheaper than film school! 
 However, I am beginning to wonder if all the new CGI/digital technologies might drastically alter the creative process of filmaking. For example, last night we watched "Shanghai Knights" (yes, I now this is not an Oscar winner, but we were in the mood for something light). Throughout the director's commentary, he'd explain how this building or that building was real while the rest would be CGI, extrapolated from plans, still photographs, and virtual models. And it seemed that it made everything very easy - all he'd have to do is say, oh, "put Jackie Chan on top of Big Ben" and boom, some CGI firm had it ready for him (yes, I'm simplifying the CGI tech's skill, but stay with me). Contrast this with a recent E! documentary on the making of "Jaws," one of my favorite films. Now when making Jaws, the mechanical shark kept breaking down. This kept forcing Spielberg and his crew to rearrange filming schedules and rethink their storytelling. One of the great things that arose from the faulty fish is underwater-fish-point-of-view shots, accompanied by the great Jon Williams bass line. Now I can't imagine "Jaws" without these shots - they really up the suspense and create anticipation for when you actually see the title character in the last third of the movie. Now today, I can imagine them saying, "Well, let's just add the fish later with CGI" and moved on, never being forced to rethink their assumptions and come up with alternatives. 
 Now, I love having obstacles when i'm crating - they often force me to find new, creative solutions, my own version of the shark POV shots. Such "resistance" helps take me out of my ego-driven first ideas and expose me to new possibilities. Yet, this resistance seems to be ebbing out of film as the computer makes more and more effects possible. My worry is that this will limit the creativity of the filmmaker accordingly 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       They Call It Default for a Reason 
  urlLink An article of interest  appeared the last few days. It announced that another new security flaw was found in Microsoft's various operating systems; according to reports this was one of the "worst" yet, allowing hackers who exploit it to have free reign on servers or desktops effected. Not much news there, if not for an announcement in the end of the article: Microsoft won a $90 Million Dollar contract to provide desktop and server software to the Department of Homeland Security.  What?!? I mean, don't they keep up with the monthly (or sometimes weekly) patches to plug security holes in Microsoft products? For more input and links, check out the responses of two of my favorite bloggers,  urlLink Eric Meyer  and  urlLink Jeffrey Zeldman . 
 Now normally I might take this opportunity to gloat over that fact that I'm currently typing on a Mac, but this got me thinking other directions. Why is it that common wisdom / the general public always find a choice and stick with it when there may be many more exciting alternatives? I can't think it's just being lazy - i'm too much of an optimist for that. For example, when I was elementary-school age the Desktop Computer was just making it's big breakthrough. Now there were many good choices out there - Commodores, Apples (pre-Mac), etc. But my father (whom I love, don't get me wrong) held out to by a PC until IBM rolled out their first desktop - codenamed "The Peanut." Now other than being back up by a nice ad campaign (featuring a Charlie Chaplin look-alike, classic), the computer was not much to look at: not much software, not much power, not much room for expansion. Certainly history shows that there were better alternatives out there, but in that day and time  Big Blue  was the big player; they were computing's 100-pound gorilla and seemed to make choices accordingly. I bring up this memory because twenty years or so later, Microsoft is the big monkey on the block. And when I see choices made like the one by Homeland Security, I just remember to that IBM Peanut and wondered what my dad was thinking. 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       More Internet History Gone 
 A sad announcement for computer history - AOL is officially axed Netscape, the first commercial browser and descendent of the first graphical browser, Mosaic. Not only did the company change the way we interact with information, but also helped spawn the Tech Boom of the 90's, the current Mozilla project, and a ill-fated investigation of an un-named software company. The word is that about half of the Netscape staff was let go, and that others are being moved about AOL. The only good news I have is that  a)  standards guru  urlLink Eric Meyer  still is employed, and  b)  those dedicated to Mozilla development will fall under a new non-profit entity,  urlLink The Mozilla Foundation . 
 So within a month of time, Netscape is closed and Microsoft announces that it will neither continue developing a browser for the Macintosh nor offer future stand-alone browsers for Windows. Now to be totally honest, I have long ceased using either product for some time; my current recommendations run toward Mozilla Firebird (for Windows) and Safari (for the Macintosh). But it seems amazing to me that such old-school software is collapsing in such a short period of time - I guess they were telling the truth when they said you can't make money from browsers! How this will change the future of the web is anyone's guess, but one thing is for sure: the  urlLink future of browser development  is in the hands of the next generation! 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Unplugged and Coping 
 Well, it's been an odd couple of days in the Stevens' household.  Storms have been rolling through the Central Ohio area for the past five days, engendering much chaos.  So far we've lost power twice (once for 20 hours), lost our phone line (still not fixed), and lost our favorite shade tree in the backyard.  But the interesting thing was how I discovered all these quirky little habits centered around the use of electricity.  Like, flipping on the light switch in a darkened room.  Now I know that the power is off, that no light switch will work, but inevitably I'd go into a room and hit the light switch before I could stop myself.  Nearly every other impulse (have to run the dishwasher, should turn on the TV, etc.) I was able to curb, but the light switch got me every time.  It's like it was a reaction wired (no pun intended) straight into the limbic system. The other thing I noticed was how much noise all that eletrical equipment really makes.  When the power finally came on (at midnight), the racket itself woke me up. And yet we ignore it 99 percent of the time! Amazing... 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       The Emergent Matrix 
 Lately I've been perusing various books which analyze, criticize, or prostelitize  The Matrix ; this is my little way to both prepare for the release of the last movie & upcoming movie as well as still keep a finger in a philosophical pot (hard to do now that I'm years removed from graduate school). These books tend to be middling-okay to good, and right now I'm into a winner:   Taking the Red Pill .  Now one of the arguements against our "being" in the Matrix is the sheer amount of effort, information, and energy our digital oppressors would have to expend to create the Matrix; one author in the book  Exploring the Matrix  estimated that it would take the total  energy output of a large star to encode a small city's worth of information.  This got me thinking, however. All these arguements depend on the fact that The Matrix would have to simulate the world to a great degree of resolution, in order to fool the mind.  But what if the mind was an unconcious conspirator in it's own enslavement? 
 Here's my thinking - there's all these folks hooked up to the Matrix power plant, right? And the machines are feeding a false illusionary world to this mass, right? So what if the machines only have to feed a small amount of info?  What if the human minds themselves (out of a need to create order, sanity, what have you) create a consensual illusion based on limited clues fed by the machines?  I mean, humans today in sensory deprivation tanks can begin to hallucinate vivid realities; the mind just gets bored and starts to make stuff up. And what are our nightly dreams but vivid illusions created solely by the mind? So the trick would be not to create the illusion world in great detail, but to simply give enough clues to convince the mind to create it for you.  The other trick would be to get all the millions of separate minds to agree on the illusion somehow, and then just let the masses dream their dream, creating a group hallucination guided by your sparse inputs and controls. 
 Now since I'm not a philosopher, psychologist, or computer scientist, I'm sure there are many arguements against this idea of a consensual dream which emerges from the inactive minds of the imprisioned. Indeed, one problem would be that the machines must be molding the Matrix to follow the same rules as the "real" world - otherwise, why wouldn't everyone dream that they could fly like Neo (I know I've taken flight in a few pleasurable dreams)?  And if there wasn't a strong correspondence between the illusion and the real, how would Neo know how to react and move when he awakens from his dream in his personal slime-pod? But still, I think the secret to building a Matrix (which I'm not suggesting, by the way) would be to create just enough and let the slaves fill in the mundane details (like the taste of food, the touch of fabric, the sky color, etc.) rather than dedicated piles of resources to controlling every aspect of the illusion. I mean, don't the machines have anything better to do? 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Random Wanderings 
 Well, I'm not sure I have anything enlightening to say (when do I ever), but with the July 4th Holiday approaching I thought I'd better say something before I go coma for the weekend. So... 
   Congrats to Apple  - Bravo to  urlLink Apple Computers  for finally updating their CPU and computer hardware, creating what looks to be a screamer of a machine. Unfortunately, I can't justify buying one, so I'll have to just watch jealously. Apple claims that this will be the fastest desktop machine on the market, but since they don't hit the shelves until August, I'm reserving opinion. Also, props to them for creating the iSight (clever name, as usual), what looks to be a fine web cam/microphone designed specifically for the Mac. Sigh, Christmas is how long from now? 
  Bravo to Free Speech  -  urlLink This just in today  - a recent court ruling defines speech on the Internet (such as blogs, like this one) as protected under free speech.  Basically, that means I probably won't get sued if I go on a tirade. Let's hope that court ruling holds; I love tirade-ing! 
  Hack-A-Thon This Weekend  - Okay, so if you're favorite web sites freak out this weekend, don't panic.  urlLink Various news sources  are reporting that a loosely-organized hacking contest will commence on July 6th, potentially affecting many sites and the general speed of the Internet. Interestingly, they give points based on how hard it is to break into the server:  1 pt. for a Windows server, 2.5 pts. for a Linux server, and 5 pts. for a Mac server (again, yeah Apple). And I swear, it wasn't me who did it!  
 That's it, I'm outta here for Hot Dogs and Fireworks - Jeff 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       The Importance of History? 
 As another entry in what is becoming a general theme, I have been wondering about the importance of tradition and history in the transmission and creation of knowledge.  This comes from a few different sources, primarily two books I've finished or about to finish:   Pencil: A History of Design and Circumstance  (a history of the development of the Pencil, and of engineering in general), and  Blue: the Murder of Jazz . In particular, to me, is this question: does the reliance on tradition and history ultimate advance or impede the development of new ideas and innovations? 
 By inclination and general assumptions, I want to first come down on the side of tradition. I mean, my personal training is in Fine Arts Printmaking, and can trace my heritage back a few hundred years.  This tradition informed the techniques and methods I learned, the influences I absorbed, and the art I subsequently created.  Like a traditional guild artisian, I learned the craft under the hands of "masters" (in these days called "professors"), who passed on general knowledge and personal techniques for success.  This is how knowledge was passed down for hundreds of years - masters who took younger craftsmen under their wings and created an environment for them to be indoctrinated in the mysteries of the craft.  Our modern school system, in general, is simply the institutional extension of the apprentice relationship.  At least, it was till the last 50 years or so. 
 However, both  Blue  and  Pencil  show flaws in this structured learning.   Pencil  illustrates the problems and roadblocks caused by this tradition in pencil innovation:  the blind adherance to old, outdated methods, the use of secrecy to prevent expansion of the industry, and the rejection of new ideas based on traditional preferences.   Blue  portrays a more scathing picture:  that of a jazz community so committed to its tradtional past that it was losing the innovative qualities and attitudes that made it so vital to 20th Century American culture.  While both books do celebrate the repective traditions, they also show that over-adherance leads to stagnation and marginalization. 
 So, where does the most innovative paths lay?  Certainly, it seems that those who push a medium forward are those who break from tradition and try "the unthinkable" (as Buddha says, kill the teacher).  Yet without a tie to one's past, one can blindly muddle forward making the same mistakes as one's predecessors. So how does one walk the tightrope between repecting the past while breaking with it to explore the new?  Any ideas? 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       A Digital Responsibility? 
  Just finished a book on tape,  IBM and the Holocaust . It describes the measures taken by IBM to secure, and then hide, cotracts with the Nazi Reich - all for the cause of profit.. The author posits, probably quite correctly, that the extermination of millions of Jews and dissidents would not have been possible without the computations of IBM punch-card machines. Hitler didn't make the trains run on time, IBM did. 
 I found this book disturbing, of course, for many reasons. But chief of these, for the purpose of this blog, is that this means that our digital inheritance is partially built on the ashes of victims. When I build a database, compile statistics, or construct a mail merge, I am inadvertantly following in the footstep of SS officers as they constructed and indexed the round-up lists for the death camps. The tattoo on each arm wasn't just a number; it was the key value on each victim's punchcards.  
 And here we are, fifty years removed, helping both public and private enterprises gather data on us. Every credit card purchase, every online form, every questionaire is remembered somewhere. Sure, it would be a monumental undertaking, but the lesson of IBM and WWII is that there's always someone who can organize the immense, for the right price. IBM touted itself as the "solutions" company; it seems our responsibilty (the digirati) to ensure our callings never again provide "final solutions." 
 And in terms of fair reporting, here is  urlLink short article  on ABC News where IBM internally warns employees about the book. 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Quick Link on Digital Design 
 Here's a  urlLink quick link  to an article by Bill Joy, Chief Scientest for Sun. He begins to articulate a vision where we design digital devices in terms of compatibility and flexibility, preparing for the era of "pervasiveness" in computing. I have always found Joy's ideas visionary and energizing; hope you enjoy! 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Travel Blog 
 From August 7 to August 10, I will be in Denver for a wedding. Unfortunately, it probably means I'll have plenty of time to think for the blog but no way to publish it. Sg I thought I'd keep these notes while gone and publish them all at once. Enjoy! 
 Darn Airport Security - Day One 
 Well, survived my first run-in with the TSA, with no lasting bruises. Actually, I have to admit that I've never had a problem with them. Sure, I've been stopped for the "random" searches many times (something about a goatee and metal briefcase), I've never had any real trouble. But I do need to remember to take off my watch before hitting the metal detectors. 
 The strange thing is that when I pulled my MP3 player from the carry on, it was completely blank. As you can guess, this is not how I left it; I had expected some Miles Davis and Charlie Parker. Now, since my Palm and Digital Camera weren't affected, it could be that the player is on the fritz. But still, fine before the checkpoint, blank after. 
 Which gets me thinking again to how fragile all this digital info really is. I mean, all our snapshots on the trip will be digital. My link to home, the cell phone, kindly remembers all my numbers digitally. This wonderfully rambling narrative isn't really "written" anywhere; it's all just magnetic states stored in silicon. So today it's the MP3's, what will it be tomorrow? Or to put it another way: I haven't actually touched a tax form in three years. Scary, no? 
 Now let's assume that Murphy and his followers are right - anything that can go wrong will go wrong. Given enough time, trouble finds us all. So do you have back-ups of everything? Does your doctor? Your business? The IRS? Just as you can expect to be involvep in a car accident or a minor fire once in your life, expect to lose digital data; we're all one checkpoint away from losing it all... 
 Photograph and Memories - Day 4 
 Well, Day 4 is here, and you can see how productive I was! It seems like the time to write was on the airplane; think of the frequent flyer miles I'd rack up if I were a novelist. Oh well... 
 So, at the rehersal dinner (and later the wedding reception), a photo-montage detailing the growth of both groom and bride played to light Christian pop. And during the ceremony, a bevy of cameras (traditional, video, and digital) sprang to life at each highpoint of the ceremony. This, naturally, got me thinking about photography and the nature of reminiscence. 
 As the critic/philosopher Walter Benjamin posits, a photograph can be seen as time travel, capturing forever the particulars of  a place  at  a time . While the first decades of the medium saw it treated as a novel form of painting, the Twentieth Century saw it concenrate on the particularities  the medium afforded. And if you look at everyday, amateur photograph, I believe you'd find "candid" shots dominating the developer's tanks. These attempts to not only capture but  freeze  our memories in place is so pervasive as to be invisible. 
 Yet this is not the only means of preserving memories. My example would be the memento, an object or heirloom to which one attaches personal memories. The advantage to this is that the memory is not tied to a particular time and place. For example, when I graduated from college my father presented me with a pocketwatch. This watch was passed down through four generations of sons; it has accompanied me to all subsequent memorable events: weddings, funerals, graduations. I already look forward to presenting it to my son, completing the circle and moving to the next generation. Although the watch doesn't actually "work," it keeps personal time through the Stevens' family. 
 Now, I don't mean to condemn photos over mementos; rather I encourage the cultivation of both forms. Photographs (digital or otherwise) are, by nature, public; they are easily duplicated, shared, dropped in a frame, and placed on the mantle. Mementos, in contrast, are private (stored in everything from sock drawers to safe deposit boxes). My belief is that by cultivating different forms of "memory," you build a richer tapestry of personal culture for your family and you. 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       A Word of Warning 
 To go along with the continuing theme of  Matrix  paranoia that I so love to indulge in, Wired.com features  urlLink an article  today that posits our replacement in the workplace by robots starting around 2030. Now, usually I'd put this in my "ah, okay" bin, but the author is kind of interesting:  Marshall Brain, author and founder of  urlLink HowStuffWorks.com . You know, the guy who makes the books that show how a machine, auto, or dark-age's castle is constructed through snazzy, cut-away illustrations. Cool guy, and not one to go off half-cocked.  Not like me, anyways... 
 And remember, just because you're paranoid doesn't mean you aren't right... 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Nature of the Future 
 I've just finished a book detailing the work process and innovative methods of IDEO, one of the world's leading industrial design firms (made things, like, the Palm V, the Handspring Visor, portable defibrilators, the Apple and Microsoft Mice, etc., etc...).  I can probably guarantee that you've handled, if not currently own, something that came out of their shop. An inspiring book about how to take their methods to incorporate more creative thinking in your business. Now I desperately would love to work there, or somewhere just like it! 
 However, the end of the book got me thinking:  they not only assert that their methods can help you innovate, but can actually help you predict the future of business enterprises. No, they don't mean in some mystical, meditation way; they mean by staying in tune with current trends and encouraging creativity, it is inevitable to find the "pulse" of whatever you are involved in, making prediction easy (although they admit no one is 100% right all the time). 
 Now, I found myself very skeptical about this. For the longest time, I've been very taken by the writings of Arthur Danto, a philosopher and art critic. In this case, it reminded me of a lecture of his I once caught which explored his ideas about utopias and the projection of utopias. To condense the lecture into a sentence or two, he posits that when we (people in general) try to project the future (utopian, distopian, or otherwise), we inevitablely project it in terms of our own current struggles and situations. Yet when the future arrives, it arrives as a genuine surprise.  A computer example would be the rise of the personal computer; experts in the field in the 1960's, and the businesses that believe in them, believed the future was in large computer mainframes plugged into many, many workstations. Yet it was a few hippies at Stanford that formed the Home Brew Computer Club that began building small personal computers.  This spawned Apple Computers, which inspired Windows, which is probably what you're using to read this right now. Unless you new these guys or their friends, you didn't know what was coming next (admittedly, this is a simplified version of computing history, so let me know if you want any other examples). 
 Now, while the IDEO theory commits one to keeping an ear to the ground at all times (so, presumably, that you'd run into the next group of future-builders), they say that knowing the future is possible while Danto says that the future is, by definition, unpredictable. So these are the thoughts I'm having - what do you think? 
 Of course, I'm still pulling for Stephen Hawking to figure out how we can look forward as well as backwards in the dimension of time; I mean, we can already do that in the first three dimensions! Now wouldn't that be cool? 

     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Guilt Posting 
 Okay, I have to admit it - I have nothing worthwhile to post.  Nada.  Zip.  I got back from Cape Cod last week exhausted, stayed exhausted through the week, and came down with a cold that floored me all weekend.  I mean, if we're so technologically advanced, why can't we do something better than Tylenol for the cold? 
 But since I've started this thing, I have an obligation to write something.  Luckily, my favorite writer,  urlLink Neal Stephenson  has just released a new book,  Quicksilver .  I'm a big Stephenson fan and reccommend picking up something from him, like  Cryptonomicon  or  Snow Crash .  I'm a chapter into the new book and am already intrigued; if you want to see where the evolution of Cyberpunk has gone, check Stephenson out.  (Also, the latest William Gibson book wasn't that bad...) 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Vivid Dreams, Part Deux 
 Man, I don't know what I'm doing before going to sleep, but the dreams have been especially vivid this week!  And as seems par for the course, they seem to get me thinking about this blog.  Talk about obsessive! 
 As some might know, my original "training" was in the Fine Arts - Printmaking, to be exact. My specific focus was hand lithography - the use of limestone, ink, and pressure to make prints.  Very analog, very 19th century.  Unfortunately, because of the large outlay of equipment required, I haven't been able to make lithographs for over five years.  In fact, it was this problem that pushed me into the digital realm; it was much easier to fit a computer into limited space and budget than a large press, limestone blocks, etc... 
 So last night, I dreamt I was working in Lithography again. I remembered grinding the stones level, drawing on the surface, which is a beautiful experience ( a former teacher described it as "drawing on the eggshell surface").  And then processing, printing, etc. Even now I can hear the hiss of the ink as the roller moves back and forth, the smell of the chemicals used to stabilize the image, all of the sights, sounds, and rhythms of this somewhat archaic disciplne.  And I miss it, miss it greatly - while I can never claim to have been a great lithographer, I can claim a great love for it's processes. 
 Which, after waking, got me thinking.  Can I claim as great a love for my new digital media?  Can I recall with loving fondness the writing of code, or the scanning of images, or the surfing of a web site? As much as I love digital imaging and web design, I can't see myself pining for it. It doesn't have the sentimental charge - a keyboard is a keyboard, a monitor is a monitor. I mean, I do love certain computers I have used, but I figure that's just my psyche personifying them - why else give them names.  They're one step below pets, nearly family members.  But the processes itself?  Can't say I remember my first program or first web page with the same fondness as my first lithographs, drawings, or other analog art objects 
 Now is this a good or a bad thing? No idea, really. I suspect both. In support of older, analog techniques, I believe that sentimentality helps instill respect and tradition to a disciplne. Just like the goofy robes you wear during graduations, traditions form to protect and pass on disciplnes; they are ties to the past and thus unite across the decades or centuries. Traditions define the transmission of technique and lore, like the old guild halls of yore.  However, without tradition the Digital Mediums can progress unhindered; without a history holding them back, digital artists are truly free to explore everything.  To paraphrase a story about John Cage, the avant-garde composer, Cage was visiting Europe when a European Musician approached him and asked, "How can you compose music with no history behind you?"  To which Cage replied (in his impish way), "How can you compose music with so much history behind you?" 
 Of course, I suspect the answer is different for all people - some require history, some require freedom to strike out alone.  Let's hope we find a way to provide both aspects in the new Digital Millenium. 

     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Vivid Dreams Up Ahead 
 Okay, I had the weirdest dreams last night.  At one point, I'm a vigilante superhero out to save the world from a messy garret.  Next, I'm sailing on a swoop that's shiprwrecked by pirates.  I mean, it was one  weird  night! 
 But the most interesting dream came near the end of my sleep.  In it, I'm with a bunch of talking animals (go figure), and we are walking in a park.  One of the animals goes into a thicket, and inside we find these strange, Tyranosaurus Rex-type creatures.  Turns out that they're from an advanced civilization, and they set up the world as a test case to study de-evolution.  They started out with their culture and let it de-evolve entropically, going through the dinosaur age, then furry creatures that de-evolved from creatures like us and the world we live in.  Needless to say, I was a bit disoriented when I woke up 
 But what an interesting idea! I mean, we are constantly fed the idea that our civilization moves forward, that everything we do is in the name of progress (okay, this is a Western concept, but hey - that's where I live, you know).  But what if we're actually moving toward dystopia?  What if the good old days were the good old days, and we'd be better off in caves than on the 'Net?  What if our entire conception of reality is simply a test in watching a system degenerate and fall to pieces.  Would explain some stuff, no? 
 Now I'm not saying that my dream-inspired uber-dinosaurs were correct. I am saying that it's an interesting and slightly entertaining idea.  And to quote my wife when I told her, "Isn't that what Devo was all about?" 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Knowledge Gets a Bad Rap 
 I have to admit, I've become a  Lord of the Rings  junkie. I mean, like every pre-teen child in the 80s, I read the series and played the random D & D game.  But with the release of the new Peter Jackson movies, I have re-immersed myself in Tolkein's world.  Books, movies, director's cut - you name it, I'm currently reading/viewing/playing it (okay, no video games yet, but that's because the wife has OUTLAWED them). 
 So, it was with excitement last week that I began watching the newly released  Two Towers  DVD.  And I began noticing something when I was viewing a preview of the "deleted" scenes available on the extended edition.  Saruman, the evil wizard featured in the first two movies and played by Christopher Lee, manipulates the events from his distant tower through magic and lackeys (human and non-human alike).  Like a chess player, he positions his forces and lets them roam about the countryside, smiting heroes whenever possible.  Why did this strike me as interesting?  Why is it in this blog?  Because nearly every time he takes action, he first consults one or another tome on magic and lore.  And many characters, in implying that Saruman has changed significantly (i.e., became "evil"), points out that he doesn't "get out" of his tower like he used to.  And while some might read that as the perils of being anti-social, I think it combines as an indictment of learned versus experienced knowledge. 
 I mean, there only appears to be three characters who interact with books in the film (so far):  Saruman, Bilbo, and Gandalf.  Saruman (representing the reader) uses the "book knowledge" to do evil while Bilbo (representing the writer) has become possessed by evil.  Even Gandalf, a certain symbol of good, gets downright pushy and paranoid after visiting an ancient library in the first movie.  In contrast, all the movie's heroes learn lessons out in the real world.  Aragorn is the typical mountain man, living off the land and learning it's secrets.  Elrond, the font of Elven wisdom, has been alive, well, forever, and thus has a phenominal personal memory of events.  Even the lowly hobbits have to journey out into the world to understand the great quest they've become embroiled in.  All of this seems to point to the fact that knowledge gained without experience (i.e., Saruman and his books) is fraught with danger and corruption. 
 Now being a big fan of knowledge in general, and books in particular, I find the implication a bit grating.  I mean, there have been many doctorial papers written on how Tolkein's series is a condemnation on mechanization and the Industrial Revolution, but what if it's actually a critique of how we impart knowledge itself?  Either that, or I was watching the movie at way-too-late an hour! 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Good Luck, Bill 
 Just a short note today:  the AP is  urlLink reporting  that Bill Joy, Co-Founder of Sun Microsystems and general technology pundit, is leaving Sun to pursure other challenges.  If you've never had the chance to hear Joy speak or read his writing, you are missing out; I have found him to be one of the most thoughtful and engaging minds in the current technology field.  Here's to wishing him the best in his next endeavors - I know I'll be keeping one eye on him. 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Me, Supporting Microsoft? 
 That's right, I am now going to come in on an issue and support Microsoft.  A weird place to be for me.  But over the last two months, the Redmond giant lost a patent infringement case with Eolas. Normally, this doesn't bother me much (I mean, Microsoft has lost or settled many cases concerning patent infringement), but this one strikes deep into my territory - the use of plug-ins on the web 
 Eolas gained rights to a University of California 1994 patent whereby information could be remotely accessed by an application through small "applets" or plug-ins, minature applications that loaded information from a server. Sounding familiar yet? They started their legal proceedings against Microsoft in 1999, claiming that Windows 95, Windows 98, and Internet Explorer violated their patents with it's use of Active X and other technologies. Well, back in July the court upheld Eolas' claims, and the case is currently winding it's way toward appeals. 
 Now, I don't mind giving up Active X. But the broadly written patent applies to  any  plug-in technology, such as Flash, Quicktime, Java, etc. Basically, if you can embed it in a web page, it is violating the patent. This is a big, big deal - embedding technology into web pages and browsers is commonplace now, and their delivery of media-rich applications is one reason for the growth of the broadband web. I just shudder to think that every single browser is now, in one way or another, illegal and at the mercy of Eolas and their legal department. 
 In many ways, this reminds me of the current legal uproar between SCO and the Linux community.  Like that issue, here is a small company with a questionable business plan using the patent and copyright laws to survive an otherwise unsupportive marketplace.  The only difference, that I see right away, is that Eolas really does have legal legs to stand on.  But isn't there a point when the good of the public must override the good of the few? That the web could, in all honesty, revert to a technological state more like 1993 than 2003 gives one pause.  And yet, without such legal protections, large corporations like Microsoft could easily steal and bully smaller companies like Eolas (okay, I guess they did that already). 
 This issues has such wide reaching implications that the W3C, the governing Internet Standards Body, is showing fear and organizing to find alternatives to this problem.  eWeek, among many others, is beginning to  urlLink cover this issue , and I suspect we shall hear much more about it in the coming months.  Keep your fingers crossed and enjoy your  urlLink Strongbad Emails  now - you might not be able to view them next year! 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Matrix Confusion 
 Okay, I geeked out all weekend watching  Matrix Reloaded  a few times; it was a happy birthday present from my lovely wife, who really didn't see much of me after giving it (ah, the irony).  Now the first thing I was surprised by was how much I enjoyed it a second time - once I was  done trying to compare it to the first film I was ready to be entertained.  The second thought in my paranoid brain was how not only can we trust what our favorite hackers see IN the Matrix, but we can also not trust what we see going on in Zion, the "real" world, etc. 
 Now many Matrix-ites have written that perhaps Zion is just another Matrix, a simulation on top of a simulation.  This doesn't hold water for me - though I have no arguments to rennounce the idea, it just doesn't "feel" right given the constant dialogue of man and machine living together versus living in opposition; I mean, if it was just another simulation than all the tension of the series is just so much hype, and I can't see the brothers pulling that on their public.  However, one of the messages of the second movie is that Zion, like the Matrix itself, is another control system meant to reign in and manipulate the silly humans.  I mean, it turns out that Zion is set up by the machines and is routinely destroyed and rebuilt.  So what I wonder is this:  wouldn't it be foolish for the machines to have built a controlling system in which they're not involved? 
 So here's what I'm thinking - what if the machines have agents interspersed within the human population of Zion?  Agent Smith's possession of Bane early in the second movie demonstrates that artificial intelligence can "hop" the gap into human bodies; indeed, I'm still not entirely sure that Neo isn't really (or at least partly) a complex program from the machine world.  So why can't the machines simply insert some "programs" into human bodies that subsequently are "freed" and brought to Zion.  Can you think of a better way to monitor and manipulate the inhabitants?  This came to me as I watched Council Member Haman (sp?) speaking with Neo in the bowels of Zion.  He certainly seems to imply a need for man/machine cooperation - the same implications the Oracle gives Neo later in the story.  So is this just parrallel thinking, or are there machine intelligences manipulating Neo from Zion as well as from the Matrix? 
 Now, there's no way to test this idea until the third movie comes out.  I mean, Smith's ability to come into the real world may be the same quirk that allows Neo to sense the machines in the real world.  Perhaps it's an unintended side-effect that the machines in general aren't aware of.  But if I were a machine, it seems silly (or at least arrogant) to set up a cell of rebellous, angry humans and then not put any failsafes into the system. 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Okay, What's Adobe's Deal? 
 As I occasionally have to admit, I'm a Mac Addict. It's not that I am a zealous Jobs-Head; I use a PC at work, I teach use of the PC at Ohio State, and I recognize that there are many advantages to using a Windows system. It's just that my Mac use is pushing ten years now. I'm more comfortable there. It's home. I'll probably always use a Mac to do my work when I can: web design, digital imaging, you name it. 
 So it is with dismay that I've watched several recent Adobe product announcements, all without Mac suport.  First, they updated their video and DVD authoring programs,  urlLink Adobe Premier Pro  and  urlLink Adobe Encore DVD .  At the time there was much brouh-hah-hah (sp?) about how Apple was writing competing software ( urlLink Final Cut Pro  and  urlLink DVD Studio Pro ), and that because Apple throws around such weight with Apple Users it becomes uncompetitive for Adobe to make competing software.  Fine.  That makes business sense.  So while I'm saddened that these great Adobe tools won't continue to be available to Macintosh users, I'll learn to live with it. 
 But then today, Adobe announced a new product -  urlLink Adobe Atmosphere .  Basically, it's a third-party plug-in that allows you to interact in a 3-D environment (that's Adobe Atmosphere Player) and an authoring environment for 3-D settings (Adobe Atmosphere proper).  The problem - both the player and the authoring tool exist only for Windows machines using Internet Explorer.  To quote from a recent  urlLink C|Net Article : 
 The initial version of Atmosphere works only with Microsoft's Windows operating system and Internet Explorer browser. Bahman Dara, senior product manager for Adobe, said the company is considering support for Apple Computer's Mac operating system in future versions of Atmosphere Player, but "it doesn't look like the audience is big enough for version 1." 
 So not only can I not develop these new, 3-D environs on my trusty Mac, I can't even view them.  In fact, I can't view them on my Window's machine easily because I long abandoned IE as my browser of choice (a whole 'nother rant there, let me tell you).  So while Adobe claims their aim is to "make this (3-D interfaces) reall accessible," they've made a program that forces the user to use a specific program on a specific platform  only .  This seems to be a choice the trades accessibility for profitability. 
 So this trend disturbs me deeply - both personally as a Mac user and generally as a supporter of computing accessibility and choice.  In the past I've praised Adobe - they represent, to me, one of the first companies that consciously attempted to standardize their programs and user experiences across multiple platforms. Yet now they seem to be only concerned with those who use the lowest common denominator - if you chose to show initative and use your machine in a different way, you're out of luck.  How ironic that this comes days after Apple released their vaunted iTunes program and iTunes Music Store for Windows users; now I can enjoy virtually the same music-listening experience no matter where I might be situtated.  I just wish Adobe would show me the same courtesy. 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Blood: Who ever thought it could be so artistic? 
 First off, I won't put any spoilers in this post. Friday I got a chance to see Quentin Tarantino's latest masterpiece,   Kill Bill  . I'm not one for scary/gory/cheesy blood lust films so I was a little apprehensive with my pre-viewing feelings about the movie. That all ended about 5 minutes into this work of theatrical art. This is the best movie that I have seen in a very long time. I thought that there was no possible way that Quentin Tarantino could ever make a movie better than Pulp Fiction. Well he did that and then some. The movie goes through stages that are very unpredictable, from the almost western theme, to 20 minutes worth of anime, into Crouching Tiger like 30 minute fight scenes. The use of blood in the fight scenes is as graceful as ballet, but it was almost  funny  in a very sinister way. Ashley was begging to leave an hour into the movie, she had to go outside because she couldn't watch anymore. All in all, I can't wait until the second half of the film is released. It's not for the faint of heart, but once you get past the 'gore' its a great film. 
 


-Rob 

     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Multimedia is Saved? 
 A while ago, I published a summary and critique of the recent Microsoft vs. Eolas ruling, which endangered the basic technology which drives the delivery of Flash, Quicktime, Shockwave, and other multimedia content.  Well, much news has occurred over the last 24 hours. After weeks of silence, companies like Microsoft, Macromedia, and Apple have come out with work-arounds which they believe will enable websites to continue functioning after Microsoft has rebuilt their browser.  I just used the Macromedia work-around and it seems to work great. Now a word of caution - reports don't have the new IE browser coming out until later this year, so don't there's not a lot of pressure to change anything till 2004. But I thought it would be useful to have some resources ready for any, in case they need to make changes. 
 Who should be thinking about this?  Well, anyone who does web development.  Basically, any HTML and XHTML code that uses the OBJECT or EMBED tags. And those are popular tags... 
 UPDATE 
 I literally went to post this entry and then check the news, and there's more developments.   urlLink News.com  just announced that Eolas has just filed a motion to stop the distribution of Internet Explorer until Microsoft begins licensing the plug-in process from Eolas. Things just keep getting better and better - stay tuned! 
 UPDATE UPDATE 
 Macromedia just posted a nice presentation that shows the changes in the next Internet Explorer and what they and Microsoft are doing to address them.  Also, it shows the use of their Breeze presentation software, very slick.  Anyways,  urlLink take a look...  
 Active Content Resources 
 
  urlLink Microsoft MSDN Developers' Page  
 Details Microsoft's upcoming changes to Internet Explorer and links to other providers 
  urlLink Macromedia's Active Content Center  
 Currently contains a concise description of the problem, as well as a set of JavaScript work-arounds for Macromedia content 
  urlLink Apple Computers  
 Instructions for including Quicktime Content 
  urlLink RealNetworks  
 Information for working with Real Media Files 
  urlLink News.com  
 An overview of Microsoft's announcement and related issues from C|Net News 
  urlLink Coverage by Web Blogger Jeffrey Zeldman  
  urlLink Coverage by Sidesh0w.com  
 

     

    
</post>

<date>05,August,2004</date>
<post>

	 
       So This Is Progress? 
 Okay, gripe of the day:  why is it that software has to have so many updates?  For example, my favorite suite of software is Studio MX from Macromedia - indispensable tools for my job as a web developer.  But every year, they come out with a whole new pile of products.  Two years ago it was Flash 5 and Dreamweaver UltraDev 4.  Last year it was the whole Studio MX line.  A few months ago, brand new Studio MX 2004.  And each upgrade costs money.  And this is but one of many, many examples... 
 So why does this burn me?  Two reasons.  The first, of course, is the money - constant outflux of money. If I only relied on one or two programs, perhaps I could keep up.  But how many professionals use only one or two programs?  I'm usually using one or two  at a time . These costs can add up - I could easily see a single workstation taking more than a few thousand or so a year just to stay caught up with the upgrades. And when your a business with scores of computers?  Forget about it... 
 My second gripe is this - do they realize how long it takes to learn their new program?  I mean, we're not talking one or two small changes here - sometimes the entire interface is redesigned.  For example, in the new version of MM's Flash program, not only are there several new interface "features," but the entire scripting language has been overhauled (and this on top of a minor overhaul last year).  How does one fine the time to keep up with the changes?  Again, it might not be so bad with one or two programs, but six or seven?  The whole point of buying these programs is to be able to  do  work, not learning  how  to do work. 
 I know, I know - even software developers have families that have to eat. And obviously there's enough people out there willing to buy the upgrades on a yearly (or less) basis.  But the divide is growing.  Last year several studies found that most US businesses were still using Windows 98 and Office 97 - programs that are now five + years old.  Why?  Price of upgrades.  Many chose to suffer with less (which had been working just fine, thank you) than risk the costs (money and time) of more. Which I think is great - democracy in action.  But at the same time there's the constant pressure to know the latest in the latest by yesterday, but some of that isn't possible without the newest baubles and bangles (or at least it isn't as "convenient" without them).  So what's a guy/gal/technoratti with no cash to do? 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       The Matrix Wants You! 
 Okay, before I go ranting - Spoiler Warning if you haven't seen  The Matrix: Revolutions .  Then again, with the high first weekend and then 66 percent drop off at the box office, is there anyone who didn't see it? 
 Anyways, after a week of letting the movie stew in the back of my meat-brain, I started having some interesting ideas.  Actually, I woke up from a Matrix-themed dream (only the second one ever, go figure) and had some thoughts about the nature of the "new" Matrix shown at the end of the movie.  Just in case you've not seen the movie but want to keep reading, here's the run-down.  Neo sacrificed himself in order a)to  save the Matrix from the viral agent Smith, b) to save the lives of the humans infected by said-same Smith, and c) to broker a peace with the Machines and thereby save Zion.  All he had to do was fight Smith to the edge of death, let Smith infect him, and then join with the Source (machine database evermind server thing) after he was infected.  He dies (apparently), the Matrix recompiles, the machines leave Zion, and the last scene has the Oracle and the Architect overlooking a beautiful, more colorful, reborn Matrix simulation.  At least that's how I saw it. 
 Now the interesting thing at the end of the movie is that they imply that folks who want to leave the Matrix will be allowed to.  So that has me thinking several thoughts.  First - does that mean that they know the Matrix is "The Matrix?"  Did Neo somehow make them aware of their situation so that they can make the choice to stay or go?  Or will things run as before, with people in Zion allowed to enter and free the enlightened members of the human race?  And if you chose to leave, can you chose to go back?  And how many people can the Machines free until the Power Plants are no longer efficient?  It seems to me that they still need the power the people provide.  I can see it now - a massive PR campaign designed to keep you in the Matrix.  The Matrix becomes a computerized Disney World, a nicer world than the gritty, grimy, sweating realtity of Zion or the dark, bleak, lifeless surface of the Earth.  Free food, free boarding - all you need to do is give up reality. 
 In one of the philosophy books to spring up around the first Matrix movie there was an interesting essay about the ethics of choosing to stay in or go back to the Matrix, ala Cypher's betrayal.  The essay positted, as I remember it, the idea that it is inherently unethical (and hence evil) to remain strapped to the Power Plant.  I'm going to butcher the arguement, but I believe it said that to live in or choose a false life is wrong, because the pursuit of truth is integral to living an ethical life.  But all those people in the Matrix simply can't choose to leave - there seems to be no ecosystem to support them anywhere on the planet.  I mean, it all looks like Detroit on a dark fall night!  So if they made the "ethical" choice, they'd be signing a mass suicide order.  Of course, the machines also might get pissed off at this point, having lost their batteries, but that's another essay (or at least another paragraph).  So do they REALLY have a choice? 
 And what about our scruffy friends in Zion?  Can they continue to waltz in and out of the Matrix freeing people, screwing up physics, and living like dark superheroes?  I mean, what's the Machine world got to gain from letting these people in?  They just leech off the power and use resources without contributing anything.  Or do you find yourself making a Matrix Customs agency that folks can use to come in and visit, perhaps to be recruited to join into the Matrix?  I mean, don't you think some people might have gotten tired of the whole living-underground-and-struggling scene?  Isn't the Matrix the ultimate welfare systerm?  All they want is your body heat, after all. And besides, now you have a colony of computer hackers - can you really find a way to keep them out? 
 On a side note, another interesting idea to arise out of the last two Matrix films is that the machines themselves aren't so happy with their own lives.  First, you start meeting characters that refuse to be deleted and use the Matrix as a virtual no-man's zone where they can continue existing.  Sure, most of these seem evil, but at that point all machines (and here I mean both machines and virtual AI entities) seem evil.  Then at the start of the newest movie, you meet the sweetest little Indian girl, the product of two Indian AI programs that were forbidden to reproduce.  They're sending the girl into the Matrix to survive, for she has no future in the Machine World.  Now these seem like very nice programs just trying to have a little home life - not such a big jump when you realize that the AI's are just like natural intelligent entities and thus probably want the same general goals (health, happiness, fulfillment, and the continuance of genes/code).  So now the Matrix is not only the prison designed for the human race but also a virtual Casablanca, where bad programs can go to hide.  Seems like the machines want the same freedom of choice that the humans want.  In fact, the Machine world seems to have a slow epidemic of "choice," ranging from the cute Indian family and the evil Merovigian to the Oracle and her assistant Seraph.  I mean, isn't that really what Neo died for - the ability to choose? In that way, isn't he a Machine savior as well as a human one? 
 So that's my thoughts.  I'm sure I'll have more (you know me, I never shut up), but I'll just stop for now and let it stew... 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Various Thoughts 
 My, haven't I been the bad blogger.  Tough to walk around and not have anything to say!  But I thought I'd just drop a short note to make sure I hadn't forgotten how!  So here are some short, and hopefully interesting, thoughts: 
 
  The Matrix, Part Trei  - Well, my wife arranged for a night of babysitting and off we went to see  Matrtix: Revolutions .  Without giving anything away, it ends in exactly the way it would end - no big surprises, no deus ex machina.  And with that said, I have to say that somehow it was an ending that felt hollow.  Not that there's anything major unresolved (and thus sequel-able), but the ending just didn't have the resounding feeling of satisfaction that I desired.  In fact, I was surprised that I desired this "satisfaction," but I know I'm not alone; my wife made the same observation.  Oh well, it had to have an end sometime - all movie franchises do 
  Screaming Fruit  - No, this isn't some report on foodstuffs that were genetically engineered with mouths - it's a congratulations to Virginia Tech for building the world's 3rd fastest supercomputer out of 1,100 off-the-shelf Macintoshes.  Currently, it is only the third comupter ever to work faster then 10 gigaflops.  This should be interesting, and not just for Apple; this success is reflected in other new supercomputers in the annual top ten built by network Linux boxes.  I've always thought that the power currently put at our collective computing fingertips was more than we imagined, and these off-the-shelf supercomputers seem to bear that instinct out.  How many years until you can build such power into your basement?  And what is possible, both good and evil, then?  Learn more  urlLink here ... 
  The Fear of Believing  - Got to finally see  Finding Nemo  this weekend (the birth of our son interrupted my wife and my first, finely-laid plans), I have to say that I was blown away.  Entirely enjoyable, technically impressive, can't wait to buy the DVD for myself.  But there was a nagging analog fear at the back of my skull once I started looking at the DVD bonus material - in one of their first rounds of tests the directors asked the Pixar research department to mimic small movies of underwater life.  Scary part?  They reproduced it too exactly!  This is not my criticism, but rather that of the directors!  They had to tone down the realism so that the movie could remain a stylized cartoon.  This is pretty amazing - reproducing nature digitally, and in motion, is very, very difficult.  It certainly seems that it won't be long till everything we see can be reproduced in a CGI environment, obliterating the line between "real" and "imagined" experiences.  On a more positive note, I think Pixar might be the single most enjoyable place to work.  That, or they have a really good PR firm working on the DVD!  I am certainly jealous 
 
 Okay, that's it for now - I'll try to be back soon... 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Speed - Is It Good? 
 Okay, first of all this is not about the strengths of "uppers" or their abuse.  Instead, this is a open-ended ponder on whether having something faster equals better.  As I write, my intrepid class of college students are playing Photoshop Tennis (or at least my version of it), a non-stop game of trading images where they only have ten minutes to modify a given visual composition.  Five turns at ten minutes each, that's all they have to make their case as digital artist supreme.  Cruel, no? 
 But it does get to the heart of our faster-is-better culture.  I mean, fast food, movies on demand, information via Google, it seems that there is little we still have to wait for.  Off the top of my head, all I can think of is traffic to move and children to be born (okay, that was a little flippant).  But seriously, in a world where Fed Ex becomes an industry bellweather and email is the communications norm, when is it advisable or desirable to slow down? 
 Or even more importantly, do our lives have room to be slowed down?  Every day at work I have a pile of things to do, plus everything left from the days before.  Every weekend I have a honey-do-list that grows exponentially.  And my newborn son ensures that I get less sleep each day, so that I have more minutes to do more stuff.  I'd take a vacation, only I'm that much behind because of it... 
 So I urge you to slow down every once and a while - be a rebel.  Smell a flower, go for a walk, play a round a golf, watch clouds float by.  Anything to break the cycle and catch up with yourself.  And pity my poor students - they're racing around like manics right now! 

     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Anger in the Ranks 
 Before blogging off, my apologies for not getting anything written last week - I was ready to go on the train of thought below when floored by a viral bug (real, not digital).  And I mean floored - lying on the floor (or couch) shivering and wondering if I had one of the plagues of Egypt.  Very nasty. But here I am, almost hale and hearty, wandering again... 
 So anyway, on with the blog. As some of you may know, I teach digital art classes for the Ohio State University. Traditionally I give my students a paper to write - keeps them honest to have to put their thoughts down on paper, even if we're trying to make visual art 99 percent of the time. So this quarter I asked them to sound off on the current state of copyright and Digital Media.  Very open-ended; the papers could range from DVDs to MP3s to Internet Images to whatever. What was interesting was what I got back 
 What I expected to read was that a few students support copyright issues, a few admit to downloading music and know it's wrong, and a few railed against the RIAA machine.  What I got was near 100 percent agreement that while they (the students) had been breaking the law by either downloading images or music for personal use, they were justified in stealing from "the man." I had never guessed that personal opinion against corporate copyright holders in general, and music companies in particular, was so vehement. The nicest reactions were that they had it coming, and the harshest was, "hey, I bought my concert T-Shirt, so get off my back!" Out of a class of 13, I had one (yep, one) student that supported the law as it stands - everyone else saw it merely as a tool for corporate America. 
 This leads me to wonder why, why, why the RIAA and other companies think that suing students will eradicate file sharing and increase CD sales.  Here's my news flash for them, based on my small sampling:  no one wants to give you money! They don't want to buy your CDs, see your concerts, or support your artists so long as they see the money they're spending going to you.  The students seemed sympathetic to the plight of individual artists, regardless of media.  I mean, they would like the protection copyright laws afford once they leave school and make things requiring protection.  But the overriding opinion is that the laws were the tools of the corporation, not the artists.  They saw the artists as getting screwed over, as screwed over as they feel every time they buy a $15 disc. So anything that they can do to screw "the man" back is justified, be it legal or not. So my conclusion is this - while the DO like getting things free (music, software, etc.), greed is not the only motivation. They are also motivated by a man vs. machine, Robin Hood vs. King John myth where every man is entitled to subvert the system and get a little back in return.  If I were in the music, movie, or DVD business, I'd be worried - my customers are pissed 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       The Need for Choice 
 Let us hope that this is my last, last posting ever on the Matrix series - I mean, you'd think I was obsessed or something!  The problem is that for all the disappointment felt for the last two movies, the franchise still asks more interesting philosophical questions than most films.  Oh well, what can you do? 
 Anyways, I was pondering the end of the franchise, specifically why Neo had to be infected with Smith before becoming "one" with the Machine Mainframe. I mean, if all you needed to do to reset the Matrix was to link into the Source, why have the fight with Smith at all?  I mean, would Smith come back in the new Matrix? Was the danger to the Machine World so great that they would make a deal with Neo?  Or what if the Machine World (and Matrix) needed something  from  Smith...? 
 Here's my thinking - what if the Machine World needed to understand what it was to make a choice?  I mean, through much of the second film all the machine entities were saying, "We're all here to do what we're all here to do" like some kind of automaton. What if they need a listen in "choice," that human trait which is harped on throughout the franchise? Certainly, having Smith compile whtat it means to make choices and then absorb his knowledge through Neo seems simple (and sly) enough. 
 Now I know, there were already machines that seemed able to make choices - the Oracle, renegade programs, etc.  I mean, all a renegade program is is a program that  chose  to flee and not be deleted. But that means that most, if not all, of the programs that can make choices are hiding in the Matrix, doesn't it? So what if the whole point of the Oracle's risk is not only to save Humanity, but to save the Machines by giving them all the power of choice and control over their environment? 
 Now I'm not entirely sold on this idea myself, but I find it interesting for two reasons.  First, I like it because it gives the whole Smith sub-plot a reason to exist - he may be evil, but it's a necessary evil - like a bite from the fruit of Knowledge.  Second, it helps explain the end to me - the little girl/program who is under the Oracle's care suddenly has the power to modify the Matrix (by creating a specatcular sunrise).  Since when did she get that power?  What if it's a by-product of the choice that Neo/Smith gave her? 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Okay, a Political Rant 
 Here's an admission that will shock no one (probably) - I'm pretty liberal, politically at least.  So take this rant with a dash of salt. But, here goes... 
 I Can't Believe We Might Re-Elect George Bush 
 I mean this, how can this be possible?  Consider the following: 
 
 Iraq and WMDs 
 Bush has overthrown a sovereign nation by claiming that we (America) were in eminent danger from Weapons of Mass Destruction. So his thinking goes, "shoot them before they shoot you."  Basic self-defense. Except that now, after taking over Iraq and capturing Saddam Hussein, we haven't found one Weapon of Mass Destruction.  Zilch, Na-Da, Nothing.  In fact, the person who until recently was overseeing the search now says that he doubts they had any at all and that there should be an investigation.  Bush claims that this doesn't matter; Saddam Hussein deserved to be overthrown. And while I can't argue that, can I point out now that the case he and his administration made for risking our soldiers overseas (for a second time in his tenure) was based on lies? Didn't the Congress try to impeach the last President because he lied about having an affair? And we're looking to re-elect a man whose mistreats sent our entire nation to war? 
 It's the Economy, Stupid 
 Or so went the famous phrase that drove the Clinton 1992 campaign against Bush Senior. So let's take the same issue to Bush. He will be the first president since Herbert Hoover (of Great Depression fame) who overseas a loss of jobs over his tenure. And while the most recent unemployment numbers showed less people applying for benefits, the slight reduction is attributed not to new jobs being created but rather to people  giving up  their search for employment. Purchasing by companies still remains sluggish, the Stock Market is finally approaching it's levels prior to Bush's administration, and the future economic outlook appears murky. So where's the economic benefit of re-electing President Bush? 
 Back to the Deficit 
 Republicans claim to be the party of fiscal conservatism; they blame Democrats for encouraging big government and run-away spending. Yet Bush inherited a Balanced Budget from Clinton and the prior Congresses.  You know, a budget where you spend as much or less than you take in? Between wars, tax cuts, and other new programs, Bush has catapulted the nation back into deficit spending, running up billions of dollars in debt that our children and us will someday have to shoulder. Doesn't seem very responsible fiscally. 
 Bill of Rights?  Right... 
 Using the tragedy of September 11 as an impetus, the Bush Administration has put forth programs which curtail or remove basic rights of privacy. Not since the run-away days of J. Edgar Hoover has it been easier to listen to the conversations, read the mail, and generally spy on the typical citizen. Visitors to our country are now videotaped and fingerprinted into a giant database, just for wanting to visit the US. Now I do believe that some of the changes may be necessary - no one wants another attack like September 11.  But I'd rather that we engage in a national debate about where the boundaries between personal privacy and public interest are, rather than allowing the Bush administration to write blank checks without informed public discourse. Now I should probably criticize the Congress for rubber-stamping such proposals, but that's another blog entry. 
 It's Good to be The King 
 Obviously, it's good to be friends with Bush and his posse.  Energy companies have benefited from their friendship - environmental restrictions have been lifted and new, pristine wilderness is being opened up for exploration. Prior employers have benefited - the Haliburton company (prior employer of V.P. Dick Cheney) alone has collected millions (dare I say billions?) for providing services for the war on Iraq. Even baseball benefits - former Texas Rangers' owner Bush pushed for greater control of athletes and steroids use in his recent State of the Union (who knew that athletes and steroid use was a  national  issue?). Makes me wish I was a Texan CEO... 
 
 And these were just issues I've pulled off the top of my head; I am sure that I could triple the list simply by reviewing the last four years and seeing how angry I get. And yet current polls have Bush winning or in a dead heat with Democratic contenders, and this is without Bush even dipping into his Millions of Dollars of election war chest. I believe there is a huge likelihood that he will be our President for another four years. So what does he have to do to get the country to want a change? 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Retro Tech is Cool 
 So I'm watching  Monsters, Inc.  last night (I'm becoming a Pixar Junkie) and I noticed that while
the movie portrays an alternative world that is at least as sophisticated, if not more so (I mean, they can harness energy from screams!), all the machines had a retro, analog look.  Check it out - the machines that fetch the doors have simple, analog light displays,  the meters recording each scream-canister's volume is a simple thermometer/pressure gauge, and most of the interfaces are dominated by dials, levers, and other analog controls.  The most sophisticated technology, it seems, is the Monsters' Training Room, dominated by closed-circuit television and puppetry.  And while the television ad for the power plant alludes to Virtual Reality training, you never see it in action. 
 Okay, so I'm being a bit obsessive by tearing apart the technology of a fictional world, but look at other recent or popular movies.   The Matrix  series, the  Aliens  series,  The Terminator  series:   all of these visions of the future feature grungy, messy, greasy machines and technologies that look as much like the 19th century as the 21st and beyond.  And I'm not talking about the machine portrayed themselves (I mean, I haven't seen flow-metal humanoid robots lately, have you?), but rather the overall style the technology portrays. 
 So why is this?  Is grungy cooler than clean? Or is it that the more tactile, analog, dystopian look gives a more "real" feeling?  I mean, look back to the classic  2001 . Even though the technology doesn't necessarily work well (HAL being one big opps), it looks clean, shiny, and new. And while I love watching that movie, the "clean" look seems both idealistic and dated - like drawings of rocket cars and Jetson-style furniture. Perhaps today's grittier sci-fi look will be a style that seems dated in another 30 years or so? 
 Of course, it could also be a reaction, a visual symbol of our collective loss of faith in technology.  In the 1960's people actually believed the technology was the answer: feed the world, eliminate both monetary and social classes, and engender a new Eden. Now we've lived with decades of technology's results: tons of pollution, spam, corporate predidation, and an overly-sped-up pace of life (to name a few results).  Perhaps we believe in the dystopic future because we've lost faith in the ideal? 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       One More Loss in the Jungle 
 A moment of silence is asked for  urlLink Webmonkey , the acclaimed online tutorial site for all things HTML-y.  Many folks I know, including myself, considered Webmonkey one of the premier sources for how-to info on programming for the web, but alas it has fallen by the Dot-Com wayside.  As reported by  urlLink Wired.com , the folks at Webmonkey announced that the site was closing down after a new round of cuts from it's parent company, Terra Lycos.  And now an incalcuable amount of browser bookmarks must be updated. 
 But seriously, this is another in a long strings of online pioneers to fall victim to the Dot.Com bubble burst of the last few years.  Magazines like  Red Herring , development companies like  marchFirst , and even browser companies like  Netscape  have turned into dust and memories as companies downsize, consolidate, or abandon online projects.  I certainly hope that the Internet economy and design fields find an equilibrium (if for no other reason than personal job opportunities), and I bemoan the lost of such a stalwart as Webmonkey. 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Does Analog Still Hold It's Charm? 
 As the many (or the very few) who follow this blog know, one of the reasons I started it was to ask a simple question -  what happens to us as creative individuals when the world slowly moves from traditional, analog-type technologies and media to newer, digital-type media? By analog I would describe anything that creates a physical representation of an experience, expression or event; for example, a vinyl record is a physical object which mimics the sound waves with actual groves in it's surface. By digital, I mean things that can be broken into abstract, on and off bits and reproduced through computers or electronic means; using the music example, a CD is digital because it is an abstracted data stream which holds the information recorded in the studio, usually by analog means (mics and such).  Now, a hidden assumption of this blog is that the analog objects and digital objects are significantly different, that there are positives and negatives to using either digital or analog means in expressing yourself.  But what if that's not so? 
 Here's what I mean.  A friend of mine in Chicago is a composer of electronic music.  He uses a variety of means (live recordings, drum machines, synthesizers, etc.) to compose ambient music.  But amid all his high tech hardware (and believe me, he has quite a set up) is a pre-digital, vacuum tube amp.  He keeps it in the studio to push compositions or parts of compositions through to re-record; he contends that the vacuum tube amp affects the sounds produced, giving them a deeper, richer sound not found in music made in a purely digital way.  It's his way to give his music a depth and soul that he feels would be lacking if he composed only using the computer. 
 Now I've heard such arguments before, usually from guitarists who swear that vacuum tube amplifiers are far superior to newer, solid-state equipment.  The idea is that the imperfections or quirks of the older technology actually adds to the listening experience.  And I went along with this assumption, for I believe that there is something different in being digital versus being analog, and that each "class" of technology had it's positives and negatives.  But last night, I began teaching a new class on Digital imaging, and one of my students happens to be a musician who works only with analog equipment. But rather than give the standard reasons I've heard about the quality of the sounds created, he instead asserted that there was no real difference; all analog "qualities" could be reproduced in a digital environment, if one chose to do so.  Rather, he remained analog because of personal tendencies.  Furthermore, he felt that a musician/composer should stick only to one or the other "classes."  In other words, he couldn't see a reason for incorporating analog equipment if you're a digital composer, and visa versa. 
 So what if he's right?  Have digital tools (for music, art, film, etc.) become so sophisticated as to remove the need to have analog technologies lying around? Does that mean that folks that hold on to older technologies (from the painter who uses canvas to the musician who uses vacuum tools) simply do so from habit?  Or fear of the new? Or a mistaken commitment to tradition?  I personally hope not, but it would be foolhardy not to examine the possibility... 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Long Rumination 
 Yesterday I got an email from a student at Ohio State with questions on my opinions about Digital Photography.  This gave me license to explore some of the ideas about digital vs. analog art making that I've touched on here (always dangerous to give me license!).  And I was so happy with the results that I thought I'd paste them here - Jeff 
 Original Questions 
 Hi myname is (name withheld) and am a student at the ohio state university and I have been asked to do an interview with someone on my topic for a research paper. My topic is digital photography. I was just wondering if you could answer 2 questions for me. 1. What do you think of digital photography? 2. Would you say deleting pictures can delete our history? thank you so much for your time. Could you please respond ASAP. thank you again. 
 Lengthy Answer 
 Hmmmmm, let me ponder.... 
 1)  What is Digital Photography? 
 Well, that one, one would hope, is the easy question.  Short answer:  digital photography is the capture, construction, and/or modification of photographic elements into a digital format.  
 Now like any good answer, this has some hidden assumptions.  First, you'll notice that I have implicitly expanded the definition from the "capture" of photographic (or "light written", from the Greek "photo-" for light and "graph" for writing) images to the "capture, construction, and/or modification."  While this has always been a hidden proposition of traditional photography, the tools and technology for constructing or modifying digital imagery makes it a much more active and explicit endeavor (see the answer to question #2 for more on this). 
 Second, the transition from an analog format (traditional negatives and printing) to a digital format greatly changes the nature of the photographic image.  Because it is digital in nature (binary, on/off data, numerical data), the photograph can be copied, transported, shared, etc. irregardless of what object (art or otherwise) you create for it.  In traditional photography, to have a "print," one has to have access to the original negative and often has to be printed by the original artist/technician.  For example, Ansel Adams would not only take the photographs but use the darkroom to accentuate areas of his photographs; a print made from the original negatives but by someone else doe snot reflect Adams' original aesthetic intent.  The variable nature inherent in the analog process makes perfect replication on any art object virtually impossible - a situation where one tries to come "close enough."  However, once a digital file has been completed by an artist, it can be copied with complete digital fidelity ad infinitum.  Because the file is broken into exact zeros and ones, the data contained within a digital file can be copied without any loss of data.  Indeed, the image itself can be translated into a variety of formats (CDs, Print-Outs, Online Images, etc.) and shared a multitude of times while remaining true to the original file created by the artist.  In fact, I would posit that this calls into question the nature of "original" - what is the original piece of art created and what are the copies?  I mean, is the Photoshop or digital file the piece of art and the print outs are the copies?  And what if I make a back-up of the Photoshop file?  Now I have two files that contain the exact same data.  Which is the original file?  Is that a meaningful question?  I've long contended that digital media (photography, music, what have you) call into question basic artistic assumptions about "original" and "copy," and since our current valuation of an art object (both monetarily and aesthetically) depends heavily on the original nature of the art object, this can have subtle but deep implications to our current understanding of what a piece of "art" is.  For example, paintings are more valued the posters of the paintings, or a carved marble sculpture is more valuable than cast plaster copies of the same sculpture.  But with digital imagery, it is easily imaginable to have the first "printing" from the digital file be exact (or virtually exact) as the 2000th printing.  So which one is more important? 
 2)  Is deleting pictures deleting our history?  Never thought much about it.  I'd have to say yep, but I'd also say that this is no different than the destruction of any "matrix" which enables replication of an image.  By matrix, I mean a thing (digital file, copper etching plate, celluloid negative) which enables an artist or technician to make high-fidelity copies of an art object.  So a destruction of any matrix, digital or analog, is the destruction of the continued representation of that reproduced object.  Now because there is a historical, biased connection between photography and "truth," one would be tempted to say that the destruction of a digital or analog photographic matrix is the destruction of a prior documentation of a historical truth.  This perception that photographic imagery represents "truth" is actually, to me, the more interesting question... 
 Walter Benjamin posited that photographs were, in effect, windows which transport the viewer into a prior place in time.  That is, when I look at, say, a Matthew Brady photograph of the Civil War, I'm looking at a historically accurate snapshot of that one moment in time.  The image fixes that moment in perpetuity, or at least until the photograph and it's matrix (the negative) are destroyed and thus lost (i.e., deleting history).  However, this opinion is based on a false premise, namely that the taking of the photograph is a purely mechanical process that is neutral.  But the photograph is composed, developed, and printed by a human being, and all of these processes can be manipulated by that said same human.  Again, go back to the Matthew Brady photograph.  The implication of the photograph is that it shows exactly what that moment in the battlefield looked like.  But it is well documented that Brady composed the bodies of the dead to make more aesthetic statements; already, before the shutter has been clicked, a level of artifice enters as Brady rearranges limbs, moves bodies, or adds/subtracts props.  Next, light values can be changed during the developing process; depending on how the chemistry is applied and manipulated, a day shot can become a dusk shot, dusk a night shot, or the opposite.  Likewise, within the darkroom Brady can manipulate the printing process to accentuate or de-emphasize certain elements of his photographs, or even combine multiple negatives to make larger composite photographs.  But because we (you, I, the culture at large) has in the past had an implicit belief in the "truth" of the photograph, we believe what Brady (or any photographer) presents, irregardless of what changes he/she may have made. 
 Now I don't think this is a bad thing, just a mistaken impression by the public at large.  Interestingly, it is the offshoot of photography, film, that has helped disabuse people of the impression.  Movie FX, both visual and otherwise, has grown extensively over the last 40 years or so, coming to a point where realities are created which we as viewers know are patently false - ALL modern Sci-Fi films come to mind.  Yet because we allow films a certain leeway, we assume that the images we see are true, but only within the confines of the film.  I get scared every time I see the "Aliens" movies, but I don't expect to stumble across the acid-spitting villains once I walk out of the theaters (except, maybe, during Halloween).  Slowly this understanding of the artificial nature of photography is percolating up from film.  People don't necessarily "believe what they see" anymore, or at least not to the same extent as 50 or 100 years ago. 
 So what does this mean to digital photography and our understanding of "history."  Well, digital images are many times more "malleable" than their analog cousins.  In analog photography I'm confined by many physical limitations; in digital photography I'm confined only by the resolutions of my images and the computing power of my tools.  A good story illustrating this can be found in the story of the making of "Finding Nemo."  One of the original assignments given the animators was to animate from scratch clips that replicated some footage of aquatic flora and fauna (things like whales breaching the ocean, fish swimming, coral reefs).  The idea was to see how close the Pixar animators could get with the tools they already had.  To everyone's surprise, the animators had both the skill and the tools to create virtual duplications of the video clips - trying to tell which clip was genuine became impossible.  In the end, they had to limit their animators so that they didn't create an animated movie that was "too real."  Stories like this illustrate that digital photography, at least at the highest levels of film, can create virtual objects, scenes, etc. that are indistinguishable from real life objects.  So the question becomes, how do you know what is "true" when everything can be created virtually (or in other words, "faked)? 
 How does this effect our concept of history?  Well, my gut tells me that it subtlety but significantly changes it.  As long as we assume that a photo is "true," the digital techniques enable us to reconstruct our history.  For example, I spent Xmas with my in-laws in Texas, and one of the goals of the stay was to take a family photograph.  So all the family shows up, dressed well and nicely groomed.  All, that is, except for one brother.  He got called away to do some work in another city and couldn't make it.  Now this irked everyone present, since there are few times when we can gather the family together, and thus he should have come to be photographed before leaving the city.  His response?  "I'll just Photoshop myself in later."  Assuming he has the skill and tools to do that, what we could have left as a family heirloom is a photograph showing everyone in the family, in a pose/situation that never truly existed.  It is a fictional photograph, not a historical one.  But again, unless you were there, how would you know?  So it seems to me that digital photography doesn't only open the possibility of "deleting" history (which, as I said before, isn't an issue confined to the digital only), but it also greatly expands the possibility of "constructing" history, manipulating the appearance of events so that a believable but entirely fictitious memory can be created.  Interesting, if not a little scary, but this is what happens when digital tools make visual recording a malleable material... 
  Hope this helps, Jeff 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       A Little Humor 
 Hi all - nothing intense today, just a link to a pleasurable comic,  urlLink Foxtrot  - Enjoy! 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       RIAA Should Be Worried 
  urlLink Click here  to read a recent article on Research done at the University of North Carolina.  The study states that despite many espoused opinions to the contrary, file downloading makes little or no effect on record sales.  As you can imagine, this has outraged many RIAA supporters who clearly state that declining sales is the fault of file sharing programs, starting with Napster and moving down the line.  Never mind that the current state of the music industry is stagnant, focused only on first week album sales while sacrificing quality and variety.  (Boy, that sounded mean!)  Far be it for Record Executives to take the blame for the industry's downturn; it must be their target customers fault! 
 This is yet another example of how passing the buck makes the music industry miss the real issue - folks don't trust you, don't like your product, and are looking for alternatives.  They don't like being bullied, prosecuted, or assumed to be criminals.  One cannot alienate potential customers  and then complain  that they don't buy your product. 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Touch Me, Feel Me 
 Now if that doesn't get your attention, nothing will... 
 Was thinking last night as I did some personal design work about the importance (or lack of) experience in the making of imagery.  By experience I don't mean training, education, wisdom gained from a history, etc.  Instead I mean the act of "experiencing," actually feeling or doing something that then is portrayed or reflected in the resulting work.  For example, currently I'm teaching Digital Imaging class using Photoshop.  To get the students going, we'll use a variety of sources - scanned images, digital photographs, online images, etc., to build our resulting images.  Now for the sake of expediency, many (if not most) of the students rely heavily on images found online; copyright issues aside (I believe that since it stays only as a classroom project, it falls under the Fair Use Guidelines, but I'm no lawyer), I wonder if this is a good thing.  Here's why - I'm starting to wonder if the act of collecting and constructing your source images (either analog or digital formats) is in itself an important part of creating authentic and personal work. 
 In the old analog world of making, this is a rather mute point.  Except for rare exceptions, artists had to have an intimate relationship with the work they created and the elements that went into it.  I mean, nothing is more intimate that having to go out and draw/paint/sculpt/photograph/film/record your subjects right in front of you, only then to go into the studio to draw/paint/edit/compose/etc. the elements into a synthetic whole. But now one can steal or purchase images online, compose them remotely through a mouse or keyboard, and email them off for printing or publication. So what did you actually experience in the process?  The keyboard? 
 This is beginning to worry me in the long term. I romantically believe that good art comes from life experiences, from what we see, do, feel, and learn. But if I can get all my components from anywhere in the world, combine them with Photoshop without ever really "touching" them, and send them off to be in shows, magazines, or whatever without ever "holding" them, what have I really experienced? Where's the elements that reflect who I am, rather than a generic designer or artist?  Or is simply the act of composing, of combining disparate elements, enough?  Do I really need to  be  there to talk about it? 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Congratulations to Blogger 
 Okay, this may seem a little self-serving, but I want to extend me congratulations to the folks at Google and Blogger for their recent upgrade.  They have totally rebuilt both the backend interface and available frontend templates to create a more visually exciting and standards-compliant environment.  Even better, the new templates are designed by some of the current top standards-compliant designers in the HTML business, including Doug Bowman and Jeffrey Zeldman.  Quite a nice job by all involved! 
 It also comes on the heals of current articles declaring that "Blogging has Arrived," whatever that means. I mean, I started this particular blog for one reason: to have a place to self-indulgently meditate on the role of digital media in our society in general and in the life of a visual artist in particular. Sometimes, that's even what I talk about. But to say that this blog has "arrived" is to be too optimistic; I'm pretty sure that my blog has an audience of one, myself.  Can't even get my wife to read it, but one might joke that after nine years of marriage I'm lucky she can still hear me now and then. However, if you're one of the lucky few who has followed the blog, let me know with the new comments feature (another handy Blogger addition). Even better, let me know if you'd like to participate - my fondest hope has been to turn this into a dialog rather than a monologue. A guy can dream, can't he? 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       My admiration goes out to Jason Rohrer, author of a new open source
project called  urlLink Monolith .
According to Jason, Monolith is: 
 
 Monolith is a simple tool that takes two arbitrary binary files
(called a Basis file and an Element file) and "munges" them together to
produce a Mono binary file (with a .mono extension). Monolith can also
reconstruct an Element file from a Basis file and a Mono file. 
 
  "So"  you may say? Well, after links to download the
Monolith alpha version, Jason spends the rest of the web page exploring
the copyright implications of his self-admitted "philosophical
experiment." Basically, he is of the opinion (and he makes an
interesting and coherent argument) that if one uses public domain files
as the "basis" of the Monolith-ic combination of files, the resulting
file is also public domain (including any copyrighted file used as the
"element" file).  That means that you could share both the Basis and
Monolith file freely (and more importantly, legally), and later use the
Monolith program to reconstruct the copyrighted file in your own home.
Thus, you side-step any copyright or file-sharing legal issues and
return the issue of "home taping" back into the unenforceable, private
sphere where it lived prior to our discovery of Napster. 
 This is an informed and interesting take on the issues of copyright
in general and music copyright in particular. Of course, I have no doubt
that should his argument prove legal, you'll soon see an expansion of
the DMCA to outlaw the "munging" of files. But the most interesting part
of his argument is that copyright is an analog phenomenon.  To again
quote Mr. Rohrer: 
 
 
Copyrightable entities are inherently analog. Music, painting,
sculpture, writing---all of these must be presented in the physical
realm to be consumed by a human audience. Even mediums that are always
created and represented digitally, such as digital photography, must be
translated into the physical realm (for example, into a lighted display
on an LCD monitor) to be consumed. The bits (the "ones and zeros") used
in the representation mean nothing to us by themselves---we cannot
experience or otherwise consume them. 
 
 Is this necessarily true? Does that mean that the copyright only
becomes "real" when something is viewed, read, or experienced? And does
this mean that all I have to do is change the digital representation of
content (through Monolith, encryption, or similar processes). To
circumvent a century of copyright protections? Man, I'm glad I don't
work for the RIAA; this guy alone would be giving me a
3-Tylenol-headache... 
 Day Later Postscript 
 I let the Monolith project wander around in my head all yesterday, knowing that something about the comparison between the program and general encryption bugged me. Then this morning I remembered that one of the newer anti-terrorism laws (I believe it was part of the Patriot Act, but it could be something else) made it illegal to use encryption to hide or abet illegal activities (basically, if you're a terrorist or a Mafia member using encryption to ply your trade, the use of encryption is illegal). Now this law has always caused me to be suspicious - if someone is doing something nefarious, go get them for those actions, not the use of encryption. But it strikes me that Monolith, being very similar to encryption, might fall within this same general heading - the obfuscation of a file (music file) to commit an illegal act (home taping of copyrighted material). Thus the user may still be breaking a law, just a different one... It will be interesting to see how this all pans out. But for now, I'm watching a Congressional Hearing on Intellectual Property on  urlLink C-SPAN , so stay tuned... 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       A few interesting announcements have bounced across my screen over  
the last few days concerning the future of Photography.  The first of  
these was the rumor that  Pentax will leave the traditional camera  
market for digital-only photography products . This immediately made  
me sad; my first serious camera was the trusty, if archaic,  Pentax  
K-1000 , a sturdy and simple SLR camera that for decades enabled  
broke art students to buy a servicable 35mm camera.  Although  
discontinued, I know that I can still use this workhorse for my own  
work, slides, and other 35mm needs. So I was sad to think that this  
"Big 5" camera maker might leave the business. 
 And yet I still have great hope for the future of the photograph.   
For example, here is a snippet from  an interview with Derrick Story ,  
author of the new  Digital Photo Hacks  from O'Reilly Publishing: 
 
  Kathryn Barrett : Now that digital cameras are  
outselling film cameras, what sort of effect will this have on the  
state of photography?  And what about the computer side of the  
equation? If computers are necessary, could that hurt widespread  
adoption? 
  Derrick Story  : I think photography is on the rise.  
It's so instantaneous. You take a picture with your digicam or camera  
phone, and then you look at it. "Hey, that's great; I'll keep it," or  
"OMG I look I've just been beat with an ugly stick; trash that picture  
now!" It all happens within a matter of seconds. 
 You don't need a computer for digital photography, although I find  
them quite helpful. But for example, my sister takes photos, puts the  
memory card directly into a printer, outputs a few snapshots, then  
files the card away. Memory cards are cheap enough where you can do  
that if you want. I can't get her to return my email, but she is  
totally digital when it comes to photography. 
 In general, we don't write letters anymore, we hardly know our  
family history, we don't sit around the dinner table and tell stories,  
but we do like to take pictures. And thanks to the metadata embedded in  
the files, those pictures are our living history. Photography is going  
to become more ubiquitous than ever before. 
 
 Pictures as living history - this is both an exciting and  
frightening time to be a visual artist in general and one who deals  
with photography in particular. Not since the invention of the Brownie  
Camera has the cutting edge of photographic technology been easily  
available to the public at large. Can we survive this onslaught of  
photos now beginning to crest? Is there room for a professional image  
maker when everyone can make images at a whim? What is the role of a  
visual artist when everyone is already capturing their own "living  
history?"  But this blog entry is getting long, so more on this  
later... 

     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Digital  Religion 
 First of  all, apologies for not posting in so long - I have such bad writing habits, all  you have to do is look at the posting frequency to see that! Anyways, been  thinking of the two of you who still read this blog (if  that!)... 
 Anyways, I  just finished listening to Ben Bova's  Jupiter  on tape; if you  haven't read any of Ben Bova's planet series, I recommend - I've read the two  Mars books and now most of Jupiter (more on that in a moment) and have enjoyed  them all. But  Jupiter  had an intriguing quote that got  me thinking. In the book, the Earth's political entities have been taken  over primarily by reformist, born-again religious institutions. From  evangelical Christians in the U.S. to right-wing Islamic reformers in the Middle  East and conservative Buddhists in the East, the scientific, intellectual and  political discourses have taken a conservative bend. In America, the right-wing  is recruiting dogmatic scientists to "spy" on more liberal researchers at  off-planetary outposts. Thus it was early in the book where a recruiter is  speaking about why the religious governments have succeeded. He states  that religion is "digital" in nature; that because conservative religions offer  clear, black-and-white distinctions between good and evil, they propagate like  digital files - ones and zeros - efficiently and without  distortion. 
 Now, at  first this seemed like a ludicrous suggestion - the proliferation of different  religious sects alone seem to contradict the fact that religions are  transmittable without distortion. But then I started to think about the  general intellectual and political tenor of today's America. I do believe  that one currently sees the radicalization of views into black-and-white views;  thirty years ago, liberals and conservatives could actually work together  civilly in running the government.&nbsp; But now, thanks to left- and right-wing  media outlets, a deeply divided Congress, and an aggressive, take-no-prisoners  style of political attack, a person is solely left or right, democrat or  republican, religious or heretical, etc. It seems that leaders want to  either succeed or perish, to destroy their enemies in a no-hold barred political  battle royale. I have long desired leaders who lead based on the good of  the people, not the good of their political viewpoint. Alas, I have been  disappointed. 
 So my  question becomes, is this black-and-white, right-and-wrong radical viewpoint a  symptom of the "digitalization" of culture? Does the digital world - it's  inventions, software, etc. - break thinking into on and off, yes and no  answers? Where middle, grey viewpoints are illusions, requiring only a  greater resolution to show the clear, polar distinctions? A frightening  thought, and one that supports the promotion of "analog"  thinking... 
 Postscript 
 By the way, the copy of  Jupiter  on tape was damaged when I checked it out of the library, so if anyone can drop a note on how the book actually ended, that would be appreciated... 
     

    
</post>

<date>05,August,2004</date>
<post>

	 
       Born Obsolete
 
 In the last few weeks I've been struck at how much I have to do to stay up-to-date with the world in general and with web design in particular. Even though I subscribe to several blogs, watch and listen to news incessently, and read a few trade magazines weekly, I feel more and more behind the curve in what is possible or what I need to know to do my job. I mean, I was sent scrambling to the bookstore for new referrences just so I could keep a new web design up and running for testing - server books, language references, etc. 
 But it also struck me that here I am, trained to make lithographs ala the 19th century, and I'm running like the  urlLink Red Queen  to stay up-to-date. I mean, there was a time a few hundred years ago when a person could reasonably understand the intellectual tenor of the times. People like Da Vinci, Bernini, Gallileo, Franklin, and Jefferson could understand their times and make learned strides in diverse fields (look at Jefferson, America's most accomplised "Renaissance" man - architect, writer, politician, university founder and president, etc.). But now one is lucky to understand a particular niche in a particular field; specialization is the name of the game. And it seems that the learning curve gets continuially steeper as technology speeds the rate of innovation.
   So it makes me wonder if there is a limit - is there a point where the rate of change is  so  fast that the vast majority of people will  never  catch up?  Will people be  born  obsolete, subject to the whims of either the few geniuses who keep up? Or worse yet, to technology itself as it blindly drives itself forward? I mean, I have a 14 month old son, and I wonder if he'll ever be able to learn fast enough to understand even a portion of the world I am bequething to him?
   And yet, what can we do?  Become neo-Luddites and plug in?  Become slaves to the machines and plug into  The Matrix ?  (See, it has been a while since I've made a   Matrix   reference!) I don't know - seems like we can't go back, and we can't afford to go forward. Maybe the future is to be cyborg after all, carrying the Expedia encyclopedia in a CD-ROM pouch in our leg...
 
     

    
</post>


</Blog>