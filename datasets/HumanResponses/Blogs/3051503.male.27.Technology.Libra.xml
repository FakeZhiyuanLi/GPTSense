<Blog>

<date>31,May,2004</date>
<post>


           
      They were brought up together in the social fabric of Kashmir.Although from different religions but the yarn was same.The close knit association of childhood broke when both got married.  Two most educated and beautiful girls of the village..whom classmates would envy and old would praise..  For thirty years they didnt meet.They searched for each other.Villages ,cities and cultures were burnt in 90's but the love of humanity made them made.Who knew whether they were alive but they met...Was the water of Vitasta same after 30 years?  How they met? What they discussed? Where they still the same... ?  Now both widows..both aboned..what could India or pakistan give them...?? What was their future course of action..Who killed their Husbands..Indian Army or terrorists from pakistan???  To know more keep on visting this site...  
     

        
</post>

<date>31,May,2004</date>
<post>


           
      In her Faren , she was drenched in sweat . This heat she had never experience in 80 years of her life.  Wondering why environment has chanegd so radically.She was still looking for the well and the milk from her cows.  She couldnt see 2 feet ahead,Decible level of millions could disturb her. Only when some would touch her ..she would realise the body heat and make out the person.  KAKNI was not realising the heat of India ?She had shut the main door of her house only after 5.30 PM bus had come.Her son would come from srinagar in that bus.But she was wandering .why the bus was not comming??She was looking Bohb ji to return.  Why teh bus didnt come and she kept waiting...?? What happened next to her..Did Bohb ji her son return back..??  Find out the answer ..in next few months on this blog...  
     

        
</post>

<date>31,May,2004</date>
<post>


           
      Jan 19 , 1990 in the chilled  winter night ...In the dark street of downtown srinagar ,there were few people disussing over a puffing smoke.  THere eyes were bluish with fear and sweat ozzing on fore head out of fear.This sweat was frrezing on their foreheads giving their skin greenish tinge.  Some one knocked the door ..Much before the youngest of them all would get up..There were bullets screening the wooden baricade of the yard.  The wooden door on which Chelpark Ink dots once sprinled out from the chinese Ink pens where now washed with blood drops of young kid ..by nothing else but chinese made pakis supplied AK 56 rifles....   Who did it..why...whom could have this young guy of 7 yrs killed ...I kept guesing ???who was the terrorist ..Was he powerful ..if yes then what was threating him..7 yr kid!!!!  Watch out the full story soon...
     

        
</post>

<date>09,June,2004</date>
<post>


           
      She was crying in pain ,But couldnt speak .ONly way she could gather attention was by screaming at the top her voice.  She was never like that before,At her one command children of her village would run for their life.But today in the dark room,on Moonless day she was desperate for help.Her husband in his eighties was snoring in the basement.  Their house was so big that it was not at all disturbing her husband and that was what her husband was trying for alwayz.  The villagers were calling her witch and were believing that she got the curse back on her for killing her brother in law.  Some even were of the notion that the whole property belonged to her Brother in law.  Her adopted son had settled far in the posh localities of New Delhi and had no intention to come back.  Kids were happy and making merry in her courtyard without her fear.  Only the man who would visit her was the postman,carrying some one else letter and reading for her as if her son had written a letter for her.  Society had long forgotten them..  Then came the turbulance which swept the whole village .Guess what next??      
     

        
</post>

<date>02,June,2004</date>
<post>


           
      On the busy interior streets of Srinagar.. along the banks of Jhelum for 50 years this man "SOne Navidh"{Golden Barber} had shaved and cut the hair of almost all big dignitaries as well as common man.  Sitting on the Warm Skinned kusock he was an integral part of the Hindu community of that area.Although,being a muslim but he was so much inter woven that at times he had the capability to perform any religious ceremony for the brahmins.  Even people at the civil secretariat would talk of his great proffesional and communication skill.He was not just respected by elders but loved by kids.  Come 1990 amd pandits were leaving the locality and mullahs were proclaiming them to be infidels.Now he was the most deserted man.  He was upset as all his frens with whom he grew up and with whom he had more bonding than with his family were no more around him.  Streets wore a deserted look,No kriya karms any more happening... where his presence was alomost like that of  the God.He would shave the heads of pandits when some had to proclaimed as pandit after the birth (ZARA KASSAY) or when some one would die and his elder son needed to tonsure his head.  ON the 10th day of Kriya (after someones death) he was a dignitary,to perform the special custom along with brahmins.The special Hair cut (Tonsure).  As the pandits were shot down like sparrows,None would approach him for the 10th day function.But what was the reason?  Soon the circumsatnces changed and one day he had to shave off his head. Did he survive the change?Why was he murdered and burried without the Fateh(last rites in muslims).?  The man who attended the funeral of thousands in his life left Kashmir with out the funeral.??Will mother earth forgive us for that.?  What happened and how?Keep on visiting this blog?the story is still not unfolded?
     

        
</post>

<date>01,June,2004</date>
<post>


           
      He was young and  charasmatic,So people called him prince charming...He was fascianted with the story of Ugly Duckling.  May be it was his own story.As he was growing up in the fields and pastures of Kashmir,He was a self proclaimed king of these meadows.  His only temptations was the "Sootu"(partially grounded and half burnt floor) with a salty tea (Nuuon Chai).  But some one was fast approaching his approaching his meadows.This gujjar boys pastures were soon going to experience a new shepherds and sheep.But who were they.Did he survive the on assualt of new shepherd armed with rocket propelled grenades.  Was he still prince charming or now yearning for past to come back...  Find out about what happened to Prince Charming.??Keep on visiting or subscribe to my stories
     

        
</post>


<date>17,June,2004</date>
<post>

	 
        
     
    
</post>

<date>08,June,2004</date>
<post>

	 
      King Porus lost to Alexender but one his Heart on this bank,I wish I  could have won some heart...    
     
    
</post>

<date>15,July,2004</date>
<post>

	 
      I took some time of this weekend and scanned my pics with a new HP scanner and it was more easy to load them at my Photo Shoot in Yahoo. &nbsp; I have kept them in albums..Dont forget to watch out the best/Worst &nbsp;of my life there at  urlLink http://photos.yahoo.com/vwangoo  &nbsp; Enjoy Madi as they say in Banglore 
     
    
</post>


<date>26,March,2004</date>
<post>

     
      Introduction Good Application Architecture is essential for any application to perform better than best ,but without the    proper Database Support it becomes a distanct dream.  Software Enggs particularly our DBA s spend hours discussing the performance pit falls of  any database design,But once production starts or in testing,whole new world of issues arise with respect to Database. Most important of them is the performance,But how do we say that any issue is performance issue?or for that matter rate the performnce of SQL Server.  May be the Hard ware is faulty or middle tier code is faulty (If written by me) .This Paper provides us a judgement tool and insight complexities involved on SQL 2000  performance.  Before studying this paper I request the audience to acknowledge themselves with various SQL Administration fundamentals.    Principles for Performance Tuning SQL Server Microsoft SQL Server 2000 introduces and enhances methods and tools to tune SQL Server for optimum performance. Keep these principles in mind when you are tuning SQL Server: •	Let SQL Server do most of the tuning. Microsoft SQL Server has been dramatically enhanced to create an auto-configuring and self-tuning database server. Take advantage of the auto-tuning settings available with SQL Server. These settings help SQL Server run at peak performance even as user load and queries change over time. •	RAM is a limited resource. An integral feature of the database server environment is the management of RAM buffer cache. Access to data in RAM cache is much faster than access to the same information from disk, but RAM is a limited resource. If database I/O can be reduced to the minimum required set of data and index pages, these pages will stay in RAM longer. Too much unneeded data and index information flowing into buffer cache quickly push out valuable pages. The focus of performance tuning is to reduce I/O so that buffer cache is best utilized. •	Pick good indexes. A key factor in maintaining minimum I/O for all database queries is to ensure that good indexes are created and maintained.  •	Evaluate disk I/O subsystem performance. The physical disk subsystem must provide a database server with sufficient I/O processing power for the database server to run without disk queuing. Disk queuing results in poor performance. This document describes how to detect and resolve disk I/O problems. •	Tune applications and queries. Tuning your applications and queries becomes especially important when a database server will service requests from hundreds or thousands of connections by way of a given application. Because applications typically determine the SQL queries that will be executed on a database server, application developers must understand SQL Server architectural basics and how to take full advantage of SQL Server indexes to minimize I/O. •	Take advantage of SQL Server Profiler and Index Tuning Wizard. SQL Server Profiler can be used to monitor and log a SQL Server workload, which can then be submitted to the Index Tuning Wizard to tune indexes for better performance. Regular use of SQL Server Profiler and the Index Tuning Wizard helps you optimize the indexes, allowing SQL Server to perform well with changing query workloads. •	Take advantage of SQL Server Performance Monitor. SQL Server  provides a revised set of Performance Monitor objects and counters, which are designed to provide helpful information for monitoring and analyzing the operations of SQL Server. This document describes key Performance Monitor counters to watch. •	Take advantage of Graphical Showplan and SQL Server Query Analyzer. SQL Server  Query Analyzer introduces Graphical Showplan, an enhancement to help analyze problematic Transact-SQL queries. SQL Server Query Analyzer also includes STATISTICS IO, another important tool option for tuning queries. Review the max async IO Option During Configuration The max async IO option should be reviewed and adjusted if necessary during your initial configuration of Microsoft SQL Server .  The max async IO option default of 32 is sufficient for lower-end disk subsystems. With a higher-end RAID storage subsystem attached to a database server that is capable of high disk I/O transfer rates, the setting of 32 may be inadequate because the RAID subsystem is capable of completing many more simultaneous disk transfer requests than 32. If the SQL Server write activity also dictates the need for more disk transfer capability, the max async IO option should be set higher.   An appropriate value for the max async IO option is one that allows checkpoint to finish before another checkpoint is needed (based upon desired recovery characteristics), but not to finish so fast that the system is seriously stressed by the event (a symptom of stress is disk queuing, which is discussed in further detail later in this document).  A general rule to follow for setting the max async IO option for SQL Servers running on large disk subsystems is to multiply by two or three the number of physical drives available to do simultaneous I/O. Then watch Performance Monitor for signs of disk activity or queuing issues. The negative impact of setting this configuration option too high is that it may cause checkpoint to monopolize disk subsystem bandwidth that is required by other SQL Server I/O operations such as reads.  To set the max async IO option value, execute this command in SQL Server Query Analyzer: sp_configure ‘max async io’, value, where value is expressed as the number of simultaneous disk I/O requests that SQL Server system can submit to the Windows® operating system during a checkpoint operation, which in turn submit the requests to the physical disk subsystem. For more information, see “Disk I/O Tuning Performance” later in this document. This configuration option is dynamic and does not require a stop and restart of SQL Server to take effect.   For more information, see these topics in SQL Server  Books Online: I/O Architecture and max async io Option and check SP_CONFIGURE  options.   Components that Consume CPU and Disk I/O Resources You can optimize SQL Server performance with some careful attention to Microsoft SQL Server components that can consume resources. Worker Threads  Microsoft SQL Server maintains a pool of Windows operating system threads to service batches of SQL Server commands submitted to the database server. The total of these threads (called worker threads) available to service all incoming command batches is dictated by the setting for the sp_configure option max worker threads. If the number of connections actively submitting batches is greater than the number specified for max worker threads, the worker threads are shared among connections actively submitting batches. The default setting of 255 works well for many installations.  Worker threads write out most of the dirty 8-KB pages from the SQL Server buffer cache. I/O operations are scheduled by worker threads asynchronously for maximum performance. For more information, see these topics and/or keywords in SQL Server  Books Online: Optimizing Server Performance Using Memory Configuration Options, SQL Server Memory Pool, Transaction Recovery, Write-Ahead Transaction Log, Freeing and Writing Buffer Pages, and SQL Server threads. Lazy Writer SQL Server lazy writer helps produce free buffers, which are 8-KB data cache pages without any data contained in them. As lazy writer flushes each 8-KB cache buffer out to disk, it initializes the cache page identity so that other data can be written into the free buffer. Lazy writer produces free buffers during periods of low disk I/O, so disk I/O resources are readily available for use and there is minimal impact on other SQL Server operations.  SQL Server  automatically configures and manages the level of free buffers. Monitor the SQL Server: Buffer Manager - Free Buffers object to ensure that the free buffer level remains steady. Lazy writer maintains that the level of free buffers keeps up with the user demand for free buffers. The SQL Server: Buffer Manager - Free Buffers object should not drop to zero because this indicates there were times the user load demanded a higher level of free buffers than the SQL Server lazy writer was able to provide.  If the lazy writer cannot keep the free buffer steady, or at least above zero, it could mean the disk subsystem cannot provide lazy writer with the disk I/O performance that it needs to maintain the free buffer level (compare drops in free buffer level to any disk queuing to confirm there is a disk subsystem problem). One solution to the disk queuing problem is to add more physical disk drives (also called spindles) to the database server disk subsystem to provide more disk I/O processing power. The SQL Server: Buffer Manager – Lazy Writes/sec object indicates the number of 8-KB pages written to disk by lazy writer.  Monitor the current level of disk queuing in Performance Monitor by looking at the counters for (logical or physical) Disk: Average Disk Queue or Current Disk Queue and ensure the disk queue is at a level less than 2 for each physical drive associated with any SQL Server activity. For database servers that employ hardware RAID controllers and disk arrays, divide the number reported by (logical or physical) disk counters by the number of actual hard disk drives associated with that logical drive letter or physical hard disk drive number reported by the Windows NT® Disk Administrator program. Windows and SQL Server are unaware of the actual number of physical hard disk drives attached to a RAID controller. You should know the number of drives associated with RAID array controller to interpret the disk queue numbers Performance Monitor reports.  Adjust lazy writer disk I/O request behavior with the max async IO option, which controls the number of 8-KB disk write requests that SQL Server (including requests coming in from lazy writer, checkpoint, and the worker threads) can simultaneously submit to the Windows operating system and in turn, to the disk I/O subsystem. If disk queuing occurs at unacceptable levels, decrease the level of the max async IO option. If the currently configured level of the max async IO option must be maintained, add more disks to the disk subsystem until disk queuing reaches acceptable levels. For more information, see these topics in SQL Server  Books Online: Freeing and Writing Buffer Pages and Write-Ahead Transaction Log. Checkpoint Checkpoint writes out dirty pages to the SQL Server data files. Dirty pages are any buffer cache pages that have been modified since being brought into the buffer cache. A buffer written to disk by checkpoint still contains the page, and users can read or update it without rereading it from disk, which is not the case for free buffers created by lazy writer.  Checkpoint allows worker threads and lazy writer do the majority of the work writing out dirty pages by waiting an extra checkpoint before writing out a dirty page. This provides the worker threads and lazy writer more time to write out the dirty pages. The conditions under which this extra wait time occurs is detailed in your SQL Server documentation. Checkpoint evens out SQL Server disk I/O activity over a longer time period with an extra checkpoint wait. To make checkpoint more efficient when there are many pages to flush out of cache, SQL Server sorts the data pages to be flushed in the order the pages appear on disk. This sorting helps minimize disk arm movement during cache flush and potentially allows checkpoint to take advantage of sequential disk I/O. Checkpoint also submits 8-KB disk I/O requests asynchronously to the disk subsystem. This allows SQL Server to finish submitting required disk I/O requests faster because checkpoint does not wait for the disk subsystem to report back that the data actually has been written to disk.  You should watch disk queuing on hard disk drives associated with SQL Server data files to notice if SQL Server is sending down more disk I/O requests than the disk(s) can handle. If it is, then more disk I/O capacity must be added to the disk subsystem to handle the load. Adjust the checkpoint dirty page flushing behavior by using the max async IO option. The sp_configure option max async IO controls the number of 8-KB cache flushes that checkpoint can submit simultaneously to the Windows operating system and in turn, to the disk I/O subsystem. If disk queuing occurs at unacceptable levels, decrease the max async IO option. If SQL Server must maintain the currently configured level of the max async IO option, add more disks to the disk subsystem until disk queuing decreases to acceptable levels. If you must increase the speed with which SQL Server executes checkpoint and the disk subsystem is powerful enough to handle the increased disk I/O while avoiding disk queuing, then increase the max async IO option to allow SQL Server to send more asynchronous disk I/O requests. Observe the disk queuing counters carefully after you change the max async IO option. Be sure to watch disk read queuing in addition to disk write queuing. If the max async IO option is set too high for a disk subsystem, checkpoint may queue up many disk write I/O requests, which can cause SQL Server read activity to be blocked. Physical disk and logical disk objects in Performance Monitor provide the Average Disk Read Queue Length counter, which can be used to monitor queued disk read I/O requests. If disk read queuing is caused by checkpoint, you can either decrease the max async IO option or add more hard disk drives so the checkpoint and read requests can be handled simultaneously. Log Manager Like other major RDBMS products, SQL Server ensures that all write activity (inserts, updates, and deletes) performed on the database is not lost if something interrupts the SQL Server online status (for example, power failure, disk drive failure, fire in the data center, and so on). The SQL Server logging process helps guarantee recoverability. Before any implicit (single Transact-SQL query) or explicit (transaction that issues BEGIN TRANSACTION, COMMIT or ROLLBACK statements) transactions can be completed, the SQL Server log manager must receive a signal from the disk subsystem that all data changes associated with the transaction have been written successfully to the associated log file. This rule guarantees the transaction log can be read and reapplied in SQL Server when the server is turned on after an abrupt shut down during which the transactions written into the data cache are not yet flushed to the data files. Flushing data buffers are checkpoint or lazy writer responsibility. Reading the transaction log and applying the transactions to SQL Server after a server stoppage is referred to as recovery. Because SQL Server must wait for the disk subsystem to complete I/O to SQL Server log files as each transaction is completed, disks containing SQL Server log files must have sufficient disk I/O handling capacity for the anticipated transaction load. The method for monitoring disk queuing is different for SQL Server log files than it is for SQL Server database files. You can use the Performance Monitor counters SQL Server: Databases database instance : Log Flush Waits Times and SQL Server: Databases database instance : Log Flush Waits/sec to view log writer requests waiting on the disk subsystem for completion.  For highest performance, you can use a caching controller for SQL Server log files if the controller guarantees that data entrusted to it is written to disk eventually, even if the power fails. For more information about caching controllers, see “Effect of On-Board Cache of Hardware RAID Controllers” later in this document. For more information, see these topics and/or keywords in SQL Server  Books Online: Transaction Recovery, Optimizing Transaction Log Performance, and log manager object. Read Ahead Manager The Microsoft SQL Server  Read Ahead Manager is completely self-configuring and self-tuning. Read Ahead Manager is tightly integrated with the operations of SQL Server query processor. SQL Server Query Processor identifies and communicates situations that will benefit from read-ahead scans to the Read Ahead Manager. Large table scans, large index range scans and probes into clustered and nonclustered index B-trees are situations that would benefit from a read-ahead. Read-ahead reads occur with 64-KB I/Os, which provide higher disk throughput potential for the disk subsystem than 8-KB I/Os do. When it is necessary to retrieve a large amount of data from SQL Server, read-ahead is the best way to do it. Read Ahead Manager benefits from the simpler and more efficient Index Allocation Map (IAM) storage structure. The IAM is the SQL Server  method of recording the location of extents (eight pages of SQL Server data or index information for a total of 64 KB of information per extent). The IAM is an 8-KB page that tightly packs information through a bitmap about which extents within the range of extents covered by the IAM contain required data. The compact IAM pages are fast to read and tend to keep regularly used IAM pages in buffer cache.  The Read Ahead Manager can construct multiple sequential read requests by combining the query information from query processor and quickly retrieving the location of all extents that must be read from the IAM page(s). Sequential 64-KB disk reads provide excellent disk I/O performance.  Read-ahead activity is monitored by the SQL Server: Buffer Manager - Readahead Pages counter. You can find more information about read-ahead activity by executing the DBCC PERFMON (IOSTATS) statement. Some of the information provided is RA Pages Found in Cache and RA Pages Placed in Cache. If the page is already hashed (the application read it in first and read-ahead wasted a read), it is a page found in cache. If the page is not already hashed (a successful read-ahead), it is a page placed in cache. Too much read-ahead can be detrimental to overall performance because it can fill cache with unnecessary pages, requiring additional I/O and CPU that could have been used for other purposes. The solution is a performance tuning goal that all Transact-SQL queries are tuned so a minimal number of pages are brought into buffer cache. This includes using the right index for the right job. Save clustered indexes for efficient range scans and define nonclustered indexes to help locate single rows or smaller rowsets quickly. For more information, see these topics and/or keywords in SQL Server  Books Online: Reading Pages, Table and Index Architecture, Heap Structures, DBCC PERFMON, and read-ahead pages. Disk I/O Performance When you configure a SQL Server that contains only a few gigabytes of data and does not sustain heavy read or write activity, you are not as concerned with disk I/O and balancing SQL Server I/O activity across hard disk drives for maximum performance. If you build larger SQL Server databases, however, that contain hundreds of gigabytes of data and/or will sustain heavy read and/or write activity, you must configure SQL Server to maximize disk I/O performance by load-balancing across multiple hard disk drives. Advertised Disk Transfer Rates and SQL Server An important aspect of database performance tuning is I/O performance tuning. Unless SQL Server is running on a computer with enough RAM to hold the entire database, I/O performance is dictated by how fast reads and writes of SQL Server data can be processed by the disk I/O subsystem.  The typical hard disk drive is capable of providing the Windows operating system and SQL Server with about 75 nonsequential (random) and 150 sequential I/O operations per second. Advertised transfer rates for these hard disk drives are around 40 megabytes (MB) per second. It is much more likely for a database server to be constrained by the 75/150 I/O transfers per second than the 40-MB per second transfer rate. This is illustrated by these calculations: (75 random I/O operations per second) X (8-KB transfer) = 600 KB per second This calculation indicates that by doing strictly random single page read and write operations on a hard disk drive, you can expect at most 600 KB (0.6 MB) per second I/O processing capability from that hard disk drive. This is much lower than the advertised 40 MB per second I/O handling capacity of the drive. SQL Server worker threads, checkpoint, and lazy writer perform I/O in 8-KB transfer sizes. (150 sequential I/O operations per second) X (8-KB transfer) = 1200 KB per second This calculation indicates by doing strictly sequential single-page read and write operations on a hard disk drive, it is reasonable to expect at most 1200 KB (1.2 MB) per second I/O processing capability from the hard disk drive.  (75 random I/O operations per second) X (64-KB transfer) = 4800 KB (4.8 MB) per second This calculation illustrates a worse-case scenario for read-aheads, assuming all random I/O. Even in the completely random situation, the 64-KB transfer size provides much better disk I/O transfer rate from disk (4.8 MB per second) than the single-page transfer rates (0.6 and 1.2 MB per second): (150 sequential I/O operations per second) X (64-KB transfer) = 9600 KB (9.6 MB) per second  This calculation indicates that by doing strictly sequential read or write operations on a hard disk drive, you can expect at most 9.6 MB per second I/O processing capability from that hard disk drive. This is much better than the random I/O case. SQL Server Read Ahead Manager performs disk I/O in the 64-KB transfer rate and attempts to arrange reads so read-ahead scans are done sequentially (often referred to as serially or in disk order). Although Read Ahead Manager performs I/O operations sequentially, page splitting tends to cause extents to be read nonsequentially rather than sequentially. This is one reason to eliminate and prevent page splitting. Log manager writes sequentially to log files up to 32 KB. Sequential vs. Nonsequential Disk I/O Operations The terms sequential and nonsequential (random) have been used to refer to hard disk drive operations. A single hard disk drive consists of a set of drive platters, each of which provides services for read and write operations with a set of arms with read and write heads that can move across the platters and read information from the drive platter or write data onto the platters. Remember these points about hard disk drives and SQL Server: •	The read/write heads and associated disk arms must move to find and operate on the location of the hard disk drive platter that SQL Server and the Windows operating system requested. If the data is located on nonsequential locations on the hard disk drive platter, it takes significantly more time for the hard disk drive to move the disk arm and read/write head to all of the necessary hard disk drive platter locations. This contrasts with the sequential case, in which all of the required data is located on one continuous physical section of the hard disk drive platter, so that the disk arm and read/write heads move a minimal amount to perform the necessary disk I/O operations. The time difference between the nonsequential versus sequential case is significant, about 50 milliseconds per nonsequential seek versus approximately 2-3 milliseconds for sequential seeks. These times are rough estimates and will vary based on how far apart the nonsequential data is spread around on the disk, how fast the hard disk platters can spin (RPM), and other physical attributes of the hard disk drive. The main point is that sequential I/O is good for SQL Server performance. •	A typical hard disk drive supports about 75 nonsequential and 150 sequential I/Os per second. It takes almost as much time to read or write 8 KB as it does to read or write 64 KB. Within the range of 8 KB to about 64 KB the disk arm and read/write head movement account for most of the time of a single disk I/O transfer operation. Perform 64-KB disk transfers as often as possible when more than 64 KB of SQL data must be transferred, because a 64-KB transfer is fast as an 8-KB transfer and eight times the amount of SQL Server data is processed for each transfer. Read Ahead Manager does disk operations in 64-KB chunks (referred to as a SQL Server extent). Log manager also performs sequential writes in larger I/O sizes. By making good use of Read Ahead Manager and separating SQL Server log files from other nonsequentially accessed files you can improve SQL Server performance.  For more information about physical hard disk drives, see the Compaq document, “Disk Subsystem Performance and Scalability.” For the location of this document, see “Finding More Information,” later in this document. Disk I/O Transfer Rates and PCI Bus Bandwidth A typical hard disk provides a maximum transfer rate of about 40 MB per second or 75 nonsequential/150 sequential disk transfers per second. Typical RAID controllers have an advertised transfer rate of about 40 MB per second or approximately 2000 disk transfers per second. PCI buses have an advertised transfer rate of about 133 MB per second and higher. The actual transfer rates achievable for a device usually differ from the advertised rate. You should understand how to use these transfer rates as a starting point for determining the number of hard disk drives to associate with each RAID controller and in turn, how many drives and RAID controllers can be attached to a PCI bus without I/O bottlenecks.  Earlier, we calculated that the maximum amount of SQL Server data that can be read from or written to a hard disk drive per second is 9.6 MB. Assuming a RAID controller can handle 40 MB per second, you can calculate the number of hard disk drives that should be associated with one RAID controller by dividing 40 by 9.6 to get about 4. This means that at most four drives should be associated with that one controller when SQL Server is doing nothing but sequential I/O of 64 KB. Similarly, we calculated that with all nonsequential I/O of 64 KB, the maximum data sent from the hard disk drive to the controller is 4.8 MB per second. Dividing 40 MB per second by 4.8 MB per second gives us the result of about 8. This means that at most eight hard disk drives should be associated with the single controller in the nonsequential 64-KB scenario. The random 8-KB data transfer scenario requires the most drives. Divide 40 by 0.6 to determine that about 66 drives are needed to saturate a RAID controller doing 100 percent random 8-KB reads and writes. This is not realistic scenario because read-ahead and log-writing use transfer sizes greater than 8-KB and it is unlikely that a SQL Server will perform 100 percent random I/O.  You can also determine how many drives should be associated with a RAID controller by looking at disk transfers per second instead of looking at the megabytes per second. If a hard disk drive is capable of 75 nonsequential (random) I/Os per second, then about 26 hard disk drives working together could theoretically produce 2000 nonsequential I/Os per second, or enough to hit the maximum I/O handling capacity of a single RAID controller. Alternately, it only takes about 13 hard disk drives working together to produce 2000 sequential I/Os per second and keep the RAID controller running at maximum throughput, since a single hard disk drive can sustain 150 sequential I/Os per second. RAID controller and PCI bus bottlenecks are not nearly as common as I/O bottlenecks related to hard disk drives. To illustrate, assume a set of hard disk drives associated with a RAID controller is busy enough to push 40 MB per second of throughput through the controller. The next consideration is how many RAID controllers can be safely attached to the PCI bus without risking a PCI bus I/O bottleneck. To make a rough estimate, divide the I/O processing capacity of the PCI bus by the I/O processing capacity of the RAID controller: 133 MB/sec divided by 40 MB/sec results in approximately three RAID controllers that can be attached to a single PCI bus. Most large servers come with more than one PCI bus, which increases the number of RAID controllers that can be installed in a single server. These calculations illustrate the relationship of the transfer rates of the components that comprise a disk I/O subsystem (hard disk drives, RAID controllers, PCI bus) and are not literal figures. These calculations assume all sequential or all nonsequential data access, which is not likely in a production database server environment. In reality, a mixture of sequential, nonsequential, 8-KB and 64-KB I/O occurs. Additional factors influence how many I/O operations can be pushed through a set of hard disk drives at one time. On-board read/write caching available for RAID controllers increases the amount of I/O that a set of drives can effectively produce. How much more is difficult to estimate for the same reason that it is difficult to determine an exact number an 8-KB versus a 64-KB I/O SQL Server environment needs.  RAID When scaling databases more than a few gigabytes (GB) it is important to have at least a basic understanding of RAID (Redundant Array of Inexpensive Disks) technology and how it relates to database performance. These are the benefits of RAID: •	Performance Hardware RAID controllers divide read/writes of all data from Windows and applications such as Microsoft SQL Server into slices (usually 16 KB - 128 KB) that are spread across all disks participating in the RAID array. Splitting data across physical drives distributes the read/write I/O workload evenly across all physical hard disk drives participating in the RAID array. This increases disk I/O performance because the hard disks participating in the RAID array are all kept equally busy, instead of some disks becoming a bottleneck due to uneven distribution of I/O requests. •	Fault Tolerance RAID provides protection from hard disk failure and accompanying data loss with two methods: mirroring and parity.  Mirroring is implemented by writing information onto two sets of drives, one on each side of the mirrored pairs of drives. If there is a drive loss with mirroring in place, the data for the lost drive can be rebuilt by replacing the failed drive and rebuilding the data from the failed drive’s matching drive. Most RAID controllers can provide a failed drive replacement and rebuild from the other side of the mirrored pair while Windows and SQL Server are online (referred to as “Hot Plug” capable drives). Mirroring is the best performing RAID option when fault tolerance is required. Each SQL Server write in the mirroring situation costs two disk I/O operations, once to each side of the mirrorset. Mirroring also provides more fault tolerance than parity RAID implementations. Mirroring can sustain at least one failed drive and may be able to survive failure of up to half of the drives in the mirrorset without forcing the system administrator to shut down the server and recover from file backup. The disadvantage of mirroring is cost. The disk cost of mirroring is one drive for each drive of data. RAID 1 and its hybrid, RAID 0+1 are implemented through mirroring. Parity is implemented by calculating recovery information about data written to disk and writing this parity information on the other drives that form the RAID array. If a drive fails, a new drive is inserted into the RAID array and the data on that failed drive is recovered by taking the recovery information (parity) written on the other drives and using this information to regenerate the data from the failed drive. RAID 5 and its hybrids are implemented through parity. The advantage of parity is low cost. To protect any number of drives with RAID 5, only one additional drive is required. Parity information is evenly distributed among all drives participating in the RAID 5 array. The disadvantages of parity are performance and fault tolerance. Due to the additional costs associated with calculating and writing parity, RAID 5 requires four disk I/O operations for each Windows NT and SQL Server write as compared to two disk I/O operations for mirroring. Read I/O operation costs are the same for mirroring and parity. Also, RAID 5 can sustain only one failed drive before the array must be taken offline and recovery from backup media must be performed to restore data.  A general rule is to stripe across as many disks as necessary to achieve solid disk I/O performance. Performance Monitor indicates whether there is a disk I/O bottleneck on a particular RAID array. Be ready to add disks and redistribute data across RAID arrays and/or SCSI channels as necessary to balance disk I/O and maximize performance. Effect of On-board Cache of Hardware RAID Controllers Many hardware RAID controllers have some form of read and/or write caching. Take advantage of this available caching with SQL Server because it can enhance the effective I/O handling capacity of the disk subsystem. The principle of these controller-based caching mechanisms is to gather smaller and potentially nonsequential I/O requests coming in from the host server (hence SQL Server) and try to batch them with other I/O requests so the batched I/Os can form larger (32 - 128 KB) and maybe sequential I/O requests to send to the hard disk drives. This helps produce more disk I/O throughput given the fixed number of I/Os that hard disks can provide to the RAID controller. The RAID controller cache arranges incoming I/O requests by making the best use of the hard disks underlying I/O processing ability. RAID controllers usually protect their caching mechanisms with a form of backup power. The backup power can preserve the data written in cache for a period of time (perhaps days) in case of a power outage. And in production environments, the backup power can provide the database server greater protection by providing adequate UPS protection to the server to flush data to disk if power to the server is disrupted. RAID Levels RAID 1 and RAID 0+1 offer the best data protections and best performance among RAID levels but costs more required disks. When cost of hard disks is not a limiting factor, RAID 1 or RAID 0+1 are the best RAID choices for performance and fault tolerance. RAID 5 provides fault tolerance at the best cost but has half the write performance of RAID 1 and 0+1 because of the additional I/O that RAID 5 has to do reading and writing parity information onto disk. RAID 5 is not as fault tolerant as RAID 1 and 0+1. The best disk I/O performance is achieved with RAID 0 (disk striping with no fault tolerance protection), but because there is no fault tolerance with RAID 0, this RAID level typically can be used only for development database servers or other testing environments. Many RAID array controllers provide the option of RAID 0+1 (also referred to as RAID 1/0 and RAID 10) over physical hard disk drives. RAID 0+1 is a hybrid RAID solution. On the lower level, it mirrors all data, like normal RAID 1. On the upper level, the controller stripes data across all of the drives (like RAID 0). Thus, RAID 0+1 provides maximum protection (mirroring) with high performance (striping). These mirroring and striping operations are transparent to Windows NT and SQL Server because they are managed by the RAID controller. The difference between RAID 1 and RAID 0+1 is on the hardware controller level. RAID 1 and RAID 0+1 require the same number of drives for a given amount of storage. For more specifics about RAID 0+1 implementation of specific RAID controllers, contact the hardware vendor that produced the controller. This illustration shows the differences between RAID 0, RAID 1, RAID 5 and RAID 0+1. To hold four disks of data, RAID 1 (and RAID 0+1) needs eight disks, whereas Raid 5 needs five disks. Be sure to involve the appropriate hardware vendors to learn more about RAID implementation specific to the hardware running the database server. Common RAID Levels    Online RAID Expansion Online RAID expansion is a feature that allows disks to be added dynamically to a physical RAID array while SQL Server is online, as long as there are hot-plug slots available. Many hardware vendors offer hardware RAID controllers capable of providing this functionality. Data is automatically restriped across all drives evenly, including the newly added drive, and there is no need to shut down SQL Server or Windows. You can take advantage of this functionality by leaving hot-plug hard disk drive slots free in the disk array cages. Thus, if SQL Server is regularly over-taxing a RAID array with I/O requests (this is indicated by disk queue length for the Windows logical drive letter associated with that RAID array), you can install one or more new hard disk drives into the hot-plug slot while SQL Server is still running. The RAID controller redistributes some existing SQL data to these new drives so that SQL data is evenly distributed across all drives in the RAID array. Then, the I/O processing capacity of the new drives (75 nonsequential/150 sequential I/Os per second, per drive) is added to the overall I/O processing capacity of the RAID array. Performance Monitor and RAID In Performance Monitor, logical and physical disk objects provide the same information. The difference is that logical disks in Performance Monitor are associated with what Windows NT interprets as a logical drive letter. Physical disks in Performance Monitor are associated with what Windows NT interprets as a single physical hard disk.  To enable Performance Monitor counters, use the command diskperf.exe from the command line of the command-prompt window. Use diskperf –y so that Performance Monitor reports logical and physical disk counters when using hard disk drives or sets of hard disk drives and RAID controllers, without the use of Windows NT software RAID.  When running Windows NT software RAID, use diskperf –ye so that that Performance Monitor correctly reports physical counters across the Windows NT stripesets. When  diskperf –ye is used in conjunction with Windows NT stripesets, logical counters do not report correct information and must be disregarded. If logical disk counter information is required in conjunction with Windows NT stripesets, use diskperf –y instead. With diskperf  y and Windows NT stripesets, logical disk counters are reported correctly but physical disk counters do not report correct information and should be disregarded. The effects of the diskperf -y command do not occur until Windows NT has been restarted. Hardware RAID controllers present multiple physical hard disk drives that compose a single RAID mirrorset or stripeset to the Windows operating system, as one single physical. Disk Administrator is used to associate logical drive letters to the single physical disk and does not need to know how many hard disks are actually associated with the single hard physical that the RAID controller has presented to it. You should know how many physical hard disk drives are associated with a RAID array so you can determine the number of disk I/O requests that the Windows operating system and SQL Server send to each physical hard disk drive. Divide the number of disk I/O requests that Performance Monitor reports as associated with a hard disk drive by the number of actual physical hard disk drives known to be in the RAID array.  To estimate I/O activity per hard disk drive in a RAID array, multiply the number of disk write I/Os reported by Performance Monitor by either 2 (RAID 1 and 0+1) or 4 (RAID 5). This accurately accounts for the number of I/O requests being sent to the physical hard disk drives. It is at this physical level that the I/O capacity for hard disk drives apply (75 nonsequential and 150 sequential per drive). But do not expect to calculate exactly how much I/O is hitting the hard disk drives when the hardware RAID controller is using caching, because caching can change the amount of I/O that is hitting the hard disk drives.  It is best to monitor on disk queuing unless I/O is causing a problem. The Windows operating system cannot see the number for physical drives in a RAID array, so to assess disk queuing per physical disk accurately, you must divide the disk queue length by the number of physical drives participating in the hardware RAID disk array that contains the logical drive being observed. Keep this number under two for hard disk drives containing SQL Server files.  For more information about SQL Server and RAID, see these topics in SQL Server  Books Online: RAID Levels and SQL Server, Comparing Different Implementations of RAID Levels, Monitoring Disk Activity, Performance Monitoring Example: Identifying Bottlenecks, About Hardware-based Solutions, and RAID. Windows NT Software RAID Windows NT provides fault tolerance to hard disk failure by providing mirrorsets and stripesets (with or without fault tolerance) through the Windows NT operating system instead of through a hardware RAID controller. Windows NT Disk Administrator is used to define either mirrorsets (RAID 1) or stripesets with parity (RAID 5). Windows NT Disk Administrator also allows the definition of stripesets with no fault tolerance (RAID 0). Software RAID utilizes more CPU resources because Windows NT is the component managing the RAID operations versus the hardware RAID controller. Thus, performance with the same number of disk drives and Windows NT software RAID may be a few percent less than the hardware RAID solution if the system processors are near 100 percent utilized. But Windows NT software RAID generally helps a set of drives service SQL Server I/O better overall than those drives would have been able to separately, reducing the potential for an I/O bottleneck, leading to higher CPU utilization by SQL Server and better throughput. Windows NT software RAID can provide a better-cost solution for providing fault tolerance to a set of hard disk drives. For more information about configuring Windows NT software RAID, see your Windows NT Server Online Help. Also see these topics in SQL Server  Books Online: About Windows NT-based Disk Mirroring and Duplexing and About Windows NT-based Disk Striping and Striping with Parity. Disk I/O Parallelism With smaller SQL Server databases located on a few disk drives, disk I/O parallelism is not a likely performance factor. But with large SQL Server databases stored on many disk drives, performance is enhanced by using disk I/O parallelism to make optimal use of the I/O processing power of the disk subsystem.  Microsoft SQL Server  introduces files and filegroups, which replace the device and segment model from earlier versions of SQL Server. The files and filegroups provide a more convenient method for spreading data proportionately across disk drives or RAID arrays. For more information, see these topics in SQL Server  Books Online: Placing Indexes on Filegroups, Placing Tables on Filegroups, Files and Filegroups, and Using Files and Filegroups to Manage Database Growth. A technique for creating disk I/O parallelism is to create a single “pool of drives” that serves all SQL Server database files, excluding transaction log files. The pool can be a single RAID array that is represented in Windows NT as a single physical disk drive. Or a larger pool can be set up using multiple RAID arrays and SQL Server files/filegroups. A SQL Server file can be associated with each RAID array and the files can be combined into a SQL Server filegroup. Then a database can be built on the filegroup so that the data is spread evenly across all of the drives and RAID controllers. The drive pool methodology depends on RAID to divide data across all physical disk drives to help ensure parallel access to the data during database server operations.  The pool methodology simplifies SQL Server I/O performance tuning because database administrators know there is only one physical location to create database objects. The single pool of drives can be watched for disk queuing and, if necessary, more hard disk drives can be added to the pool to prevent disk queuing. This technique helps optimize for the common case, where it is not known which parts of databases will see the most use. Do not segregate part of available I/O capacity on another disk partition because SQL Server might do I/O to it 5 percent of the time. The single pool of drives methodology can make all available I/O capacity available for SQL Server operations. SQL Server log files always should be physically separated onto different hard disk drives from all other SQL Server database files. For SQL Servers with busy databases, transaction log files should be physically separated from each other. Transaction logging is primarily sequential write I/O. Separating transaction logging activity from other nonsequential disk I/O activity can result in I/O performance benefits. That allows the hard disk drives containing the log files to concentrate on sequential I/O. There are times when the transaction log must be read as part of SQL Server operations such as replication, rollbacks, and deferred updates. Administrators of SQL Servers that participate in replication should make sure all transaction log files have sufficient disk I/O processing power because of the reads that need to occur. Physically separating SQL Server objects from the rest of their associated database through SQL Server files and filegroups requires additional administration. Separating the objects can be worthwhile to investigate active tables and indexes. By separating table or index from all other database objects, accurate assessments can be made of the object I/O requirements. This is not as easy to do when all database objects are placed within one drive pool. Physical I/O separation can be appropriate during database development and benchmarking so that database I/O information can be gathered and applied to capacity planning for the production database server environment. These are the areas of SQL Server activity that can be separated across different hard disk drives, RAID controllers, PCI channels, or combinations of the three: •	Transaction log files •	tempdb •	Database files •	Tables associated with considerable query or write activity •	Nonclustered indexes associated with considerable query or write activity The physical separation of SQL Server I/O activities is made convenient by using hardware RAID controllers, RAID hot plug drives, and online RAID expansion. The approach that provides the most flexibility is arranging the RAID controllers so that a separate RAID SCSI channel is provided for each database activity. Each RAID SCSI channel should be attached to a separate RAID hot plug cabinet to take full advantage of online RAID expansion (if it is available through the RAID controller). Windows logical drive letters are associated with each RAID array and SQL Server files can be separated between distinct RAID arrays based on known I/O usage patterns. With this configuration you can relate disk queuing to a distinct RAID SCSI channel and its drive cabinet as Performance Monitor reports the queuing behavior during load testing or heavy production loads. If a RAID controller and drive array cabinet support online RAID expansion and slots for hot-plug hard disk drives are available in the cabinet, disk queuing on that RAID array is resolved by adding more drives to the RAID array until Performance Monitor reports that disk queuing for that RAID array has reached acceptable levels (less than 2 for SQL Server files.) This can be done while SQL Server is online. The tempdb database is created by SQL Server to be a shared working area for a variety of activities, including temporary tables, sorting, subqueries and aggregates with GROUP BY or ORDER BY, queries using DISTINCT (temporary worktables must be created to remove duplicate rows), cursors, and hash joins. You should enable the tempdb database I/O operations to occur in parallel to the I/O operations of related transactions. Because tempdb is a scratch area and update-intensive, RAID 5 is not as good a choice for tempdb as RAID 1 or 0+1. The tempdb database is rebuilt every time the database server is restarted, so RAID 0 is a possibility for tempdb on production SQL server computers. RAID 0 provides the best RAID performance for the tempdb database with the least physical drives. The concern with using RAID 0 for tempdb in a production environment is that SQL Server must be stopped and restarted if physical drive failure occurs in the RAID 0 array and this does not necessarily occur if tempdb is placed on a RAID 1 or 0+1 array. To move the tempdb database, use the ALTER DATABASE command to change the physical file location of the SQL Server logical file name associated with the tempdb database. For example, to move the tempdb database and its associated log to the new file locations E:\Mssql and C:\Temp, use these commands: alter database tempdb modify file (name='tempdev',filename= 'e:\mssql\tempnew_location.mDF') alter database tempdb modify file (name='templog',filename= 'c:\temp\tempnew_loglocation.LDF') The master, msdb, and model databases are not used much during production compared to user databases, so thay are not typically a consideration in I/O performance tuning. The master database is used only for adding new logins, databases, and other system objects. Nonclustered indexes reside in B-tree structures that can be separated from their related database tables with the ALTER DATABASE statement. In this example, the first ALTER DATABASE creates a filegroup. The second ALTER DATABASE creates a file with a separate physical location associated with the filegroup. At this point, indexes can be created on the filegroup as illustrated by creating the index called index1. The sp_helpfile stored procedure reports files and filegroups present for a given database. The sp_help tablename has a section in its output that provides information about the table indexes and filegroup relationships. alter database testdb add filegroup testgroup1 alter database testdb add file (name = 'testfile',  	filename = 'e:\mssql\test1.ndf') to filegroup testgroup1 create table test1(col1 char(8)) create index index1 on test1(col1) on testgroup1  sp_helpfile sp_help test1 For more information, see these topics and/or keywords in SQL Server  Books Online: ALTER DATABASE, sp_helpfile, Files and Filegroups, Placing Indexes on Filegroups, Monitoring Disk Activity, Physical Database Files and Filegroups, adding and deleting data, and transaction log files. SQL Server Indexes This section contains information about how SQL Server data and index structures are physically placed on disk drives and how these structures apply to disk I/O performance.  SQL Server data and index pages are each 8 KB. SQL Server data pages contain all of the data associated with rows of a table, except text and image data. For text and image data, the SQL Server data page, which contains the row associated with the text or image column, contains a pointer to a B-tree structure of one or more 8-KB pages.  SQL Server index pages contain only data from columns that comprise a particular index, therefore index pages compress information associated with many more rows into an 8-KB page than does an 8-KB data page. The I/O performance benefit of indexes is a result of this information compression. If the columns picked to be part of an index form a low percentage of the rowsize of the table then information about more rows can be compressed in an 8-KB index page, which has performance benefits. When a SQL query requests a set of rows from a table in which columns in the query match values in the rows, SQL Server can save I/O operations by reading the index pages for the values and then accessing only the rows in the table required to satisfy the query. Then SQL Server does not have to perform I/O operations to scan all rows in the table to locate the required rows. SQL Server indexes are built upon B-tree structures formed out of 8-KB index pages. The difference between clustered and nonclustered indexes is at the bottom of the B-tree structures, (referred to as leaf level). The upper parts of index B-tree structures are referred to as nonleaf levels of the index. A B-tree structure is built for every single index defined on a SQL Server table.  The illustration shows the structural difference between nonclustered and clustered indexes. In a nonclustered index, the leaf-level nodes contain only the data that participates in the index. In the nonclusterered index leaf-level nodes, index rows contain pointers to the remaining row data on the associated data page. At worst, each row access from the nonclustered index requires an additional nonsequential disk I/O to retrieve the row data. At best, many of the required rows will be on the same data page and thus allow retrieval of several required rows with each data page fetched. In the clustered index, the leaf-level nodes of the index are the actual data rows for the table. Therefore, no bookmark lookups are required for retrieval of table data. Range scans based on clustered indexes perform well because the leaf level of the clustered index (and hence all rows of that table) is physically ordered on disk by the columns that comprise the clustered index and will perform I/O in 64-KB extents. These 64-KB I/Os are physically sequential if there is not a lot of page splitting on the clustered index B-tree (nonleaf and leaf levels). The dotted lines indicate there are other 8-KB pages present in the B-tree structures but they are not shown.   Clustered Indexes There can be only one clustered index per table because, while the upper parts of the clustered index B-tree structure are organized like the nonclustered index B-tree structures, the bottom level of the clustered index B-tree consists of the actual 8-KB data pages associated with the table. There are performance implications: •	Retrieval of SQL data based on key search with a clustered index requires no bookmark lookup (and a likely nonsequential change of location on the hard disk) to get to the associated data page, because the leaf level of the clustered index is already the associated data page. •	The leaf level of the clustered index is sorted by the columns that comprise the clustered index. Because the leaf level of the clustered index contains the actual 8-KB data pages of the table, the row data of the entire table is physically arranged on the disk drive in the order determined by the clustered index. This provides a potential I/O performance advantage when fetching a significant number of rows from tables greater than 64-KB based on the value of the clustered index, because sequential disk I/O is being used unless page splitting is occuring on this table. For more information about page-splitting, see “FILLFACTOR and PAD_INDEX” later in this document. You should pick the clustered index on a table based on a column that is used to perform range scans to retrieve a large number of rows. Nonclustered Indexes Nonclustered indexes are most useful for fetching few rows with good selectivity from large SQL Server tables based on a key value. Nonclustered indexes are B-trees formed out of 8-KB index pages. The bottom or leaf level of the B-tree of index pages contains all the data from the columns that comprised that index. When a nonclustered index is used to retrieve information from a table based on a match with the key value, the index B-tree is traversed until a key match is found at the leaf level of the index. A bookmark lookup is made if columns from the table are needed that did not form part of the index. This bookmark lookup will likely require a nonsequential I/O operation on the disk. It might even require the data to be read from another disk if the table and its accompanying index B-tree(s) are large. If multiple bookmark lookup lead to the same 8-KB data page, then there is less of an I/O performance penalty since it is only necessary to read the page into data cache once. For each row returned for a SQL query that involves searching with a nonclustered index, one bookmark lookup is required. These bookmark lookups are the reason that nonclustered indexes are better suited for SQL queries that return only one or a few rows from the table. Queries that require many rows to be returned are better served with a clustered index. For more information, see this keyword in SQL Server  Books Online: nonclustered index. Covering Indexes A special situation that occurs with nonclustered indexes is called the covering index. A covering index is a nonclustered index built upon all of the columns required to satisfy a SQL query, both in the selection criteria and in the WHERE clause. Covering indexes can save I/O and improve query performance. But you must balance the costs of creating a new index (with its associated B-tree index structure maintenance) with the I/O performance gain the covering index will bring. If a covering index will benefit a query or a set of queries that run often on SQL Server, then creating the covering index may be worthwhile. Example: SELECT col1,col3 FROM table1 WHERE col2 = 'value' 	CREATE INDEX indexname1 ON table1(col2,col1,col3) or From SQL Server Enterprise Manager, use the Create Index Wizard. The indexname1 index in this example is a covering index because it includes all columns from the SELECT statement and the WHERE clause. During the execution of this query, SQL Server does not need to access the data pages associated with table1. SQL Server can obtain all of the information required to satisfy the query by using the index called indexname1. When SQL Server has traversed the B-tree associated with indexname1 and has found the range of index keys where col2 is equal to value, SQL Server fetches all of required data (col1,col2,col3) from the leaf level of the covering index. This provides I/O performance in two ways: •	SQL Server obtains all required data from an index page, not a data page, so the data is more compressed and SQL Server saves disk I/O operations. •	The covering index organizes all of the required data by col2 physically on the disk. The hard disk drives return all of the index rows associated with the WHERE clause (col2 = value) in sequential order, which gives better I/O performance. From a disk I/O standpoint a covering index becomes a clustered index for this query and any other query that can be satisfied completely by the columns in the covering index.  If the number of bytes from all the columns in the index is small compared to the number of bytes in a single row of that table, and you are certain the query taking advantage of the covered index will be executed frequently, then it may make sense to use a covering index. But, before building a lot of covered indexes, consider how SQL Server  can effectively and automatically create covered indexes for queries on the fly. Automatic Covering Indexes or Covered Queries The Microsoft SQL Server  Query Processor provides index intersection. Index intersection allows the query processor to consider multiple indexes from a given table, build a hash table based on those multiple indexes, and utilize the hash table to reduce I/O for a query. The hash table that results from the index intersection becomes a covering index and provides the same I/O performance benefits that covering indexes do. Index intersection provides greater flexibility for database user environments in which it is difficult to determine all of the queries that will run against the database. A good strategy in this case is to define single column, nonclustered indexes on all columns that will be queried frequently and let index intersection handle situations in which a covered index is needed. For more information, see these topics in SQL Server  Books Online: Query Tuning Recommendations and Designing an Index. Example: SELECT col3 FROM table1 WHERE col2 = 'value' 	CREATE INDEX indexname1 ON table1(col2) 	CREATE INDEX indexname2 ON table1(col3) or From SQL Server Enterprise Manager, use the Create Index Wizard. In this example, indexname1 and indexname2 are nonclustered, single column indexes created on the SQL Server table called table1. When the query executes, the query processor recognizes that index intersection using the two indexes is advantgeous. The query optimizer automatically hashes the two indexes together to save I/O while executing the query. No query hints were required. Queries handled by covering indexes (whether by explicitly declared covering indexes or index intersection) are called covered queries. Index Selection How indexes are chosen significantly affects the amount of disk I/O generated and, subsequently, performance. Nonclustered indexes are appropriate for retrieval of a few rows and clustered indexes are good for range scans. In addition, you should try to keep indexes compact (few columns and bytes). This is especially true for clustered indexes because nonclustered indexes use the clustered index to locate row data. For more information, see these topics in SQL Server  Books Online: Using Clustered Indexes, Index Tuning Recommendations, and Designing an Index. Consider selectivity for nonclustered indexes because if a nonclustered index is created on a large table with only a few unique values, use of that nonclustered index does not save I/O during data retrieval. In fact, using the index will cause much more I/O than a sequential table scan of the table. Possible candidates for a nonclustered index include invoice numbers, unique customer numbers, social security numbers, and telephone numbers. Clustered indexes are much better than nonclustered indexes for queries that match columns or search for ranges of columns that do not have many unique values, because the clustered index physically orders the table data and allows for sequential 64-KB I/O on the key values. Possible candidates for a clustered index include states, company branches, date of sale, zip codes, and customer district. Defining a clustered index on the columns that have unique values is not beneficial unless typical queries on the system fetch large sequential ranges of the unique values. To pick the best column on each table to create the clustered index ask if there will be many queries that must fetch many rows based on the order of this column. The answer is very specific to each user environment. One company may do more queries based on ranges of dates whereas another company may do many queries based on ranges of bank branches. These are examples of WHERE clauses that benefit from clustered indexes: …WHERE    > some_value …WHERE   BETWEEN some_value AND some_value …WHERE    Clustered Index Selection Clustered index selection really involves two major steps. First, determine the column of the table that will benefit most from the clustered index by providing sequential I/O for range scans, and second, use the clustered index to affect the physical placement of table data while avoiding hot spots. A hot spot occurs when data is placed on hard disk drives so many queries try to read or write data in the same area of the disk(s) at the same time. This creates a disk I/O bottleneck because more concurrent disk I/O requests are received by the hard disk than it can handle. The solution is either to stop fetching as much data from this disk or to spread the data across multiple disks to support the I/O demand. This consideration for the physical placement of data can be critical for good concurrent access to data among hundreds or thousands of SQL Server users. These two decisions often conflict with one another and the best decision is to balance the two. In high user load environments, improved concurrency (by avoiding hot spots) can be more valuable than the performance benefit gained by placing the clustered index on that column. With SQL Server , nonclustered indexes will use the clustered index to locate data rows if there is a clustered index present on the table. Since all nonclustered indexes need to hold the clustered keys within their B-tree structures, it is better for performance to keep the overall byte size of the clustered index keys as small as possible. Keep the number of columns in the clustered index to a minimum and carefully consider the byte size of each of the columns chosen to be included in a clustered index. This helps reduce the size of the clustered index and subsequently, all nonclustered indexes on a table. Smaller index B-tree structures can be read quicker and help improve performance. For more information, see this topic in SQL Server  Books Online: Using Clustered Indexes. In earlier versions of SQL Server, tables without a clustered index (called heaps) inserted rows are placed at the end of the table on disk. This created the possibility of a hot spot at the end of a busy table. The SQL Server  storage management algorithms provide free space management that removes this behavior. When rows are inserted in heaps, SQL Server uses the Page Free Space (PFS) pages to quickly locate available free space in the table in which the row is inserted. PFS pages find free space throughout the table, which recovers deleted space and avoids insertion hot spots. Free space management affects clustered index selection. Because clustered indexes affect physical data placement, hot spots can occur when a clustered index physically sequences based on a column where many concurrent inserts occur at the highest column value and are at the same physical disk location. For columns with monotonically increasing values, a clustered index sequentially orders data rows on disk by that column. By placing the clustered index on another column or by not including a clustered index on the table, this sequential data placement moves to another column or does not occur at all. Another way to think about hot spots is within the context of selects. If many users select data with key values that are very close to but are not in the same rows, most disk I/O activity will occur within the same physical region of the disk I/O subsystem. This disk I/O activity can be spread out more evenly by defining the clustered index for this table on a column that spreads these key values evenly across the disk. If all selects are using the same unique key value, then using a clustered index does not help balance the disk I/O activity of this table. By using RAID (either hardware or software) you can alleviate this problem by spreading the I/O across many disk drives. This behavior can be described as disk access contention. It is not locking contention.  Clustered Index Selection Scenario A scenario can illustrate clustered index selection. For example, a table contains an invoice date column, a unique invoice number column, and other data. About 10,000 new records are inserted into this table every day and the SQL queries often search this table for all records for one week of data. Many users have concurrent access to this table. The invoice number is not a candidate for the clustered index. The invoice number is unique and users do not usually search on ranges of invoice numbers, so placing invoice numbers physically in sequential order on disk is not appropriate. Next, the values for invoice number increase monotonically (1001,1002,1003, and so on). If the clustered index is placed on invoice number, inserts of new rows into this table occur at the end of the table beside the highest invoice number on the same physical disk location and create a hot spot.  Consider the invoice date column. To maximize sequential I/O, the invoice date column is a candidate for a clustered index because users often search for one week of data (about 70,000 rows). But from a concurrency perspective, the invoice date column may not be a candidate for the clustered index. If the clustered index is placed on an invoice date, all data tends to be inserted at the end of the table, and a hot spot can occur on the hard disk that holds the end of the table. The insertions at the end of the table are offset by the 10,000 rows that are inserted for the same date, so invoice date is less likely to create a hot spot than invoice number. Also, a hardware RAID controller helps spread out the 10,000 rows across multiple disks, which can also minimize the possibility of an insertion hot spot. There is no perfect answer to this scenario. You can place the clustered index on invoice date to speed up queries involving invoice date ranges even at the risk of hot spots. In this case, you should monitor disk queuing on the disks associated with this table for possible hot spots. It is recommended that you define the clustered index on invoice date because of the benefit to range scans based on invoice date and so that invoice numbers are not physically sequential on disk. In this example, a table consists of invoice number, invoice date, invoice amount, sales office where the sale originated, and other data. Suppose 10,000 records are inserted into this table every day and users often query invoice amounts based on sales office. Sales office should be the column on which the clustered index is created because that is the range on which scans are based. Newly inserted rows will have a mix of sales offices; inserts should be spread evenly across the table and across the disks on which the table is located. In some cases, range scans may not be an issue. For example, a very large employee table has employee number, social security number, and other data. As rows are inserted, employee number is incremented. There are 100,000 retrievals from this table every day and each retrieval is a single record fetch based on social security number. A nonclustered index created on social security number provides excellent query performance in this scenario. A clustered index on social security number provides slightly better query performance than the nonclustered index but may be excessive because range scans are not involved. If there will be only one index on this table, place the clustered index on the social security number column. The question then is whether to define a clustered index on this table. In earlier versions of SQL Server, it was important to define a clustered index on a table even if it was not required for queries because it helped with deleted row space recovery. This is not an issue with the SQL Server  space allocation algorithms and storage structures.  The recommendation in this example is to create the clustered index on social security number. The reason is that the social security number has data distributed so it does not follow the sequential pattern of employee number and social security number tends to have an even distribution. If a clustered index is created on this evenly distributed column data, then the employee records will be evenly distributed on disk. This distribution, in conjunction with FILLFACTOR and PAD_INDEX, which will be discussed later, provides open data page areas throughout the table to insert data. Assuming that newly inserted employee records have an even distribution of social security numbers, the employee table fills evenly and page splitting is avoided. If a column with even distribution does not exist on the table, it is worthwhile to create an integer column on the table and populate the column with values that are evenly distributed and then create the clustered index column. This “filler” or “dummy” column with a clustered index defined on it is not being used to query, but to distribute data I/O across disk drives evenly to improve table access concurrency and overall I/O performance. This can be an effective methodology with large and heavily accessed SQL tables. Another possible solution in this example is to not to create a clustered index on this table. In this case, SQL Server  manages all aspects of the space management. SQL Server finds a free space to insert the row, reuses space from deleted rows, and automatically reorganizes physical ordering of data pages on disk when it makes sense (to allow greater amounts of sequential I/O). The reorganization of data pages happens during database file autoshrink operations. For more information, see these topics in SQL Server  Books Online: Managing Space Used by Objects and Space Allocation and Reuse. FILLFACTOR and PAD_INDEX If a SQL Server database is experiencing a large amount of insert activity, you should plan to provide and maintain open space on index and data pages to prevent page splitting. Page splitting occurs when an index page or data page can no longer hold any new rows and a row must be inserted into the page because of the logical ordering of data defined in that page. When this occurs, SQL Server must divide the data on the full page and move about half of the data to a new page so that both pages have some open space. This consumes system resources and time. When indexes are built initially, SQL Server places the index B-tree structures on contiguous physical pages, which supports optimal I/O performance by scanning the index pages with sequential I/O. When page splitting occurs and new pages must be inserted into the logical B-tree structure of the index, SQL Server must allocate new 8-KB index pages somewhere. This occurs elsewhere on the hard disk drive and breaks up the physically sequential index pages. This switches I/O operations from sequential to nonsequential and cuts performance in half. Excessive page splitting should be resolved by rebuilding the index to restore the physically sequential order of the index pages. This same behavior can be encountered on the leaf level of the clustered index, which affects the data pages of the table. Use Performance Monitor to watch the SQL Server: Access Methods - Page Splits counter. Nonzero values for this counter indicate page splitting. Further analysis should be done with the DBCC SHOWCONTIG statement. For more information about how to use this statement, see this topic in SQL Server  Books Online: DBCC SHOWCONTIG. The DBCC SHOWCONTIG statement can reveal whether excessive page splitting has occurred on a table. Scan Density is the key indicator that DBCC SHOWCONTIG provides. This value should be as close to 100 percent as possible. If this value is below 100 percent, rebuild the clustered index on that table by using the DROP_EXISTING option to defragment the table. The DROP_EXISTING option of the CREATE INDEX statement permits re-creation of existing indexes and provides better index rebuild performance than dropping and recreating the index. For more information, see these topics in SQL Server  Books Online: CREATE INDEX and Rebuilding an Index. The FILLFACTOR option on the CREATE INDEX and DBCC DBREINDEX statements provides a way to specify the percentage of open space to leave on index and data pages. The PAD_INDEX option for CREATE INDEX applies what has been specified for FILLFACTOR on the nonleaf-level index pages. Without the PAD_INDEX option FILLFACTOR mainly affects the leaf-level index pages of the clustered index. You should use the PAD_INDEX option with FILLFACTOR. For more information, see these keywords in SQL Server  Books Online: page split and PAD_INDEX. The optimal value to specify for FILLFACTOR depends on how much new data is inserted into an 8-KB index and data page within a given timeframe. Keep in mind that SQL Server index pages typically contain many more rows than data pages because index pages contain only the data for columns associated with that index whereas data pages hold the data for the entire row. Also consider how often there will be a maintenance window that permits the rebuilding of indexes to avoid page splitting. Strive to rebuild the indexes only as the majority of the index and data pages have become filled with data by properly selecting clustered index for a table. If the clustered index distributes data evenly so that new row inserts into the table occur across all of the data pages associated with the table, then the data pages will fill evenly. This provides time before page splitting occurs and you must rebuild the clustered index. FILLFACTOR should be selected based partly on the estimated number of rows that will be inserted within the key range of an 8-KB page in a certain time frame and partly by how often scheduled index rebuilds can occur on the system.  You must make a decision based on the performance trade-offs between leaving a lot of open space on pages and page splitting. A small specified percentage for FILLFACTOR leaves large open spaces on the index and data pages, which helps avoid page splitting but also negates some performance gained by compressing data onto a page. SQL Server performs faster if more data is compressed on index and data pages because it can fetch more data with fewer pages and I/Os if the data is compressed on the pages. Specifying too high a FILLFACTOR may leave too little open space on pages and can allow pages to overflow too quickly, which causes page splitting. Before using FILLFACTOR and PAD_INDEX, remember that reads tend to outnumber writes, even in an online transaction processing (OLTP) system. Using FILLFACTOR slows down all reads, because it spreads tables over a wider area (reduction of data compression). Before using FILLFACTOR and PAD_INDEX, you should use Performance Monitor to compare SQL Server reads to SQL Server writes and use these options only if writes are a substantial fraction of reads (more than 30 percent). If writes are a substantial percentage of reads, the best approach in a busy OLTP system is to specify as high a FILLFACTOR as will leave a minimal amount of free space per 8-KB page but still prevent page splitting and allow the SQL Server to reach the next available time window for rebuilding the index. This method balances tuning I/O performance (keeping the pages as full as possible) and avoiding page splits (not letting pages overflow). You can experiment by rebuilding the index with varying FILLFACTOR values and then simulating load activity on the table to validate an optimal value for FILLFACTOR. After the optimal FILLFACTOR value has been determined, automate the scheduled rebuilding of the index as a SQL Server task. For more information, see this keyword in SQL Server  Books Online: creating a task. SQL Server Performance Tuning Tools Microsoft SQL Server version  includes several tools that can assist database administrators with performance tuning. Sample Data and Workload This example shows how to use SQL Server performance tools. First, the table is constructed: CREATE TABLE testtable (nkey1 int IDENTITY, col2 char(300) DEFAULT 'abc', ckey1 char(1)) Next, the table is loaded with 10,000 rows of test data: DECLARE @counter int SET @counter = 1 WHILE (@counter  BEGIN 	INSERT testtable (ckey1) VALUES ('a') 	INSERT testtable (ckey1) VALUES ('b') 	INSERT testtable (ckey1) VALUES ('c') 	INSERT testtable (ckey1) VALUES ('d') 	INSERT testtable (ckey1) VALUES ('e') 	SET @counter = @counter + 1 END These queries comprise the database server workload: SELECT ckey1,col2 FROM testtable WHERE ckey1 = 'a'  select nkey1,col2 FROM testtable WHERE nkey1 = 5000 SQL Server Profiler SQL Server Profiler records detailed information about activity occurring on the database server. SQL Server Profiler can be configured to watch and record one or many users executing queries on SQL Server and to provide a widely configurable amount of performance information, including I/O statistics, CPU statistics, locking requests, Transact-SQL and RPC statistics, index and table scans, warnings and errors raised, database object create/drop, connection connect/disconnects, stored procedure operations, cursor operation, and more. For more information about what SQL Server Profiler can record, see this keyword in SQL Server Books Online: SQL Server Profiler. Using SQL Server Profiler with Index Tuning Wizard SQL Server Profiler and Index Tuning Wizard can be used together to help database administrators create proper indexes on tables. SQL Server Profiler records resource consumption for queries into a .trc file. The .trc file can be read by Index Tuning Wizard, which evaluates the .trc information and the database tables, and then provides recommendations for indexes that should be created. Index Tuning Wizard can either automatically create the proper indexes for the database by scheduling the automatic index creation or generate a Transact-SQL script that can be reviewed and executed later. These are the steps for analyzing a query load: 	To set up SQL Server Profiler (Enterprise Manager) 1.	On the Tools menu, click SQL Server Profiler. 2.	On the File menu, point to New, and then click Trace. 3.	Type a name for the trace. 4.	Select Capture to file, then select a .trc file to which to output the SQL Server Profiler information.  	To run the workload (Enterprise Manager) 1.	On the Tools menu, click SQL Server Query Analyzer. 2.	Connect to SQL Server and set the current database to be where the table was created. 3.	Enter these queries into the query window of SQL Server Query Analyzer: 	SELECT ckey1,col2 FROM testtable WHERE ckey1 = 'a'  	SELECT nkey1,col2 FROM testtable WHERE nkey1 = 5000 1.	On the Query menu, click Execute.  	To stop SQL Server Profiler 1.	On the File menu, click Stop Traces. 2.	In the Stop Selected Traces dialog box, choose the traces to stop.  	To load the .trc file into the Index Tuning Wizard (SQL Server Profiler) 1.	On the Tools menu, click Index Tuning Wizard and then click Next. 2.	Select the database to analyze, then click Next. 3.	Make sure I have a saved workload file is selected, then click Next. 4.	Select My workload file, locate the .trc file created with SQL Server Profiler, click OK, and then click Next. 5.	In Select Tables to Tune, select the tables and then click Next. 6.	In Index Recommendations, select the indexes to create, and then click Next. 7.	Select the preferred option, then click Next. 8.	Click Finish. This is the Transact-SQL generated by Index Tuning Wizard for the sample database and workload: /* Created by: Index Tuning Wizard 	*/ /* Date: 9/7/98 			*/ /* Time: 6:42:00 PM 			*/ /* Server: HENRYLNT2 			*/ /* Database : test 			*/ /* Workload file : E:\Mssql\Binn\Profiler_load.sql */  USE [test]  BEGIN TRANSACTION CREATE CLUSTERED INDEX [testtable2] ON [dbo].[testtable] ([ckey1]) if (@@error <> 0) rollback transaction CREATE NONCLUSTERED INDEX [testtable1] ON [dbo].[testtable] ([nkey1]) if (@@error <> 0) rollback transaction COMMIT TRANSACTION The indexes recommended by Index Tuning Wizard for the sample table and data are expected. There are only five unique values for ckey1 and 2000 rows of each value. Because one of the sample queries (SELECT ckey1, col2 FROM testtable WHERE ckey1 = a) requires retrieval from the table based on one of the values in ckey1, it is appropriate to create a clustered index on the ckey1 column. The second query (SELECT nkey1, col2 FROM testtable WHERE nkey1 = 5000) fetches one row based on the value of the column nkey1. nkey1 is unique, and there are 10,000 rows; therefore, it is appropriate to create a nonclustered index on this column. SQL Server Profiler and Index Tuning Wizard are powerful tools in database server environments in which there are many tables and queries involved. Use SQL Server Profiler to record a .trc file while the database server is experiencing a representative set of queries. Then load the .trc file into Index Tuning Wizard to determine the proper indexes to build. Follow the prompts in Index Tuning Wizard to automatically generate and schedule index creation jobs to run at off-peak times. Run SQL Server Profiler and Index Tuning Wizard regularly (perhaps weekly) to see if queries executed on the database server have changed significantly, thus possibly requiring different indexes. Regular use of SQL Server Profiler and Index Tuning Wizard can keep SQL Server running in top form as query workloads change and database size increase over time. For more information, see these topics in SQL Server  Books Online: Index Tuning Wizard and Index Tuning Recommendations. Analyzing SQL Server Profiler Information SQL Server Profiler provides an option to log information into a SQL Server table. When it is complete, the table can be queried to determine if specific queries are using excessive resources. 	To log SQL Server Profiler information into a SQL Server table (Enterprise Manager) 1.	On the Tools menu, click SQL Server Profiler. 2.	On the File menu, point to New, and then click Trace. 3.	Type a name for the trace, then select Capture to Table. 4.	In the Capture to Table dialog box, enter a SQL Server table name to which to output the SQL Server Profiler information. Click OK. 5.	On the File menu, click Stop Traces. 6.	In the Stop Traces dialog box, choose the traces to stop. For more information, see these topics in SQL Server  Books Online: Viewing and Analyzing Traces, Troubleshooting SQL Server Profiler, Tips for Using SQL Server, Common SQL Server Profiler Scenarios, Starting SQL Server Profiler, and Monitoring with SQL Server Profiler. SQL Server Query Analyzer After the information is recorded into the SQL Server table, you can use SQL Server Query Analyzer to determine which queries on the system are consuming the most resources, and database administrators can concentrate on improving the queries that need the most help. For example, this query is typical of the analysis done on data recorded from SQL Server Profiler into a SQL Server table: select top 3 TextData,CPU,Reads,Writes,Duration from profiler_out_table order by cpu desc The query retrieves the top three consumers of CPU resources on the database server. Read and write I/O information, along with the duration of the queries in milliseconds is also returned. If a large amount of information is recorded with the SQL Server Profiler, you should create indexes on the table to help speed analysis queries. For example, if CPU is going to be an important criteria for analyzing this table, you should create a nonclustered index on CPU column. Statistics I/O SQL Server Query Analyzer provides a Show stats I/O option under the General tab of the Connections Options dialog box. Select this checkbox for information about how much I/O is being consumed for the query just executed in SQL Server Query Analyzer. For example, the query SELECT ckey1, col2 FROM testtable WHERE ckey1 = a returns this I/O information in addition to the result set when the Show stats I/O connection option is selected:  Table 'testtable'. Scan count 1, logical reads 400, physical reads 382, read-ahead reads 400. Similarly, the query SELECT nkey1, col2 FROM testtable WHERE nkey1 = 5000 returns this I/O information in addition to the result set when the Show stats I/O connection option is selected: Table 'testtable'. Scan count 1, logical reads 400, physical reads 282, read-ahead reads 400. Using STATISTICS I/O is a good way to monitor the effect of query tuning. For example, create the two indexes on this sample table as recommended by Index Tuning Wizard and then run the queries again. In the query SELECT ckey1, col2 FROM testtable WHERE ckey1 = a, the clustered index improved performance as indicated below. The query must fetch 20 percent of the table; therefore, the performance improvement is reasonable. Table 'testtable'. Scan count 1, logical reads 91, physical reads 5, read-ahead reads 32. In the query SELECT nkey1, col2 FROM testtable WHERE nkey1 = 5000, the creation of the nonclustered index had a dramatic effect on the performance of the query. Because only one row of the 10,000 row table must be retrieved for this query, the performance improvement with the nonclustered index is reasonable. Table 'testtable'. Scan count 1, logical reads 5, physical reads 0, read-ahead reads 0. ShowPlan ShowPlan can be used to display detailed information about what the query optimizer is doing. SQL Server  provides text and graphical versions of ShowPlan. Graphical ShowPlan output can be displayed in the Results pane of SQL Server Query Analyzer by executing a Transact-SQL query with Ctrl+L. Icons indicate the operations that the query optimizer will perform if it executes the query. Arrows indicate the direction of data flow for the query. Details about each operation can be displayed by holding the mouse pointer over the operation icon. The equivalent information can be displayed in text-based ShowPlan by executing SET SHOWPLAN_ALL ON. To reduce the query optimizer operation details from text-based ShowPlan, execute SET SHOWPLAN_TEXT ON. For more information, see these topics and/or keywords in SQL Server  Books Online: Understanding Nested Loops Joins, worktables, and showplan. Examples of ShowPlan Output Use the previous example queries and the set showplan_text on option in SQL Server Query Analyzer: Query: SELECT ckey1,col2 FROM testtable WHERE ckey1 = 'a' Text-based ShowPlan output: |--Clustered Index Seek(OBJECT:([test].[dbo].[testtable].[testtable2]), SEEK:([testtable].[ckey1]='a') ORDERED) This query takes advantage of the clustered index on column ckey1, as indicated by Clustered Index Seek.  Equivalent graphical showplan output:  If the clustered index is removed from the table, the query must use a table scan. The ShowPlan output below indicates the change in behavior: Text-based showplan output: |--Table Scan(OBJECT:([test].[dbo].[testtable]), WHERE:([testtable].[ckey1]='a')) Equivalent graphical showplan output:     Table scans are the most efficient way to retrieve information from small tables. But on larger tables, table scans indicated by ShowPlan are a warning that the table may need better indexes or that the existing indexes must have their statistics updated (by using the UPDATE STATISTICS statement). SQL Server  provides automatically updating indexes. You should let SQL Server automatically maintain index statistics because the maintenance helps guarantee queries will always work with good index statistics. Query: SELECT nkey1,col2 FROM testtable WHERE nkey1 = 5000 Text-based showplan output: |--Bookmark Lookup(BOOKMARK:([Bmk1000]), OBJECT:([test].[dbo].[testtable]))        |--Index Seek(OBJECT:([test].[dbo].[testtable].[testtable1]), SEEK:([testtable].[nkey1]=5000) ORDERED) Equivalent graphical showplan output:     This query uses the nonclustered index on the column nkey1, which is indicated by the Index Seek operation on the column nkey1. The Bookmark Lookup operation indicates that SQL Server must perform a bookmark lookup from the index page to the data page of the table to retrieve the requested data. The bookmark lookup was required because the query asked for the column col2, which was not part of the nonclustered index.  Query: SELECT nkey1 FROM testtable WHERE nkey1 = 5000 Text-based showplan output: |--Index Seek(OBJECT:([test].[dbo].[testtable].[testtable1]), SEEK:([testtable].[nkey1]=[@1]) ORDERED) Equivalent graphical showplan output:   This query uses the nonclustered index on nkey1 as a covering index. No bookmark lookup operation was needed for this query because all of the information required for the query (both SELECT and WHERE clauses) is provided by the nonclustered index. No bookmark lookup to the data pages is not required from the nonclustered index pages. I/O is reduced in comparison to the case in which a bookmark lookup was required. Performance Monitor Performance Monitor provides information about Windows and SQL Server operations occurring on the database server. For SQL Server specific counters, see your SQL Server documentation. In Performance Monitor graph mode, note the Max and Min values. Do not emphasize the average because polarized data points can affect this value. Study the graph shape and compare to Min and Max to gather an accurate understanding of the behavior. Use BACKSPACE to highlight counters. You can use Performance Monitor to log all available Windows NT and SQL Server performance monitor objects and counters in a log file and also view Performance Monitor interactively in chart mode. Setting a sampling interval determines how quickly the log file grows. Log files can get big fast (for example, 100 MG in one hour with all counters turned on and a sampling interval of 15 seconds). The test server should have a couple of gigabytes free to store these files. But if conserving space is important, try running with a large log interval of 30 or 60 seconds so Performance Monitor does not sample the system as often, and all of the counters are resampled with reasonable frequency but a smaller log file size is maintained. Performance Monitor also consumes a small amount of CPU and Disk I/O resources. If a system does not have a lot of disk I/O and/or CPU to spare, consider running Performance Monitor from another computer to monitor the SQL Server over the network. This applies to Performance Monitor graph mode only. When using log mode, it is more efficient to log Performance Monitor information locally on the SQL Server. If you must use log mode over the network, then consider reducing the logging to only the most critical counters. You should log all counters available during performance test runs into a file for analysis later. Configure Performance Monitor to log all counters into a log file and at the same time monitor the most interesting counters in one of the other modes, such as graph mode. Then all of the information is recorded but the most interesting counters are presented in an uncluttered Performance Monitor graph while the performance run is taking place. 	To start the logging feature (Performance Monitor) 1.	On the View menu, click Log. 2.	Click (+) button. 3.	In the Add to Log dialog box, select the counters to add to the log. 4.	Click Add, then click Done. 5.	On the Options menu, click Log. 6.	In File Name, enter the name of the file into which the performance information will be logged. 7.	Click Start Log.  	To stop the logging feature (Performance Monitor) 1.	On the Options menu, click Log. 2.	Click Stop Log.  	To load the logged information into Performance Monitor (Performance Monitor) 1.	On the View menu, click Log. 2.	On the Options menu, click Data From, and then select Log File. 3.	Click the browse (…) button, then double-click the file. 4.	In the Data From dialog box, click OK. 5.	On the View menu, click Chart, and then click the (+) button. 6.	Click on the button with the + sign on it. 7.	In the Add to Chart dialog box, add desired counters to the graphical display by highlighting the object/counter combination, and then click Add.  	To relate Performance Monitor logged events back to a point in time  1.	Follow the steps for How to load information into Performance Monitor. 2.	On the Edit menu, click Time Window. 3.	In the Input Log File Timeframe dialog box, you can adjust the start and stop time window of the logged data by clicking and holding down the mouse button on the slidebars. 4.	Click OK to reset the chart to display only data logged for the selected time window.   Key Performance Monitor Counters You can observe several Performance Monitor disk counters. To enable these counters, run the diskperf –y command from a Windows NT command window and restart Windows NT. The diskperf -y command does consume some resources on the database server but it is worthwhile to run the diskperf -y command on all production SQL Server servers so disk queuing problems can be confirmed immediately and all Performance Monitor counters are immediately available to diagnose disk I/O issues. If disk I/O counters are required, the diskperf -y command must be executed and Windows NT must be restarted before the disk I/O counters will report data in Performance Monitor. (Physical or Logical) Disk Queue > 2  Physical hard disk drives that experience disk queuing hold back disk I/O requests while they catch up on I/O processing. SQL Server response time is degraded for these drives, which costs query execution time.   If you use RAID, you must know how many physical hard disk drives are associated with each drive array that Windows NT interprets as a single physical drive so you can calculate disk queuing per physical drive. Ask a hardware expert to explain the SCSI channel and physical drive distribution in order to understand how SQL Server data is held by each physical drive and how much SQL Server data is distributed on each SCSI channel. There are several choices for looking at disk queuing in Performance Monitor. Logical disk counters are associated with the logical drive letters assigned by Disk Administrator, whereas physical disk counters are associated with what Disk Administrator sees as a single physical disk device. What looks like a single physical device to Disk Administrator may be either a single hard disk drive, or a RAID array, which consists of several hard disk drives. The Current Disk Queue counter is an instantaneous measure of disk queuing, whereas the    Average Disk Queue counter averages the disk queuing measurement over the Performance Monitor sampling period. Take note of any counter in which   Logical Disk: Average Disk Queue is greater than 2,  Physical Disk: Average Disk Queue is greater than 2,  Logical Disk: Current Disk Queue is greater than 2, or   Physical Disk: Average Disk Queue is greater than 2.   These recommended measurements are specified per physical hard disk drive. If a RAID array is associated with a disk queue measurement, the measurement must be divided by the number of physical hard disk drives in the RAID array to determine the disk queuing per physical hard disk drive. On physical hard disk drives or RAID arrays that hold SQL Server log files, disk queuing is not a useful measure because SQL Server log manager does not queue more than a single I/O request to SQL Server log file(s).  For more information, see this topic in SQL Server  Books Online: Monitoring Disk Activity. System: Processor Queue Length > 2 (per CPU)  When this counter is greater than 2, the server processors are receiving more work requests than they can handle as a group, therefore Windows must place these requests in a queue.  Some processor queuing can be an indicator of good SQL Server I/O performance. If there is no processor queuing and if CPU use is low, it can be an indication of a performance bottleneck somewhere in the system, and most likely in the disk subsystem. A reasonable amount of work in the processor queue means that the CPUs are not idle and the rest of the system is keeping pace with the CPUs. A general rule for determining an optimal processor queue number is to multiply the number of CPUs on the database server by 2.  Processor queuing significantly above 2 per CPU should be investigated. Excessive processor queuing costs query execution time. Eliminating hard and soft paging can help save CPU resources. Other methods that help reduce processor queuing include tuning SQL queries, picking better SQL indexes to reduce disk I/O (and hence CPU), or adding more CPUs (processors) to the system. Hard Paging - Memory: Pages/Sec > 0 or Memory: Page Reads/Sec > 5  Memory: Pages/Sec greater than 0 or Memory: Page Reads/Sec greater than 5 means that Windows is going to disk to resolve memory references (hard page fault), which costs disk I/O and CPU resources.  The Memory: Pages/Sec counter is a good indicator of the amount of paging that Windows is performing and the adequacy of the database server RAM configuration. A subset of the hard paging information in Performance Monitor is the number of times per second Windows had to read from the paging file to resolve memory references, which is represented by the Memory: Pages Reads/Sec counter. A value of Memory: Pages Reads/Sec greater than 5 is bad for performance. Automatic SQL Server memory tuning adjusts SQL Server memory use dynamically so that paging is avoided. A small number of pages per second is normal but excessive paging requires corrective action.   If SQL Server is automatically tuning memory, then adding more RAM or removing other applications from the database server are options to help bring the Memory: Pages/Sec counter to a reasonable level.   If SQL Server memory is being manually configured on the database server, it may be necessary to reduce memory given to SQL Server, remove other applications from the database server, or add more RAM to the database server.   Keeping Memory: Pages/Sec at or close to zero helps database server performance because Windows and all its applications (including SQL Server) are not going to the paging file to satisfy any data in memory requests so the amount of RAM on the server is sufficient. A Pages/Sec value slightly greater than 0 is not horrible but a relatively high performance penalty (Disk I/O) is paid every time data is retrieved from the paging file rather than from RAM.  You should understand the difference between the Memory: Pages Input/sec counter and the Memory: Pages Reads/sec counter. The Memory: Pages Input/sec counter indicates the actual number of Windows 4-KB pages being brought from disk to satisfy page faults. The Memory: Pages Reads/sec counter indicates how many disk I/O requests are made per second to satisfy page faults, which provides a slightly different point of view. So a single page read could contain several Windows 4-KB pages. Disk I/O performs better as the packet size of data increases (64 KB or more), so it can be worthwhile to consider these counters at the same time. You should also remember that for a hard disk drive, completing a single read or write of 4 KB costs almost as much time as a single read or write of 64 KB. Consider this situation: 200 page reads consisting of eight 4-KB pages per read could finish faster than 300 page reads consisting of a single 4-KB page. And we are comparing 1600 4-KB page reads finishing faster than 300 4-KB page reads. The key to all disk I/O analysis is to watch not only the number of disk bytes/sec, but also the disk transfers/sec. For more information, see   “Disk I/O Counters” and “The EMC Disk I/O Tuning Scenario” later in this document.  It is useful to compare the Memory: Page Input/sec counter to the Logical Disk: Disk Reads/sec counter across all drives associated with the Windows NT paging file, and the Memory: Page Output/sec counter to the Logical Disk: Disk Writes/sec counter across all drives associated with the Windows paging file. They provide a measure of how much disk I/O is strictly related to paging as opposed to other applications, such as SQL Server. Another way to isolate paging file I/O activity is to ensure the paging file is located on a separate set of drives from all other SQL Server files. Separating the paging file from the SQL Server files also can help disk I/O performance because disk I/O associated with paging can be performed in parallel to disk I/O associated with SQL Server. Soft Paging - Memory: Pages Faults/Sec > 0   If the Memory: Pages Faults/Sec counter is greater than 2, it indicates that Windows NT is paging but includes hard and soft paging within the counter. Soft paging means that there are application(s) on the database server that request memory pages still inside RAM but outside of the application’s Working Set. The Memory: Page Faults/Sec counter is helpful for deriving the amount of soft paging that occurs. There is no counter called soft faults per second. Instead, calculate the number of soft faults happening per second this way:   Memory: Pages Faults/sec - Memory: Pages Input/sec = Soft Page Faults per Second  To determine if SQL Server is causing excessive paging, monitor the Process: Page Faults/sec counter for the SQL Server process and note whether the number of page faults per second for Sqlservr.exe is similar to the number of pages per second. Because soft faults consume CPU resources, soft faults are better than hard faults for performance. Hard faults consume disk I/O resources. The best environment for performance is to have no faulting of any kind. Until SQL Server accesses all of its data cache pages for the first time, the initial access to each page causes a soft fault. Do not be concerned if soft faulting occurs under these circumstances. For more information, see “Monitoring Processors” in this document. For more information on memory tuning, see this topic in SQL Server Books Online: Monitoring Memory Usage. Monitoring Processors  Keep all server processors busy enough to maximize performance but not so busy that processor bottlenecks occur. The performance tuning challenge is to determine the source of the bottleneck. If CPU is not the bottleneck, then a primary candidate is the disk subsystem, so the CPU is being wasted. CPU is the most difficult resource to expand above a specific configuration level, such as four or eight on many current systems, so it is a good sign when CPU utilization is above 95 percent. In addition, the response time of transactions should be monitored to ensure they are within reason; if they are not, then greater than 95 percent CPU use can mean that the workload is too much for the available CPU resources and either the CPU must be increased or the workload must be reduced or tuned. Monitor the Processor: Processor Time % counter to ensure all processors are consistently below 95 percent utilization on each CPU. The System:Processor Queue counter is the processor queue for all CPUs on a Windows NT-based system. If the System: Processor Queue counter value is greater than 2 per CPU, there is a CPU bottleneck and it is necessary to either add processors to the server or reduce the workload on the system. Reducing workload can be accomplished by query tuning or improving indexes to reduce I/O and subsequently CPU usage. Another counter to monitor when a CPU bottleneck is suspected is System: Context Switches/sec because it indicates the number of times per second that Windows NT and SQL Server had to change from executing on one thread to executing on another. Context switching is a normal component of a multithreaded, multiprocessor environment, but excessive context switching slows down a system. Only be concerned about context switching if there is processor queuing. If processor queuing is observed, use the level of context switching as a gauge when you performance tune SQL Server. Consider using the lighweight pooling option so that SQL Server switches to a fiber-based scheduling model versus the default thread-based scheduling model. Think of fibers as lightweight threads. Use command sp_configure 'lightweight pooling', 1 to enable fiber-based scheduling. Watch processor queuing and context switching to monitor the effect. The DBCC SQLPERF (THREADS) statement provides more information about I/O, memory, and CPU use mapped to SPID. Disk I/O Counters  The Disk Write Bytes/Sec and Disk Read Bytes/Sec counters provides the data throughput in bytes per second per logical drive. Weigh these numbers carefully along with the values of the Disk Reads/Sec and Disk Writes/Sec counters. Do not let a low number of bytes per second lead you to believe that the disk I/O subsystem is not busy. A single hard disk drive is capable of supporting a total of 75 nonsequential and 150 sequential disk reads and disk writes per second. Monitor the Disk Queue Length counter for all drives associated with SQL Server files and determine which files are associated with excessive disk queuing. If Performance Monitor indicates that some drives are not as busy as others, you can move SQL Server files from drives that are bottlenecking to drives that are not as busy. This spreads disk I/O activity more evenly across hard disk drives. If one large drive pool is used for SQL Server files, than the resolution to disk queuing is to make the I/O capacity of the pool bigger by adding more physical drives to the pool. Disk queuing may be a symptom that one SCSI channel is saturated with I/O requests. Performance Monitor cannot directly detect if this is true. Hardware vendors can provide tools to detect the amount of I/O serviced by a RAID controller and whether the controller is queuing I/O requests. This is more likely to occur if many disk drives (10 or more) are attached to the SCSI channel and they all perform I/O at full speed. To resolve this issue, take half of the disk drives and connect them to another SCSI channel or RAID controller to balance the I/O. Rebalancing drives across SCSI channels requires a rebuild of the RAID arrays and full backup and restore of the SQL Server database files. Performance Monitor Graph Output     The chart shows typical counters that Performance Monitor uses to observe performance. The Processor Queue Length counter is being observed. Click BackSpace to highlight the current counter. This helps to distinguish the current counter from other counters being observed and can be particularly helpful when observing many counters at the same time with Performance Monitor. The Max value for Processor Queue Length is 22.000. Max, Min, and Average values for the Performance Monitor Graph cover only the current time window for the graph as indicated by Graph Time. By default, Graph Time covers 100 seconds. To monitor longer periods of time and to be sure to get representative Max, Min, and Average values for those time periods, use the logging feature of Performance Monitor. The shape of the Processor Queue Length graph line indicates that the Max of 22 occurred only for a short period of time. But there is a period preceding the 22 value when Processor Queue Length is greater than 5, 100 percent is 22 and there is a period prior to the 22 value when the graph has values above 25 percent, which is approximately five. In this example, the database server SQLCON has only one processor and should not sustain Processor Queue Length greater than two. Therefore, Performance Monitor indicates that the processor on this computer is being overtaxed and that further investigation should be made to reduce the load on the processor or more processors should be added to SQLCON to handle these periods of higher processor workloads. Other Performance Topics This section presents other factors that can influence the performance of Microsoft SQL Server. Reduce Network Traffic and Resource Consumption  Database programmers who work with SQL Server relatively easy to use interfaces such as Microsoft ActiveX Data Objects (ADO), Remote Data Objects (RDO), and Data Access Objects (DAO) database APIs must remain aware of the result sets they are building. ADO, RDO, and DAO provide programmers with database development interfaces that allow rich database rowset functionality without requiring a lot of database programming experience. Programmers can encounter performance problems if they neither account for the data their application is returning to the client nor stay aware of where the SQL Server indexes are placed and how the SQL Server data is arranged. SQL Server Profiler, Index Tuning Wizard, and ShowPlan are helpful tools for pinpointing and fixing problem queries. Look for opportunities to reduce the size of the result set by eliminating columns in the select list that do not have to be returned, or by returning only the required rows. This reduces I/O and CPU consumption.  For more information, see these topics in SQL Server  Books Online: Optimizing Application Performance Using Efficient Data Retrieval, Understanding and Avoiding Blocking, and Application Design. Deadlocking  If applications accessing SQL Server are built so transactions access tables in the same chronological order across all user transactions, you may avoid deadlocking. You should explain the concept of chronological table access to SQL Server application developers as early as possible during the application design process to avoid deadlocking problems, which are expensive to solve later.  By reducing SQL Server query I/O and shortening transaction time you can help prevent deadlocking. This can make queries faster, lock resources are held for a shorter period of time, and reduce all locking contention (including deadlocking). Use the SQL Server Query Analyzer Show stats I/O option to determine the number of logical page fetches associated with large queries. Use the SQL Server Query Analyzer Show query plan option to review index use and then consider a SQL Server query redesign that is more efficient and uses less I/O. For more information, see these topics in SQL Server  Books Online: Avoiding Deadlocks, Troubleshooting Deadlocking, Detecting and Ending Deadlocks, and Analogy to Nonserializable Transactions. Language to Avoid in Queries Use of inequality operators in SQL queries forces databases to use table scans to evaluate the inequalities. These queries generate high I/O if they run regularly against large tables. For example, using any WHERE expression with NOT in it: WHERE   != some_value WHERE   <> some_value If you must run these queries, restructure the queries to eliminate the NOT keyword or operator.  Instead of using: SELECT * FROM tableA WHERE col1 != 'value' Try using: SELECT * FROM tableA WHERE col1   'value' SQL Server can use the index (preferably clustered), if it is built on col1, rather than resorting to a table scan. Smart Normalization On frequently accessed tables, move columns that a database application does not need regularly to another table. By eliminating as many columns as possible, you can reduce I/O and increase performance. For more information, see these topics in SQL Server  Books Online: Logical Database Design and Normalization. Partitioned Views You can horizontally partition tables through views in SQL Server . This provides I/O performance benefits when database users query subsections of large views. For example, if a large table documents sales for all sales departments for a year and that retrievals from this table are based on a single sales department, a partitioned view could be employed. A sales table is defined for each sales department, a constraint is defined on the sales department column on each table, and then a view is created on all of the tables to form a partitioned view. The query optimizer uses the constraint on the sales department column. When the view is queried, all of the sales department tables that do not match the sales department value provided in the query are not ignored by the query optimizer and no I/O is performed against those base tables. This improves query performance by reducing I/O. For more information, see these topics in SQL Server  Books Online: Scenarios for Using Views, CREATE VIEW, Using Views with Partitioned Data, Modifying Data Through a View, Copying To or From a View, and Partitioning. Replication and Backup Performance If you ensure that the disk I/O subsystem and CPUs are performing well, you ensure performance benefits to all SQL Server operations, including replication and backups. Transactional replication and transaction log backups read from transaction log files. Snapshot replication and backups perform serial scans of database files. The SQL Server  storage structures have made these operations fast and efficient, as long as there is no queuing occurring in the database server CPUs or disk subsystems. For more information, see these topics and/or keywords in SQL Server  Books Online: Optimizing Backup and Restore Performance, Creating and Restoring Differential Database Backups, Creating and Applying Transaction Log Backups, Using Multiple Media or Devices, Minimizing Backup and Recovery Times in Mission-Critical Environments, Backup/Restore Architecture, SQL Server  on Large Servers, and replication performance. The EMC Disk I/O Tuning Scenario For those implementing SQL Server database systems on the unique EMC Symmetrix Enterprise Storage Systems, some disk I/O balancing methods can help avoid disk I/O bottleneck problems and maximize performance. Symmetrix storage systems contain up to 16 GB of RAM cache and contain internal processors within the disk array that help speed the I/O processing of data without using host server CPU resources. You must understand the four components in the Symmetrix storage system to balance disk I/O. One component is the 16-GB cache inside the Symmetrix. Up to 32 SA channels can be used to cable up to 32 SCSI cards from Windows NT-based host servers into the Symmetrix; all of these SA channels can be requesting data simultaneously from the 16-GB cache. Within the Symmetrix storage system, there are up to 32 connectors called DA controllers (internal SCSI controllers) that connect all of the internal disk drives within the Symmetrix into the 4-GB internal cache. And finally, there are the hard disk drives within the Symmetrix.  EMC hard disk drives are SCSI hard disk drives with the same I/O capability of the other SCSI drives referred to in this document. One feature commonly used with EMC technology is referred to as hyper-volumes. A hyper-volume is the logical division of an EMC hard disk drives, so the hyper-volume looks like another physical drive to the Windows NT Disk Administrator and can be manipulated with Windows NT Disk Administrator like any other disk drive. Multiple hyper-volumes can be defined on each physical drive. You should work closely with EMC field engineers to identify how hyper-volumes are defined when conducting database performance tuning on EMC storage. You can overload a physical drive with database I/O if you think two or more hyper-volumes are separate physical drives but actually are two or more hyper-volumes on the same physical drive. SQL Server I/O activities should be divided evenly among distinct DA controllers because DA controllers are assigned to a defined set of hard disk drives. DA controllers are not likely to suffer an I/O bottleneck, but the set of hard disk drives associated with a DA controller may be more susceptible. SQL Server disk I/O balancing is accomplished the same way with DA controllers and their associated disk drives as with other vendors disk drives and controllers.  To monitor the I/O on a DA channel or separate physical hard disk drives, get help from EMC technical support staff because I/O activity occurs beneath the EMC internal cache and is not visible to Performance Monitor. EMC storage units have internal monitoring tools that allow an EMC technical support engineer to monitor I/O statistics within the Symmetrix. Performance Monitor can only see I/O coming to and from an EMC storage unit by the I/O coming from a SA channel. This is enough information to indicate that a SA channel is queuing disk I/O requests, but is not enough information to determine the disk or disks that are causing the disk queuing. If SA channel is queuing, it may be the disk drives and not the SA channel that is causing the bottleneck. One way to isolate the disk I/O bottleneck between the SA channels and the DA channels and drives is to add a SCSI card to the host server and connect it to another SA channel. If Performance Monitor indicates that I/O across both SA channels has not changed in volume and disk queuing is still occurring, then the SA channels are not causing the bottleneck. Another way to isolate the I/O bottleneck is to have an EMC engineer use EMC monitoring tools to monitor the EMC system and analyze the drives or DA channels that are bottlenecking. Divide SQL Server activities evenly across as many of disk drives as are available. If you work with a smaller database that sustain a large amount of I/O, consider the size of hyper-volume to have EMC technical engineers define. Suppose the SQL Server will consist of a 30-GB database. EMC hard disk drives can provide up to 23 GB in capacity, so you can fit the entire database onto 2 drives. For manageability and cost, this is appealing; but for I/O performance, it is not. An EMC storage unit can work with more than 100 internal drives. Involving only 2 drives for SQL Server can lead to I/O bottlenecks. Consider defining smaller hyper-volumes, perhaps of 2 GB each. Then approximately 12 hyper-volumes can be associated with a given 23-GB hard disk drive. Assuming 2-GB hyper-volumes, 15 hyper-volumes are required to store the database. Make sure that each hyper-volume is associated with a separate physical hard disk drive. Do not use 12 hyper-volumes from 1 physical drive and then 3 hyper-volumes associated on another physical drive, because this is the same as using 2 physical drives (150 nonsequential I/O / 300 sequential I/O across the 2 drives). But with 15 hyper-volumes, each of which are associated with a separate physical drive, SQL Server uses 15 physical drives for providing I/O (1125 nonsequential / 2250 sequential I/O activity per second across the 15 drives). Also consider employing several SA channels from the host server to divide the I/O work across controllers for host servers that support more than a single PCI bus. Consider using one SA channel per host server PCI bus to divide I/O work across PCI buses as well as SA channels. On EMC storage systems, each SA channel is associated with a specific DA channel and a specific set of physical hard disk drives. Because SA channels read and write their data to and from the EMC internal cache, it is unlikely the SA channel is a point of I/O bottleneck. Because SCSI controller bottlenecks are not likely, it is probably best to concentrate on balancing SQL Server activities across physical drives rather than worry about how many SA channels to use.   To find more information on this subject mail to  sqlcon@yahoogroups.com 
     
    
</post>

<date>22,April,2004</date>
<post>

     
      Hi      From MUGH and SQLCON we would like to thank all who participated in the yesterday discussion.    Am attaching few links which where part of our discussion and other helpful links that could be useful for performance tuning.   Join Hints            urlLink http://www.mssqlcity.com/Articles/General/NestLoop.htm          urlLink  http://www.mssqlcity.com/Articles/General/HashJoins.htm         urlLink http://www.mssqlcity.com/Articles/General/mergeJoins.htm      Other helpful links     ·          Inside SQL Server 2000/7.0 by Kalen Delaney,  urlLink http://www.insidesqlserver.com    ·          The Guru's Guide to Transact-SQL,  urlLink http://www.khen.com/books.htm    ·          SQL Server Performance and Scalability References,  urlLink http://www.microsoft.com/sql/evaluation/compare/performance.asp    ·          Understanding Performance Testing,  urlLink http://msdn.microsoft.com/library/techart/d5dplyover.htm    ·          224587 INF: Troubleshooting Application Performance with SQL Server  urlLink http://support.microsoft.com/?id=224587    ·          243586 INF: Troubleshooting Stored Procedure Recompilation  urlLink http://support.microsoft.com/?id=243586    ·          298475 INF: Information Required to Successfully Troubleshoot Application http://support.microsoft.com/?id=298475   ·          175658 How To Gather Information for Effective Troubleshooting of Performance http://support.microsoft.com/?id=175658   ·          319942 HOW TO: Determine Proper SQL Server Configuration Settings http://support.microsoft.com/?id=319942   ·          297864 INF: Performance Considerations for an Upgrade from SQL Server 6.5 http://support.microsoft.com/?id=297864   ·          314648 INF: Differences in STATISTICS IO, SQL Profiler and Sysprocesses IO http://support.microsoft.com/?id=314648   ·          311826 INF: Index Tuning Wizard Best Practices http://support.microsoft.com/?id=311826   ·          Index Tuning Wizard 2000, http://msdn.microsoft.com/library/techart/itwforsql.htm   ·          Index Tuning Wizard 7.0, http://msdn.microsoft.com/library/techart/msdn_sqlindex.htm   ·          Troubleshooting the Index Tuning Wizard, http://msdn.microsoft.com/library/psdk/sql/tr_servtools_48ro.htm   ·          195565 INF: How SQL Server 7.0 and SQL Server 2000 Autostats Work http://support.microsoft.com/?id=195565   ·          197297 INF: Comparison of Join Techniques http://support.microsoft.com/?id=197297   ·          139609 PerfMon: High Number of Pages/Sec Not Necessarily Low Memory http://support.microsoft.com/?id=139609   ·          268343 Umdhtools.exe: How to Use Umdh.exe to Find Memory Leaks http://support.microsoft.com/?id=268343   ·          286350 HOWTO: Use Autodump+ to Troubleshoot "Hangs" and "Crashes" http://support.microsoft.com/?id=286350   ·          316365 INFO: ROADMAP for How to Use the .NET Performance Counters http://support.microsoft.com/?id=316365   ·          150934 How to Create a Performance Monitor Log for NT Troubleshooting http://support.microsoft.com/?id=150934   ·          130926 Using Performance Monitor to Identify a Pool Leak http://support.microsoft.com/?id=130926   ·          146005 Optimizing Windows NT for Performance http://support.microsoft.com/?id=146005   ·          248345 How to Create a Log Using System Monitor in Windows 2000 http://support.microsoft.com/?id=248345   ·          275685 INF: Importing Performance Monitor Logs into SQL Server http://support.microsoft.com/?id=275685         Regards,  Viswanath  MUGH   Home Page      urlLink http://www.mugh.net    SQLCON     urlLink http://www.mugh.net/sql  
     
    
</post>

<date>23,April,2004</date>
<post>

     
       Check out the cool guide introduced by Microsoft for handling the Production issues on SQL 2000.  This guide is presented within the structure of the Microsoft Operations Framework (MOF) methodology. There are numerous documents that should be read to fully understand MOF and its related Frameworks, and a summary has been provided in the next section. MOF is a representation of the cyclical process that any operation goes through, and it has been broken into four quadrants: changing, operating, supporting, and optimizing.     urlLink Download it from Here    A fantastic guide that can revolutionize the work you handle your SQL support process  
     
    
</post>

<date>27,May,2004</date>
<post>

     
      Before planning for the capacity of the SQL Server we must include the following details   1.	Resource Requirements. 2.	Process Flow Chart. 3.	System Classification. 4.	CPU and Memory Management. 5.	Disk Planning. 6.	Data Storage Subsystem Management. 7.	Database File Placement. 8.	Monitoring For capacity management.   1.	Resource Requirements: -  By Resource Requirements we must be able to recognize and use the various resource heads available around us. Here we are talking of knowledge and Skill Set domains, which can be of effective role while planning for capacity management. They are as follows: -   (A)	Network domain administrator: - He should have the perspective of network bandwidth and when application is online how much congestion it can create. Also for the ports and other network issues related.  (B)	Vendor Representative: - you must get the critical technical know how about the hardware supplied by your vendor. It is not the matter of financial perspective only but from the performance/scaling/reliability perspective as well. One must get the docs for best practices involved with the respective Hardware.  (C)	Application Developer: - They are the people from whom any change in the application scenario will come .So interaction with them is critical. Any change in Data access or Database schema has to be logged and carefully analyzed for performance and other side effected.   3. System Classification  We must create a standardized classification system, intended for a common reference point in dealing with the transactional volume, read/write activity, and data mass (size).So, rather than describing a system as a “small reporting system,” you can more accurately identify your system in relation to others.     The generic codes used here are simply for the purpose of providing an example. To create a classification code that is specific to your system, use the following measures:  Transactions per second (TPS): -  (For the purposes of this paper, TPS is measurable through the \SQLServer:Databases (Total)\Transactions/sec counter in System Monitor.   You can also observe transaction statistics by analyzing a trace containing Transaction Event Class data, or by creating a customized counter specific to your system.)  The read/write ratio The total size of the related database files (mass)  The measurement of the database size (mass) can be applied at any level in the chart. If you have a terabyte of data, you would indicate this by adding the prefix “1T” to the classification code, like this: 1T 7000 R100. If you have 100 GB of data, the code might look like this: 100G 500 R20.  4.CPU and Memory Capacity Management: -  Some of the most critical components included in any system are CPU and Memory usage. Based on the historical performance data and track record of the growth requirement of resources, One can forecast and predict the usage of both.  CPU Planning Processor planning is fairly straightforward. Monitor your current CPU utilization (\\Processor (_Total)\% Processor Time). If the average is over 50 percent, if you have frequent peak usage periods when the current CPU utilization spikes over 90percent, or if you have a situation in which the usage spikes and stays up for a While, then you should consider adding either additional or faster processors.  In general, the processors you choose should be able to deliver the speed implied in your other system purchases. If your system is highly specialized and filled with processor-intensive activities, you will become aware of that as you observe the system over time. Examples of such activities include extensive or frequent usage of Data Transformation Services, or anything involving many calculations (science, accounting, and so on). SQL Server is a CPU-intensive application, so look for processors with a large high-speed cache. Always get the fastest and newest when it comes to processing power, because the processor enables the rest of the server to do its job well.   If you have a dedicated SQL Server computer, use all the processors for SQL Server. If your system is running applications in addition to SQL Server (such as Microsoft Commerce Server), then consider restricting SQL Server from using one or more processors. Otherwise, allow SQL Server and Windows to balance across all processors, as they were designed to do.   Memory Planning  While the sum of all hardware together dictates the capacity of a system, memory serves mainly to optimize data access. SQL Server uses memory to store execution plans, store data pages between uses, and so on. Without enough memory, you will incur more disks I/O in reading data. If your system does many reads, you might reduce disk I/O by significantly increasing your memory, because the data will then remain in cache. Insufficient memory, or over-allocation of memory, can result in Paging. Memory plays an important role in SQL Server, and it is a resource you should carefully monitor.  For systems such as decision support systems (DSS) for which reads are the highest priority, more memory is better. Memory can be used to compensate for disk I/O, and large amounts of memory can significantly decrease the number of disk spindles you will need to achieve high performance.  For systems such as online transaction processing (OLTP) systems for which writes are the highest priority, memory is still an important part of the system, but you may benefit more from the addition of disk spindles and more or faster controller channels, rather than memory. Carefully monitor your system to see which resources are in highest demand.  5. Disk Planning  The important point to remember about data storage is that the number of disks is far more important than the total storage size of the disks.   At any moment the raw space required for any system is defined as per the following equation  Minimum Disk Space required =   Size of Data (per Database, including the system databases) + Size of Indexes (per Database, including the system database) +  Planned growth  + MS DTC logging space  +  Amount of OS Reserved Space  + Amount Reserved for hardware Optimization  To know the system we must know the way the Application is using its Databases not just user defined but system databases as well like TEMPDB and MSDB.  One big physical disk may hold all your data, but it still has only one disk arm to execute, individually, each data request.   The more disk arms you have, the better off you will be. So, when you size for new disks, do a quick check to be sure this is enough drive space; but spend more time on analyzing how many spindles you really need. For example, if your system performs a lot of transactions, you will enhance performance by adding more spindles (provided there is sufficient memory and CPU to support the system).  When you are ordering your hardware, request a specific number of disks, rather than a specific amount of disk space. Having more small disks is better than having fewer large disks. If you have external storage, go for the fastest array controller card, and one that has multiple channels. Look at this card as a potential bottleneck:  If you have multiple spindles, you need to invest in a card that can support them. The performance you achieve will be directly proportional to the quality of the controller, and the type of I/O your system produces.   In OLTP, you can have more disks per controller card, which means the disk spends more time looking for the data, and the controller channel will not become so saturated.   DISK CONTROLERS  Cache Not all write caching is safe for use by a database server. Make sure that your disk controller has features such as safeguards against uncontrolled reset of the caching controller, on-board battery backup, and mirrored or error-checking-and-correcting memory.  Do not implement write caching unless the hardware vendor guarantees that their write cache includes these features and any others required to prevent data loss.  Array accelerator cache settings can be left at the default value, which is typically 50:50 read:write. These settings can also be adjusted to favor reads or writes if you know which your system requires. Note that if you are using a write setting above Zero here, you have enabled write caching.   If your array configuration controller supports using more than one channel, make sure you take advantage of it. Fast channels have a tremendous effect on I/O performance.  Windows NT File System (NTFS) Allocation Unit  SCSI Drives: When you format the new drives in Disk Administrator, you should consider an allocation unit, or block size, that will provide optimal performance.   Significant performance gains may be obtained by sizing this to a larger value in order to reduce disk I/O; however, the default value is based on the size of the physical disk. The best practice for SQL Server is to choose 64 KB, because this reduces the likelihood of I/O that spans distinct NTFS allocations, which then might result in split I/O.   Keep in mind that although this information can be useful, the type of storage you are using (and in some cases your backup software) will drive the format of your disks. If you are changing the block size on an existing system, be sure to run a baseline in your test environment and another after you have tested the changes.   6. Data Storage Subsystem Management  When Ever you are opting for the disk storage consult the Run Book as well Point 12,which discusses RAID. We shouldn’t be bothered only about the space requirements in GB-S and TB-s but also on the physical Drive setup. We must prefer Hard ware RAID as compared to the OS Raid. Reason being the Hardware RAID gives more redundancy if any hardware crashes as well as has performance benefit. OS Raid uses Processor Cycles and hampers SQL from performing to the best.  Two core RAID levels are of value for a database server: striping with parity (RAID 5) and striped mirror (RAID 0+1). The best overall option is to choose RAID 0+1 (also called RAID 01 or “striped mirror”). RAID 5 can be used in certain circumstances, but is generally more expensive and less reliable in the long run.  In RAID 5, each time data is written to disk, it actually takes four I/O operations to create the read data and parity blocks, and the write data and parity blocks. This is slow for two reasons: First, each process is consecutive, so they must wait for each other; second, this operation occurs while many other transactions are vying for the disk resources. RAID 0+1 writes to the primary disk and the mirror in one operation. Although you do have to wait for the write to complete on both drives, both writes are simultaneous.  Moreover RAID 5 has performance hit in writing data. Only advantage is that you get more space at less cost as compared with RAID 0 + 1.   Although data can be restored from database backups, it is important to note the effect that failed drives can have. If any two disks fail in RAID 5, the database will stop (unless you have a hot standby disk that has been synced within the chain, but in any case, you cannot lose more than one disk from the whole working set).  RAID 0+1 will stop the database only if a disk fails in both sides of a mirrored set at the same time, and the odds of that occurring based on random factors are about 5.3 percent. RAID 5 imposes a significant penalty for losing even one disk. When one drive is lost on a RAID 5 system, the read performance of the system immediately decreases.   Every read or write request to the failed drive initiates a verification process against all other drives in the parity group.   This performance degradation exists until the drive is replaced and completely rebuilt by the system. During the rebuild process, the system is more sensitive to system load due to the considerably heavier I/O requirements of the failed system.   This can be a critical consideration. RAID 0+1 sees minimal loss of performance in a failed state where the hardware allows reads from both disks in a set. In this case, read performance is slightly reduced,  but only for data stored on that particular set. RAID 0+1 can actually read simultaneously from both drives in a mirrored set. This is not a simultaneous read for the same I/O operation, but for different ones.   So when you have multiple read requests for the same physical disk, the I/O operations are spread over the two disks in the mirrored set.   7. Database File Placement.  As discussed in Run book About the Database file groups placement.we are just reemphasizing the same here.  A. For SCSI and SAN systems, the most important considerations in determining where to place your files on the server are the number of disk spindles available to a particular drive and the speed of the drives involved. For this reason, it is good to design the server hardware requirements and/or layout with your database needs in mind. Be careful about buying hardware before you have a firm design plan.  B. If you have a set of tables that is used together frequently, consider putting these tables in separate filegroups on separate physical drives, to balance I/O between them. In a larger, more heavily used system; this could make a significant difference.  C. If disk I/O is a problem, and you cannot add more spindles to the set of disks, consider putting non-clustered indexes in a separate filegroup on a separate disk, in order to split I/O between filegroups.  D. Group your tables based on usage, in order to generate as many simultaneous reads to different filegroups (and therefore disks) as possible. Grouping tables into filegroups based on a maintenance need for convenient backup plans will not generate as much performance as separating the tables and indexes by usage.  E. If you have more than enough spindles for your data performance, consider breaking the data across filegroups on this set of disks, for the purpose of speeding up some of your administrative tasks, such as reindexing. The main reason to do this, however, is to speed up any necessary restores. Microsoft SQL Server 2000 Operations Guide 160  G.  For systems smaller than Class 1000A, you could use Auto Grow for your database files. For systems that fit in this class or above, keep in mind that when a “grow” is initiated, transactions must wait while the database grows.  In a small database or lightly queried system this is not a big issue, but if you have a 100 GB OLTP database set to grow in 10 percent increments, and it runs out of space during peak times, the online users will be held up while the 10 GB is allocated. (Allocation speed per GB can be measured by copying a 1 GB file to the data drive on that server.) For these systems, the best practice is to anticipate database growth and manually increase the database at a scheduled time. Or, choose a reasonable amount to grow by that is neither too cumbersome nor so small that it will initiate expansion too frequently.  Ideally, the best plan is to expand your database for six to twelve months’ growth, or whatever seems feasible for the size of your data and the rate of growth compared to the budget and hardware available. Although it is administratively easier to let the system autogrow, there are two risks associated with this:   H. If you autogrow to the point that the disk fills beyond 80 percent, you will experience performance degradation as the disk fills beyond that. I. If the disk fills completely or to the point that the database cannot grow by the allotted amount or percentage, the database will stop. If this occurs, the only option is to make more physical space available by adding storage:   If the data disk is configured as a dynamic disk, you can add more disks from your emergency spares and expand the array and the logical disk. If it is configured as a basic disk, such as in the case of a failover cluster, then you can add sufficient disks to make a new RAID set, and create another file on that drive and add it into the database file group.  J. If the transaction logs disk fills completely or to the point where it cannot autogrow in the amount of space left, the database will stop. To rectify this, you must analyze whether you have an unusual problem that can be resolved by dumping and shrinking the transaction log  K. If you have multiple files in your filegroup and you add another one, you will need to expand each of them in order to re-establish proportional fill.  LOG FILE PLACEMENT  (Important)  Here are a few tips and best practices regarding the placement of your log files:  ·	Create the transaction log on a physically separate disk or RAID array. The transaction log file is written sequentially; therefore, using a separate, dedicated disk allows the disk heads to stay in place for the next write operation. For this reason, smaller systems will do well by using a single mirrored disk for the transaction log. A single mirrored physical disk should support up to approximately 1,000 transactions per second, depending on the speed of the disk itself. Systems requiring more than that should stripe the transaction log across a RAID 0+1 array for maximum performance. For highest bandwidth, the RAID controller on this array should have a (battery-backed) write-back cache to speed log writes.  ·	Set your transaction log to autogrow, but try to size it so it should not need to grow. Base the optimal size on your recovery model, the level of logged activity in the database, and the interval of time between backups. Set the growth increment to a reasonable percentage, but try to anticipate when the log should be resized. If the transaction log expands too frequently or takes a long time to expand, performance can be affected.  ·	Base the size of the log on your current recovery model and your application design. If you find that you need to shrink the log periodically, investigate what is causing the log to fill up, in order to fix the problem rather than simply fixing the symptom.    TEMPDB FILE PLACEMENT  Here are a few tips and best practices regarding the placement of your tempdb files:  ·	Place the tempdb database on a fast I/O subsystem to ensure good performance. Stripe the tempdb database across multiple disks for better performance. Move the tempdb database to disks different from those used by user databases.  ·	The tempdb database can be located with the data in most situations. For larger systems that make heavy use of tempdb, consider putting tempdb on a set of disks by itself, to achieve extra performance. It is not a good idea to co-locate any database files with the page file of the operating system.   8. Monitoring for capacity management. From the capacity point of view we must monitor following points and store the historic data and forecast accordingly.  ·	Disk drive capacities ·	Database sizes and amount of free space inside each database ·	Comparison of the database size to the disk drive space ·	Rate of database growth ·	Location (drives) of data files   
     
    
</post>

<date>15,June,2004</date>
<post>

     
      To convert the format of a Date Time String transformation   1. On the Transformations tab of the Transform Data Task Properties or Data Driven Query Task Properties dialog box, click the Source column containing the date or time to be modified, and then click the Destination column where you want the modified string to be placed.   2. Do one of the following:   --If there is a mapping arrow connecting the two columns, click Delete, and then click New. --If there is no mapping arrow, click New.   3. In the Create New Transformation dialog box, click DateTime String.  4. Click the General tab, and then click Properties.  5. In the Date Format list, select the format you want.   6. Click Naming to display the Calendar Names dialog box, where you can select long or short day or month names and the A.M. and P.M. designators you want.  7. In the Language list, select the language you want, and then click Set Language Defaults.  
     
    
</post>

<date>15,June,2004</date>
<post>

     
      It happened in UK last month that 137 (UDP), 138 (UDP), 139 (TCP), 445 (UDP & TCP), 593 (TCP), 1433 (TCP), 1434 (UDP), 27374 (TCP)  were blocked by NTL broadband services on the pretext of Virus vulnerability. This inturn affected MS SQL users a lot as 1433 was blocked.  I request the whole group to debate this issue. I am putting the initial crux of these articles as they appear on the NTL Broadband Site.  However those who wish to browse can do the same at  1.  http://www.cooljules.co.uk/ntl.htm  To my horror on the 8th of June 04 NTL have shut down port 1433 across their network on a permanent basis. Microsoft SQL Users can no longer develop software via ODBC across the Internet or use Enterprise manager. (they have also stopped other ports such as exchange)  This article refers to why NTL have done this:   There main reason seems to be to stop viruses! Which now, I can't wait to see if they shut down port 80, 110, 25 and 21 since viruses run on those too. They stopped access without advanced warning stopping my ability to work from home. I now have to find another provider if I cannot convince them to change their mind.  I have huge long term concerns about the use of global port blocking via comms companies and I hope the they don't decide to shut down all but 4 ports of the Internet. Whats next? MSN, Online gamming disabled...Who knows and NTL will do this without any warning. I think Microsoft should put up a stand against their products been blocked by companies like NTL. They havent blocked competitor databases such as MySQl So I feel its a very anti Microsoft move.  Government guidelines state that we should be able to work from home if we want to, to help reduce congestion and enable more time to be spent with family, however this move ensures that Microsoft SQLServer developers can no longer work from home.  2.  http://www.ntlworld.com/tunnel.php?task=portBlocking   Ports 1433 (TCP), 1434 (UDP) Blocking these ports is likely to prevent the use of MS SQL Server and on occasion disrupt normal connections. This disruption is because these ports are above 1024. Ports above 1024 are used occasionally for 'temporary' communications e.g. the fetching of web pages. This is not a common practice, but it is worth noting that blocking these 2 ports may cause the (very) occasional "page not found" error due to port blocking. In these cases, if a customer tries to load the page again, or visit another site, the connection will work as normal. 
     
    
</post>

<date>16,June,2004</date>
<post>

     
      Since last two mails I was wondering as to what is this RSS Stuff going:- Very confusing word ???  I was wondering how cum our technical group went into political doldrums... No wonder as they say lesser mortals think a lot...  So here we break the jinx.I have collected some info and links to  some sites that will make you understand the whole crux of the RSS technology.     What is RSS?     RSS (Rich Site Summary or Really Simple Syndication ) is a format for delivering summaries of regularly changing web content. Many news-related sites, weblogs and other online publishers syndicate their content as an RSS Feed to whoever wants it. usually it located  by a simple XML  sign board .  If you frequent Weblogs, you've seen the little XML icons inviting you to "syndicate this site", but what does that really mean?   A long time ago, newspaper managers realized that if they could use articles and stories from other newspapers in their paper, they could garner more readers because they could cover a wider area than they could with just their own reporters.  This is an example of how syndication can work in print.  Online, there are potentially millions of authors writing about millions of topics each day. It can be very difficult to keep track of without some type of automated system.  And that's where RSS comes in. Really Simple Syndication (RSS) is an easy way for Web sites to share headlines and stories from other sites. Web surfers can use sophisticated news readers to surf these headlines using RSS aggregators.     Why RSS? Benefits and Reasons for using RSS ?      RSS solves a problem for people who regularly use the web. It allows you to easily stay informed by retrieving summaries of the latest content from the sites you are interested in. You save time by not needing to visit each site individually. You ensure your privacy, by not needing to join each site's email newsletter.  The number of sites offering  urlLink RSS feeds  is growing rapidly and includes big names like  Yahoo News  and  urlLink Amazon.com .  What do I need to do to read an RSS Feed? RSS Feed Readers and News Aggregators Feed Reader or News Aggregator software allow you to grab the RSS feeds from various sites and display them for you to read and use.if you have DOT NET Framework 1.1 installed, you can download  urlLink www.sharpreader.com  and use it.   A variety of  urlLink RSS Readers  are available for different platforms. Some popular feed readers include  urlLink Amphetadesk  (Windows, Linux, Mac),  urlLink FeedReader  (Windows), and  urlLink NewsGator  (Windows - integrates with Outlook).  Once you have your Feed Reader, it is a matter of finding sites that  urlLink syndicate content  and adding their RSS feed to the list of feeds your Feed Reader checks. Many sites display a small icon with the acronyms RSS, XML, or RDF to let you know a feed is available.   Here is where you can configure RSS support for   urlLink SQLCON    Thanks a lot Sudhakar and Ansari...for brining in some info 
     
    
</post>

<date>22,June,2004</date>
<post>

     
        Resources Covered: -   	  ·	Introduction ·       Review of Features ·	Enhancements ·	DTS Redefined ·	Deployment and Management ·	Hand Shake of Brothers (.NET and YUKON)  or Marriage of Romeo and Juliet.    Author: - Veer Ji Wangoo  Applies to : - SQL SERVER “YUKON” Beta 1  Target Audience :-  SQL DBA,Solution Architects,IT analysts ,Developers etc    Introduction   The world of Programming had never moved so fast as it did in last couple of years. After Y2K the things got so fast that it became increasingly difficult for lesser mortals to keep the pace with changing technologies (particularly if one got stuck on some Legacy project).  But As I saw the only one way to update was to keep on reading the white papers  ( I wish I could   start a daily newspaper for all IT professionals like Washington post).  So alternative is the white paper like this.  Few Months back I heard about the YUKON and sooner I brought it under my Microscopic thought. So here it an attempt for a successful failure or failed success. However I would reiterate the fact that this paper is for educational and awareness purpose only.   The new version of Microsoft SQL server, named “YUKON” is a next generation robust, scalable, Reliable database for windows.  We can write a new bible on its marketing propositions and value editions, but we will concentrate more on technical stuff.   Review of Features   There is lot of activity going between the middle tier ADO.NET and the backend SQL 2000.With DOT NET CLR, it became more important for the DBAs to know what lies above their head and same for the Architects/Analysts/programmers etc to know what lies below their feet. As all of us know net Dot net environment is simply outstanding. But for this beautiful Building to stand the cross current of competitiveness it required a strong foundation (better than SQL2000), So here is what YUKON gave it.  The infrastructure of YUKON is divided into following components and Constituents ,all working in tandem.  Some of then are being listed as    ·	SQL SERVICE BROKER ·	REPORTIG SERVICE ·	NOTIFICATION SERVICE ·	SQL SERVER MOBILE EDITION ·	WORK BENTCH ·	DTS-WIZARD   Lets take a small trip around this YUKON park one by one. We will compare it with some other features currently available as well.   SQL Service Broker:-   For all those who have worked on MSMQ /PMQ and all other asychrnous mechanisms to make the applications more fault tolerant and ended up writing huge lines of  code here is a good new for all of them.  Service broker gives a more scalable and fault tolerant architecture of message routing. It allows internal and external processes to send and receive messages using normal T-SQL.   Reporting Service: -    It is a complete server based platform for creating and managing reports from the data. It has lots of new API s that can be used to by developers .data providers to integrate reporting with the legacy systems as well as some third party tools. Major leap forward is the expansion of Microsoft Business Intelligence vision.   It is shipped with the Yukon along with the tools for creating, managing and viewing reports. It also has separate engine to host and process reports. During the set up it self it will ask for the reporting server and  the virtual sites  for the IIS to be created for reports. It is one of the best features that one can utilize .   Notification Service :-   For operation and administration  of  SQL we have been using  notifications through alerts and operators. Sending E-mail / pagers /Netsends have been in SQL since long. But whats new is that it is a service of its own now. Added to that it can be subscribed for particular trigged action and reaction on data modification, Job out comes etc.we can send the notification to various devices as Mobiles/PDAs/Messenger services and off course Emails.   SQL Server mobile Edition: -   All of us are aware of SQL CE which shipped along with SQL 2000 .It is now renamed as SQL Server Mobile Edition and can be developed and managed using SQL Server “WORK BENCH’. We can create the Mobile Edition database on the desktop or on the devices directly from the SQL workbench.  As such any database manipulations become independent of the device/site where the Database is residing, this includes the subscription and publication of database, making queries across to-and –fro to the devices. DTS into the mobile edition from non-SQL databases using Whidbey Applications has been made feasible.   SQL Server Workbench: -  			  One of the greatest and revolutionary features of this system is WORKBENCH. It is actually a DBA suite but has been made fine for developers as well. There are two types of workbench BI-workbench and SQL server –workbench. Both are capable of doing similar tasks like developing projects and solutions. Point to clear here is that solution can contains more than one projects, while projects contain data sources, data source views,dts packages and other  miscellaneous things like dml/ddl files.  The BI workbench doesn’t require direct connection to the SQL server RDBMS to design packages etc nor any connection to save your work.it works just like VS.NET project studio. one can implement VSS on to it and check for code modifications  as well  SQL Server workbench unlike BI- workbench is aimed primarily at DBAs to manage SQL.Analysis and reporting servers. It also supports designing, executing and scheduling DTS packages, but does not include source control for packages saved on sql server (rather than file system)   To chose between two is entirely your choice. However key is if you want to execute the package etc directly put it on sql server workbench otherwise to develop, test and do cross platform development before the deployment into production then your choice is definitely BI-workbench.   It supports back up to SQL 7.0 and can be effectively used to connect and work with SQL Servers, Analysis services and Mobile Editions as well.  It is focused primarily on development, deployment, and Management. there are lots of constituents that are .We will just name and define some here.  1.	Query Editor: - Replaces Query Analyser. 2.	Xquery Designer: - Introduces support to write queries for xml. 3.	Windows installer:  - used by Yukon for installation but no more typical and minimal modes are there. But simple customization options are available.  4.	Consistence Checker for set up - Set up configuration checker validates the target machine for deployment. In case of any issues it guides the installer for necessary action before deployment. 5.	Failure reporting for set up: - creates log and directs user for corrective action.  6.	Computer Manger: - Replaces Client network utility/Server network utility and SQL service manager; It is accessible through WMI (Windows management instrumentation). It helps to work with lots of things like Analysis Services, Previous versions of SQL Servers, Reporting services, MS cum Full text search etc 7.	Object Explorer: - It is a combination of Enterprise manager and Analysis service                          manager. 8.	Profiler and Tuning Advisor :- Much like profiler and index tuning wizards but lots of features like profiling Analysis services, saving traces/show plans as XML etc 9.	SQL Server agent :- Much like earlier one but with few changes like creation of agent user roles ,creation of proxy roles and assignment of users/groups to these Proxy groups for monitoring/creating/editing of Jobs.   Enhancements   The  enhancements in YUKON can be divided into two categories namely the Database Engine enhancements and Reporting service enhancements.However we will not go along these lines reason being there are other generic enhancements in yukon as well.  Some of the genric problems that we used to face like Server Reboot incase of change in SQL system parameters.the Database affinity for CPU and RAM memory may also no \w be altered without requiring the restatrt.  so we follow a conventional line to enhancements .  When we talk of enhancements the following categories have been mainly enhanced :- ·	.net framework programing ·	batches ·	Data types  Lets take them one by one  1  .Net Framework   :- we will take it up in another chapter But briefly describing it here.Just like any other Object we can now define and create a database object inside an instance of  sql server that can be programmed in the microsoft .NET Framework common language runtime.As such all the properties of this object like Stored Procedures ,model functions,triggers,user defined types and aggregates can be done in rich  CLR languages.We can create,alter and drop assemblies  2.  Batches  :- YUKON introduces a new feature called multiple active result sets (MARS).it allows client drivers to have more than one pending request per connection.It can interact with ODBC/OLEDB and batch execution environment   3.  DataTypes  :- There are  new data types as well as enhancements on several existing datatypes.This is very important fom the common usage point of view.I will take some leverage to explain them here.        XML datatype :- helps you store and interact  with the xml document or fragment.This data type can be in Instances of this data type can be ued in tables,functions,Sps,etc.We can put constraints on it by refercing the validating schema .As such we can specify Xquery against the xml data stored and apply inserts/updates/deletes to the data.        UTCDatetime  :-is  a datetime datatatype which has intellisence of time zone.As such for global operations of Ecom sites it becomes handy       Date:-  datatype has been modified to the precision of 100 NS       Varchar(max)-Nvarchar(max)-varbinary(max)  can hold data upto 2GB.so be free of 8000 limit of  varchar.  4.  Failover Clustering  :-  Yukon provides a high  availibility  support for server scope  failures.With FailOver clustering the OS ad SQL server work in tandom to protect from failure by  providing reduntant hardware and an automated mechanism to ove the database server to secondary hardware in the event the primary failes.Upto eight nodes we can map in ,depending on the OS version.  Yukon has brought Analysis,notification,replication environment into fail over clustering as well.  5. Database Mirroring  :-  The  basic work it does is shiping the trnsaction log to another server ie mirroring the work don on primary.As such durng planned downtime DBA s can save al the downtime incurred in the past.BY usage with service broker  applications can  connect with secondary server within no time.Ulike failover clustering ,the mirrored server is  fully cached and ready to accept workloads because of synchronised state. One of the most imporant feature sof this mirroring is two way udation,which means synchronising the flow in both directions.It requires no standard server  controls or  SCSI disk of  array s etc..   6. Database View :-  Database view provides a read only ,stable view of a database that can be created with out any overhead,in case of any divergence / accidental changes View can be used to reapply the pages.  7. Replication  :-    Although muuch of SQL 2000 replication methodologies seems to be excellent.but adding to what we have already new features like enabling replication through http:// and https:// increases the availability of datat for mobile scenerios which enables synchronisation over the internet.New features like “Peer-to-Peer”  model have been introduced to    to make applications more scalable for SQL read workload.         8. Online Index Operations  :-  Gone are the days when any online reindexing wuld cause havocs with exclusive locks and bring the application to stand still.we can continue to make updates and perform queries against the data even during the rebuilding of clustered index. Additionally the concept of parallel processing allows server to take the workload of online indexing 9. Online Restore  :- Sql Server introduces the ability to perform a restore operations without getting the database offline.Only the data to be restored becomes offline not the whole database.However  depending on the criticality of the suituation we can chose between the offline and online restore. 10. Fast Recovery  :- Users can reconnect to a recovery database after the transaction log has been rolled forward.Earlier versions of  SQL Server required users to wait until the incomplete transactiosn have been rolled  back,even if the y didn’t need to access  those  parts of  data .But key is that if  you are access the  shady data of  uncommited transaction then normal blocking can be expected.  11.	 Mirrored backup :-  In Yukjon backup media can now be mirrored ,upto four sets .which can be used in case of backup failing.  12.	 DBCC CheckSum  :- Dbcc  has been modified a lot to take into consideration the new feaures of yukon.But the exemplary to metion here is the dbcc checksum which verifies data at the page level by doing a checksum of  each page,report the partitioning correctness and error free of registered assemblies etc.  13.	 ShowPlan and Deadlock Enhanc ements:- Now  we can have graphical represntationm of  deadlock occurances collected through trace events.The graphical shows dead lock cycle of  chains,providing you means to analyse dead occurances.These results can be saved in XML formatAs such we can ship and view the plan without the underlying database.  14.	 SQLCMD  :- A new command line utility replacing OSQL/ISQL.It is coming out  with  rich set of  commaands, yet to be reviewed for writing in this paper.(So lets watch out on this)  15.	 Dedicated Administrator Connnection :- to access a running server even if the server is in no mood to talk to anybody (Hung or otherwise unavalable-like my Girl friend) ,It is hot wire connection and activated by varous  memebers of the sysadmin role and is only available through the SQLCMD commmand  locally as well as on remote  machine.thus DBAs have a special access to server for diagnosis and treatment of SQL issues.  16.	 SQL management Objects (SMO):  -   To enhacne  and extend the functioanlity of SQL server database and replication management through programing ,we have SMO object model in .NET environment.it replaces and extends the current DMOs.(they will stillbe there but without new features.).SMO is implemented as .net assembly by using WMI xobjects as before.So now all CLR features can be used to create applications that can help in the Online maintenance and adminsitration of  SQl server.  17.	 Transact SQL  :-  previously all T-SQL was as per  ANSI 92  stadards now YUKON implements  ANSI 99 standards so we must expect lots of  changes as new releases of  yukon come along.many of the improvenments are based on the assumption that we need to be more  expressive in querries and  as  such Microsft has taken lot of  feed back from customer using T-SQL.there are lots of updates on this section beyong the scope of this article.For mention only  Common table Exoression,recursibe queries,pivot and un pivot operators,realtional operators,outer apply ,error handling capability through try catch  constructs..  18.	 Security  :- a beautiful paradigm for security, both for developers and admistrators.However  more  details are likely to come in beta 2 .Currently lot of  investment is being made in  number of  features covering a broad spectrum like enforcing policies for sql login passwords(authentication scope) with  finer granularity in permissions (authorization scope ),to seperation of  owners and schemas (in security management space)    DTS Redefined    DTS has been raw defined in the since that it is totally new and matured for enterprise development and deployment. The overall architecture of the product has changed a lot. The designers have built several new wizards into workbench environment to help the DTS.   DTS Import/Export wizard   Much like as we have in sql 2000 but with some new features like status monitoring, control flow check, saving the result file as  .dtsx  or text file ,mailing it  etc and new GUI as well. However for the DTS packages created in 2k and below and you want them to run in YUKON. Then we are sorry to say you w\cant cum toll free. You will have to use migration wizard for it become YUKON complaint and run tax free…(everything comes at a cost)   DTS Package installer wizard   Now this wizard is something new. Here you can use BI-workbench to create a new deployment project where in you can set the deployment out put path where in your deployment files will be kept. You can have DTS packages, the DTSInstall.exe, the DTSdeploymentutility.Msi as well manifest file. Simple clicking of exe will call wizard and you are ready to deploy.   DTS Configuration wizard   As the word of this topic sound CONFIGURATION wizard guides you through the set up for creating configuration but what exactly is configuration? Well it is same like Dynamic Properties tasks in 2000. to configure the package variables and properties at run time. Only difference is that format is XML, they are external to package and values get set before execution rather than during run time. In any package assigned to a wizard, there can be more than one configuration however configuration lower in the list will get priority-i.e. The largest  one.    Deployment and Management    If you ask any body how to deploy any Windows Application, Answer will be “click next”. That is true but that’s where most of us make the first mistake.  Although MS gives us easy deployment wizards but still there are lots of place where we need to think,analyse ,estimate and make decision. It is the foundation for getting  good performance out of any application.  To get the best out of YUKON we must set configuration options properly, basic parameters and locations of files provides greater flexibility for application security and resource management.  We have Windows Installer like SQL2000 but minimum and typical modes are no longer implemented. Instead we have a feature tree with default options enabled and administrators can then customize by selection and clearance of items involved on feature tree. Remote set up and Multiple installations are also available in this wizard.  Coolest feature is the Setup consistency checker which checks the target computer for all necessary infrastructure before setup begins. In case of any discrepancy, It will give necessary instructions and actions to be taken. It uses WMI technology to do the same.  Even if the setup fails the error codes are well trapped ,with full description of error along with recommended corrective action    Hand Shake of Brothers   As I mentioned in the review about the tighter integration of CLR and SQL in Yukon. Some of it got its mention in enhancements section also. I am putting a separate article on this issue as I this is a vast area to summarize here.    
     
    
</post>

<date>24,June,2004</date>
<post>

     
      SQL Server's default TCP/IP port is 1433 and uses the UDP port of 1434. You cannot change the UDP port from 1434 to another, which is what helped the SQL Slammer virus spread so quickly. The UDP port is constantly listening on port 1434 and when ping responds with the TCP/IP port SQL Server listens on.  Default instances of SQL Server use TCP port 1433 by default. 1433 is the official Internet Assigned Number Authority (IANA) socket number for SQL Server. Named instances, however, dynamically assign an unused TCP port number the first time the instance is started. The named instance can also dynamically change its TCP port address on a subsequent startup if the original TCP port number is being used by another application. SQL Server only dynamically changes to an unused TCP port if the port it is currently listening on was dynamically selected. That is, if the port was manually selected, SQL Server will display an error and continue to listen on other ports.  The error messages are held in:  %systemroot%\program files\microsoft sql server\mssql\log  Any port errors are recorded in this file, which can be opened with notepad.  However, It is important to point out that dynamic port detection is ONLY available for named instances of SQL. The network libraries assume either 1433 or the global default port established with the Client Configuration Utility. If a default instance is listening on a port other than the standard 1433 port, you must provide an alias or alter the global default port. To change or allocate ports manually, you can either run server network utility, or edit the following Registry key:  HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Microsoft SQLServer\instance_name\MSSQLServer\SuperSocketNetLib\Tcp BUT you need to make sure that the now port is not in use by any other process. (Type  netstat -a -n  at the command prompt to get a list of the port numbers in use.) The implications of changing the default ports are that the clients may not be able to see the server. SQL clients use DBNETLIB to detect the ports. DBNETLIB is always loaded by the ODBC or SQLOLEDB components. DBNETLIB is responsible for making either direct IP/SPX calls or forwarding requests directly to the Shared Memory, Named Pipes or other network libraries. The Client Configuration Utility has been extended in SQL 2000 to provide an option for dynamic port detection. When you enable the Client Configuration Utility, no port number is stored for the alias entry and DBNETLIB attempts to contact the server through a known UDP port to obtain the proper connection information.  
     
    
</post>

<date>06,July,2004</date>
<post>

     
      Just last week this buzz word hit my head at South Ex Cofe Shop while talking to Sourabh..then all of a sudden things where getting carzy as Sudhakar mentioned in his comments on pooja BLog ("Your blog gonna be a one stop shop for Monad ")..Thats what Happened to me as well..  It was my final stop there..So To lok at originals as Pooja has put a Copyright sign on her Paper on Monad so I am redirecting you there  Click Here   Poojas MONAD Session 
     
    
</post>

<date>13,July,2004</date>
<post>

     
      /* Written By :-  Veer ji Wangoo (India) http://vsql.blogspot.com Inputs from:-  Dinesh Asanka(Sri Lanka)  SqlServer Central  		http://www.geocities.com/dineshasanka/index.htm Dated :- 12 July 2004  A very simple and unique way to encrypt Password in your database application  using a function called pwdencrypt   Lets have look at it,we first create a table named [UserTab]  which will have two columns along with a UserTab .Note the data Type varbinary(255) to include Hashed encrypted value in the Table */  CREATE TABLE [dbo].[UserTab] ( [ID] [int] IDENTITY (1, 1)  NOT NULL , [UserName] [varchar] (50) COLLATE  SQL_Latin1_General_CP1_CI_AS NULL , [Password] [varbinary] (255) NULL , ) ON [PRIMARY] GO   /* Next insert the data with a usage pwdencrypt() --This is an Undocumented Function. */ INSERT INTO [UserTab](UserName,Password)  VALUES ( 'Vsql',pwdencrypt('Varsha'));  --When you select you will find the PWD in Encryted Form.  select * from [user]  --A Sp  can be  written with below logic and  to retreive an d check the Password authentication --We can ask the user to input teh USername and PWD into the Sp and check it with the matching Pwd --If compare value comes as true then we give them access else we reject the access to th application  DECLARE @varPassword 	varbinary(255) 	SELECT @varPassword = [Password] FROM [UserTab] where UserName = 'Vsql' DECLARE @chkPassword varchar(255) 	SELECT @chkPassword = 'Varsha' if  (pwdcompare(@chkPassword, @varPassword, 0) = 1)  Begin 	print 'Get along' End else 	print 'Permission Denied'  /* Although this doesnt garuntee the Safegaurd of your passwords and  any hacking attacks but will certainly help in maintaining the integrity of the  applications.Like freaking with Usernames/passwords of any user by the people who has  access to tables in your Application Database  You can also use various Encryption algorithms availble in registry  and Devlopment Environments.  */  
     
    
</post>

<date>30,July,2004</date>
<post>

     
      He should have been the Lockup Man in Tihar because he knows better ways to lock and unlock the system....   Here is a nice trick to lock your system as I found it in  urlLink Hashims Blogs ..Liked it alot...    If CTRL-ALT-DELETE seems like too much of a hassle, try this instead:    1. Right click an empty spot on the desktop, point to New and clickShortcut.    2. In the Create Shortcut dialog box, type the following into the Type thelocation of the item text box:" rundll32 user32.dll,LockWorkStation " // remove quotes while typing    3. Click Next.    4. In the Select a Title for the Program dialog box, type "Lock Desktop" inthe Type a name for this shortcut text box.    5. Click Finish.    
     
    
</post>


</Blog>