<Blog>

<date>29,June,2004</date>
<post>

	 
      Blogging is new thing to me.  Just thought I'd take it for a test dive.
     
    
</post>


<date>04,July,2004</date>
<post>


       
       urlLink http://www.raycomm.com/techwhirl/  TECHWR-L ("tech-whirl") is the Internet-based community and resource for technical communicators worldwide. The TECHWR-L discussion list has more than 5,000 direct subscribers and an estimated daily readership of more than 10,000 people worldwide. The TECHWR-L site serves more than two million page views per month and provides a unique combination of articles, features, and services unrivaled by any other Internet-based technical writing resource.    urlLink http://www.useit.com/  Jakob Nielsen's (need I say more?) website on "usable information technology" .   urlLink http://www.w3.org/  The World Wide Web consortium website. The World Wide Web Consortium (W3C) develops web specifications, guidelines, software, and tools and is a forum for information, commerce, communication, and collective understanding.   
     

    
</post>

<date>03,July,2004</date>
<post>


       
       Research and Writing    urlLink http://www.visualthesaurus.com/index.jsp  Visual Thesarus is a way cool interactive thesaurus. You type in a term, click, and your term is instantly surrounded on-screen by a web of synonyms and antonyms. When you click on one of those surrounding words, it becomes the center of the page and a whole new set of related words pops up. It's very intuitive, and a lot more fun than a book thesaurus!     urlLink http://www.landmark-project.com/citation_machine/index.php  This is a solid (but not perfect) citation maker. Just select the type of source document, fill in the template, and Citation Machine creates an MLA and APA citation.  http:// urlLink www.libraryspot.com/  Library Spot is a great starting point for research. Here's their tag-line: Simplifying the Search for the Best Library and Reference Resources on the Web. Tons of links to dictionaries, thesauri, reference libraries, etc.    Medical stuff   urlLink http://www.ncbi.nlm.nih.gov/entrez/query.fcgi  If you need to search for real medical research, PubMed is the place to go. Part of the National Library of Medicine, PubMed has millions of citations for biomedical articles from MEDLINE and additional life science journals. It's easy to search, and includes links to many sites providing full text articles and other related resources.   
     

    
</post>

<date>02,July,2004</date>
<post>


       
        The I Need to Laugh  group o' links  You know how some days you just need a break?  Well, here are some of my faves.   urlLink www.davebarry.com  Need I say more?     urlLink http://www.schickele.com/  Ever heard of PDQ Bach?  If you are into classical music and like to laugh a little, check this out.  Professor Peter Schickele is the brain behind the whole concept of PDQ.     urlLink http://www.homestarrunner.com  This is totally inane--but it is about the most hilarious thing on a bad day or when you are extremely sleep deprived.     The  I Need Computer Help  group o' links  I'm a novice at the whole computer maintenance thing, but these sites offer a little guidance.   urlLink http://www.Helpwithpcs.com    urlLink http://www.pchell.com      The  I Need to Learn More about Technical Communication  group o' links  Realistically, this isn't one group that most people would be interested in, but I think it's the bomb!   Society for Technical Communication   urlLink http://Stc.org    Society for Technical Communication Technical Editing Special Interest Group  urlLink http://www.stcsig.org/te/  (Because my favorite sub-category of technical communication is technical editing)         Technical communication  journal   urlLink www.techcomm-online.org    EServer TC Library: A Cooperative Library for Tech Communicators   urlLink tc.eserver.org  Another great resource with lots of technical editing links   The "no formal title" link   urlLink http://www.december.com/info/techcomm  I just found this one, but it has some good stuff on it!
     

    
</post>

<date>02,July,2004</date>
<post>


       
       Here's a list of links of my favorite Tech Writing, Research, and Diversion sites.      Technical Writing    urlLink Council for Programs in Technical and Scientific Communication  I have to admit I don't remember ever using this site, but it was on my favorites list and it looks terribly important.    urlLink The Bay Area's Editors' Forum  I have used this one. The forum offers a number of useful resources for editors of the technical or non-technical variety.   urlLink eServer TC Library  Perhaps this site would be better listed under the research header, but I'm running low on tech writing sites and the TC Library is an immensely useful site, so use it and be happy.   Research   urlLink Bartleby  This site is so cool that I'm afraid to let the word out.  Unlimited access to tons (I weighed them) of reference, verse, fiction, and nonfiction texts.   urlLink Baywood  Fee-based service offers articles from journals you can't get anywhere else.   urlLink Voice of Shuttle  As deep as it is wide, VOS offers a dive into the murky depths of humanities research.     Diversions   urlLink Bloodshoot records  Treat yourself to the latest from the Insurgent Country scene.  Be the first on your block to discover Trailer Bride, Neko Case, and The Waco Brothers.    urlLink Mundane Behavior  Don't just live a commonplace life, read scholarly studies about it.   urlLink The Unemployed Philosophers Guild  Fuel up for your next discussion on postmodern angst with a Nietzche "Will to Power Bar" or convince your analyst that you really are ready to start healing by giving him/her an Anna Freud doll.  
     

    
</post>

<date>02,July,2004</date>
<post>


       
       XSL (eXtensible Stylesheet Language)   XSL is a new and exciting league of professional American football sweeping the nation.  Take a American football and add a twist of Aussie rules, throw in liberal amounts of lethal explosives, a healthy dose of heavy metal music, scantily clothed women, and lots of alcohol and you have . . . okay, so that’s the WWF.  XSL has something to do with websites.        Definition provided by Shannon Brown.   What is it?   Extensible Stylesheet Language, or XSL, is a special kind of Extensible Markup Language, or XML. As its name suggests, XSL was drafted initially to give developers specific language tools to augment XML functionality. Neil Bradley, in The XSL Companion, gives the following concise history of XSL to the year 2000:  “[XSL] is a family of standards developed by W3C (the World Wide Web Consortium). These standards emerged out of a proposal for a stylesheet language, submitted in 1997, which was to be called 'XSL' (eXtensible Stylesheet Language). However, during its gestation, this proposal was eventually pulled apart into three separate standards. The first of these, XPath, defines a mechanism for locating information in XML documents, and it has many other uses beyond its role in formatting documents. The second, XSLT, provides a means for transforming XML documents into other data formats, including (but not limited to) formatting languages. Finally, the term ' XSL ' is now properly used only to name a proposed standard for embedding formatting information in documents using XML elements.” (Bradley 2000, iii)  In other words, almost at its inception XSL split into three standards, each with different purposes: Xpath, XSLT, and a third standard that Bradley sees as the proper heir to the larger name “XSL.” Nonetheless, W3C, the body responsible for overseeing the development of XSL, still treats the XSL family as a single entity in its official definition, with three distinct parts:  “XSL is a family of recommendations for defining XML document transformation and presentation. It consists of three parts:  XSL Transformations (XSLT), a language for transforming XML, the XML Path Language (XPath), an expression language used by XSLT to access or refer to parts of an XML document. (XPath is also used by the XML Linking specification), [and] XSL Formatting Objects (XSL-FO) an XML vocabulary for specifying formatting semantics.”(World Wide Web Consortium 2004, w3.org)  Pairing off the terms from Bradley’s discussion of XSL to the terms in the W3C definition, we’re left to conclude that the third standard Bradley mentions simply as “XSL” is actually “XSL Formatting Objects.” We should note that there is actually some ambiguity to current usage of the term ‘XSL’—some authors use it strictly to refer only to the entire family, while others appear to use it interchangably with XSL-FO. Nonetheless, given the differences between the three parts of the XSL family, we should take care to keep them straight. So, how do these different parts of XSL work with XML?   Transformation and Presentation   The W3C definition’s reference to “transformation and presentation” reveals key characteristics of two of the sub-lanaguages in XSL. Elliotte Rusty Harold describes the transformative power of XSL in The XML Bible:  “The Extensible Stylesheet Language (XSL) includes both a transformation language and a formatting language . . . The transformation language provides elements that define rules for how one XML document is transformed into another XML document. The transformed XML document may use the markup and DTD [document-type definition] of the original document, or it may use a completely different set of elements. In particular, it may use the elements defined by the second part of XSL, the formatting objects.”(Harold 2001, 481)  Hence, in addition to specifying the formatting of the resulting XML content, XSL can also provide a map for turning one XML document into a second, different XML document. XSLT is the language for performing this kind of change. But why would anyone want to do this?  XSLT: An example  Miloslav Nic gives several applications in his XSLT tutorial, available online at zvon.org, which I used as the basis for the attached examples.   What’s left in XSL?   G. Ken Holman, in the article What is XLST? he wrote for xml.com, describes the difference between XLST and the rest of XSL:  “[There are] two distinct styling steps: transforming the instance of the XML vocabulary into a new instance according to a vocabulary of rendering semantics; and formatting the instance of the rendering vocabulary in the user agent . . . In order to meet these two distinct processes in a detached (yet related) fashion, the W3C Working Group responsible for the Extensible Stylesheet Language (XSL) split the original drafts of their work into two separate Recommendations: one for transforming information and the other for rendering information.” (Holman 2000, xml.com)  We’ve already discussed the confusion in some of the literature over the entire XSL family and its constituent parts, especially XSL Formatting Objects. Given this confusion, we should try to decipher Holman’s reference to “XSL” as one of the three parts named in the W3C definition, most likely XSL-FO. In The XML Bible (again), Harold describes XSL-FO as sharing some transformative characteristics with CSS. XSL-FO, however, is much more powerful and dynamic.  “While CSS (Cascading Style Sheets) is primarily intended for use on the Web, XSL-FO is designed for broader use. You should, for instance, be able to write an XSL style sheet that uses formatting objects to lay out an entire printed book. A different style sheet should be able to transform the same XML document into a Web site.” (Harold 2001, 571)  XSL-FO, then, is portable to many different formats, which makes it ideal for content managers working in many different media. Of course, this flexibility means that XSL-FO must use its own language for attribute and element commands. Miroslav Nic offers a searchable reference of XSL-FO commands, with plenty of examples, at zvon.org.   And, finally: XPath   Elliotte Rusty Harold and W. Scott Means, in XML in a Nutshell, explain XPath in terms of what it does for XSLT:  “XPath is a non-XML language for identifying particular parts of XML documents. XPath lets you write expressions that refer to the first person element in a document, the seventh child element of the third person element, the ID attribute of the first person element whose contents are the string “Fred Jones”, all xml-stylesheet processing instructions in the document’s prolog, and so forth. XPath indicates nodes by position, relative position, type, content, and several other criteria. XSLT uses XPath expressions to match and select particular elements in the input document for copying into the output document or further processing . . . String manipulation in XPath lets XSLT perform tasks such as making the title of a chapter uppercase in a headline or extracting the last two digits from a year.” (Harold and Means 2002, 154)  So, XPath is the tool used by XSLT for finding and deploying parts of one XML document in another XML document. We saw it at work in the XSLT example without pointing it out—XPath is the standard from the XSL family responsible for pulling the “title” and “author” elements from the XML source.   Sources   Bradley, Neil. The XML Companion. London: Addison Wesley, 2000. Portions available online at O'Reilly.com  Nic, Miroslav. XSLT Tutorial,1999.  –––––. XSL FO reference, 2000.  Harold, Elliotte Rusty. The XML Bible, 2nd edition. New York: Hungry Minds, Inc., 2001. Portions available online at ibiblio.org  Harold, Elliotte Rusty and W. Scott Means. XML in a Nutshell, 2nd edition. Sebastopol, CA: O’Reilly, 2002. Portions available online at O'Reilly.  Holman, G. Ken. “What is XSLT?,” O’Reilly xml.com , 2000.  World Wide Web Consortium. The Extensible Stylesheet Language Family (XSL). w3.com, 2004.  
     

    
</post>

<date>02,July,2004</date>
<post>


       
       WORKFLOW   I’ll give you three guesses on this one.  How about “Workflow is the flow of work”?  Fork please, my job here is done.     Definition provided by Gary Hernandez     Workflow   Last week I read an article from ASTD’s magazine, T&D, which addressed workflow in terms of learning while working.  The article spoke to the efficiencies of employees learning while engrossed in on-the-job activities.  The efficiencies are realized when employees do not have to remove themselves from work to attend learning events about work.  Hence productivity improves and, perhaps, the knowledge is further embedded through kinesthetic means.  When I saw workflow in the list of terms for us to define, I jumped right on it thinking I was at least on article ahead of the game.  Alas, much like granularity, semantics, and life cycle, workflow is yet another term that has a differential meaning in the context of content management than it does in other fields.      Rockley on workflow   “Workflow, as its name implies, is the way tasks flow through a cycle on their way to getting a job done” (Rockley 228).  According to Rockley, workflow in its simplest form articulates the path from the first task of a work plan to each successive milestone and task through to completion.  Workflow could be illustrated in a traditional flowchart or swimlane diagram.  One would presume a PERT chart and Gantt chart could be used as well. The importance is that players, tasks, and process are clearly delineated.   The players are people and roles that complete each task, the tasks are discreet units of work, and processes indicate the flow and interdependencies of the tasks.  Workflow can also be automated.     “Workflow 101,” Jim Miniham  Jim Miniham states that workflow is not a definitive term.  Miniham points out that the term originated in the manufacturing industry where it had organizational implications.  In the manufacturing world workflow referred to the flow of a product through its entire production life span: how long the product spent in each stage of production, how many items moved through a given work location, and even where specific products were at any given time.  In the software industry, workflow came to mean the “routing and distribution” of documents as well as everything that happened to it along the way, including who touched it, how long it spend in what location, and its status at any given time.  With such a wide span of functionality, the word workflow began to be applied to any number of applications that approximated any stage production.  In addition, workflow began to be applied with production-class tools as well.  In this realm workflow expanded its influence from tracking products to being critical in managing the production process.     “The Wild, Wild World of Workflow,” Molly W. Joss  Molly Joss, similar to Jim Miniham, indicates that workflow is terminally ubiquitous and can stand to mean any number of things depending on the context.  Postmoderm implications aside, workflow principally means the configuration of work processes.  From this standpoint, all organizations engaged in work are in engaged in workflow.  How one manages workflow is perhaps what people are really talking about when they mention the term.  To help in that management process, of course, is quite a plethora of computer applications.  Joss cites three types of workflow tools available in the graphic arts industry:  ·         Task-oriented workflow tools which “automate or simplify a specific prepress task” (62).  ·         RIP-centric workflow systems for a full system that address the entire workflow paradigm  ·         Workflow analysis tools to help “capture and calculate data on various prepress processes” (67).    Joss concludes that the ability of workflow applications to improve productivity has given rise to a world of workflow vendors, giving would be organizations wishing to optimize workflow through automation the blessing of choice and curse of choice.    “Why Workflow Works,” Robert B. Segal  To the relatively flat discussion of workflow, Robert Segal adds a refreshing twist.  The benefit of automating workflow is, yes, improved productivity through efficient use of resources, but the beneficiary of that productivity is the customer.  Segal notes that “Workflow automation systems are designed to ensure that information is processed efficiently, all activities are monitored and measured and expectations are reported” (94).  The key for Segal in workflow is the monitoring and reporting functionality that gives the line manager the ability to respond to and correct workflow deficiencies that often directly effect the customers.   With this in mind, the marriage of workflow management with customer insight data produces Customer Relationship Management (CRM).  Segal sees workflow management as the foundation to CRM.           “Workflow,” Tom Singer  Tom Singer quotes Workflow Management Coalition as “the automation of a business process, in whole or part, during which documents, information, or tasks are passed from one participant to another for action, according to a set of procedural rules” (34).  Singer notes that workflow optimization is not only about managing productivity and maximizing efficiencies, it can also help manage maintenance issues.  The point is that maintenance is also driven by workflow processes and hence can reap doubly—1) by improving efficiencies and 2) as maintenance becomes more efficient, the benefits of maintenance can be brought to bear more quickly on the organization.      Conclusion  Workflow principally focuses on maximizing efficiency in the processes embedded in a work plan.  Though originating in manufacturing, it can be applied to any environment which has work processes but perhaps reap more benefit in an automated work stream.  The benefits of workflow management can be felt in productivity, customer service, and even in some collateral areas such as maintenance and training.      WORKS CITED   Adkins, Sam S.  “The Brave New World of Learning,” T&D.  June 2003.    Joss, Molly W. “The Wild, Wild World of Workflow,” www.electonic-publishing.com.    September 1999.   Miniham, Jim.  “Workflow 101,” www.edocmagazine.com. July/August 2003.    Segal, Robert B. “Why Workflow Works,” Mortgage Banking. June 1999.    Singer, Tom.  “Workflow,” www.plantengineering.com.  September 2002. 
     

    
</post>

<date>02,July,2004</date>
<post>


       
       WIKI  WIKI is not a form of wicker furniture found at Ikea, it has nothing to do with outdoors Hawaiian barbecues, and it is not that threatening dance the New Zealand Rugby team does at the beginning of their games.  To discover exactly what this term is, read below and be fascinated.    Definition provided by Shannon Brown.     What is wiki?   The idea of Wiki, as explained by the wiki.org site, is deceptively simple:  “Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser.” 1  Of course, “wiki” has come to refer to much more than this server software—it has become identified with the activity this server software makes possible.  Ward Cunningham created and christened the first “wiki” in 1995, naming it “wiki-wiki,” which means “super fast” in Hawaiian, in order to avoid the blander “quick-web.” 2 The wiki was created so that anyone, regardless of his or her previous experience with computer languages, could contribute to the web publishing community. More importantly, though, the wiki software and wikis themselves were created to be used by everyone, all together, as described in the entry for wiki in the Wikipedia:  “AWikiWikiWeb enables documents to be authored collectively in a simple markup language using a web browser . Because most wikis are web-based, the term 'wiki' is usually sufficient. A single page in a wiki is referred to as a 'wiki page,' while the entire body of pages, which are usually highly interconnected, is called 'the wiki.'” 3  So, a wiki page, in theory at least, has no single author. Anyone can contribute, everyone on equal footing. The word usage distinction between single pages (wiki pages) and an entire group of pages (the wiki) is significant: the software is designed to support an entire body of interconnected pages, of which individual pages are only parts.   What makes wiki different?   Probably the most unique aspect of wiki, however, is its emphasis on collective authorship. Although several web-publishing methods offer interconnected communities of easy-to-author pages of content, only the wiki opens up the authoring privilege for every page to every visitor. The ISP Webopedia notes wiki’s crucial difference from blogs in this respect:  “Similar to a blog in structure and logic, a wiki allows anyone to edit, delete or modify content that has been placed on the Web site using a browser interface, including the work of previous authors. In contrast, a blog, typically authored by an individual, does not allow visitors to change the original posted material, only add comments to the original content.” 4  Likewise, although the wiki’s approach to community ownership might appear to make it a kind of message board, the epowiki site, in a page with text attributed to Randy Karamer, explains the differences between the two publishing methods:  “Compared to a plain vanilla message board, a Wiki's advantage is in DocumentMode. In a message board, somebody writes, somebody answers, somebody else answers, somebody asks a follow up question, ..... You end up with a whole lot of messages but not an integrated story. On a wiki, somebody can start a wiki page by asking a question. Someone can come along and edit that page to answer the question. Somebody else can come along and edit the answer to cover an additional point, or clarify a point, or whatever. If somebody edits the page by adding a follow-up question, it can be left on that page and then answered. Maybe it makes more sense to move it to its own page.” 5  Comparing a wiki to a physical message board reveals how radical the wiki’s approach to authorship really is. Imagine that each person who wants to can not only add new messages to the board, he or she can edit, reorganize, even merge and delete messages that are already there.  Of course, since anyone can edit, reorganize, merge, or delete wiki pages, a wiki is vulnerable to the whims of users with less-than-honorable intents. Vandals can lay waste to months of collaborative work in a matter of hours. Ward Cunningham, the creator of the first wiki, describes this as the wiki’s inherent “fragility.” 6 The wiki, however, can fight back, as The Humaine Portal (itself a wiki) explains:  “Vandalism is prevented through an uneditable document history -- if anyone vandalizes the current version of a document, it is easy to restore a previous version and ban that user.” 7  This unalterable change history, since it is normally accessible to any visitor, allows law-abiding contributors to a wiki to police their community. Violators can be permanently exiled for their transgressions.   Open, semi-open, private   Naturally, not all wikis are the same. While there are many large wikis that still hold to the original wiki precepts of collective, egalitarian authorship, other communities have leveraged the wiki idea without completely agreeing to the wiki ideals. The Humaine Portal explains a fundamental difference in wiki culture:   “At the core of the wiki principle is the idea that the obstacle for contributing to a document should be lowered. In open wikis, anyone can edit a web page. In semi-open wikis, as the HUMAINE know-how melting pot, users must register before they can contribute.” 8  The Wikipedia is perhaps the largest open wiki in existence, with nearly 300,000 articles/entries. 9 On the other end of the spectrum is a category the above quote from the The Humaine Portal wiki ignores: the private wiki, housed on private servers. Obviously, since all that’s needed to start a wiki is a server, a piece of database server software, and a network of users, the wiki technology holds promise for many different collaborative teams, not just the public at large. Several companies are already offering their wiki services to private groups or individuals, including Altassian and Project Forum among many, many others.   Sources   1 “What is Wiki?,” wiki.org, 2004.  2 “Wiki History,” Portland Pattern Repository Wiki, 2004.  3 “Wiki,” Wikipedia, the Free Encyclopedia, 2004.   4 “Wiki,” ISP Webopedia, 2004.   5 “What is wiki,” epowiki, 2004.   6 Rupley, Sebastian. “What’s a Wiki?,” PC Magazine (May 9, 2003). Available online at pcmag.com.  7 “What is wiki,” The Humaine Portal, 2004.   8 Ibid.  9 “Main page,” Wikipedia, the Free Encyclopedia, 2004. (as viewed on June 30, 2004).    
     

    
</post>

<date>02,July,2004</date>
<post>


       
       VERSIONING   Okay, isn’t there a grammar rule about not turning nouns into verbs?  This word is clearly out of line, and I am shocked to see it in a technical writing list of terms!  Out of protest I will not provide an intro to this term and, instead, insist that it be struck from the tomes of technical communication forever and anon, amen.   Definition provided by Tim Carter.   What is Versioning?  Versioning, or version control applies to a content management system or strategy. I have found that it is identified in two distinct working environments: technical writing and software development. It is a system or strategy that tracks the revisions made to a document or software code in an environment with multiple writers or programmers. The system keeps control of the document or program so that no two people are working on the same version at the same time, and it can track when documents were “checked out” and when they were “checked in” (source control). In this way, control is maintained, the original documentation is never lost, never incorrectly duplicated, or unnecessary changes made to a down-level document.    How Can I Use It?  As defined by Anne Rockley’s Managing Enterprise Content, versioning says that each time content is saved, a new version will be created. In the sample unified content life cycles (pg 149), there would be two levels of versioning: draft version and full version. Content would remain in draft version until approved, then it would be fully versioned. Through version control, authors would be able to view previous versions of the content or revert to an earlier version if necessary. Authors and reviewers can see the complete detailed history for each item. The history will indicate who made the change, when the change was made, and the reason for the change. Similarities exist between programmer and writer versioning in that both groups need to maintain control over their documentation to eliminate or reduce lost time and money on extensive projects.    Where Are All These Versions Stored?  According to David Thomas and Andrew Hunt in their book, Pragmatic Version Control at http://www.pragmaticprogrammer.com/starter_kit/vc/whatis.pdf, all versions of documentation in most version control systems are stored in a repository. The repository is a central place that holds the master copy of all versions of your project’s files. Some systems use a database, some use regular files and still others use a combination of the two. In any case, the repository is the pivotal component in the versioning strategy.   This repository doesn’t simply store the current copy of each of your files, it stores every version of every file that has ever been checked in. If you check out a file, edit it and check it back in, the repository will hold both the original version and the version that contains your changes. Then the versions will be assigned a numbering system. For example, the first version of a file is assigned the revision number 1.1. If a changed version is checked in, that change is given number 1.2. The next changes gets 1.3 and so on.    More Contrasts and Similarities  The TechEncyclopedia at www.techweb.com/encyclopedia narrowly defines versioning (or version control this way: The management of source code, documents, graphics and related files in a large software project. Version-control software provides a database that is used to keep track of the revisions made to a program by all the programmers and developers involved in it. This narrow definition, applied to XML language, is echoed in the Versioning SML Vocabularies by David Orchard at http://www.xml.com/pub/a/2003/12/03/versioning.html. Orchard describes versioning as, “adding, deleting, or changing parts of the language.”  Another similar versioning concept comes from John Gruber in his article, “BBEdit Version Control” at http://www.macworld.com/2003/09/secrets/bbeditversioncontrol. With the CVS (Concurrent Versions System) software, writers (or programmers) can record all changes to a document and you can quickly step back to an earlier version – even if you’ve changed and saved the file many times … a personal time machine. The CVS is useful for managing any type of document. In fact, a Web site is a perfect example of the sort of multifile project that can benefit from versioning. The difference with this concept is that you can make a mistake, then easily track down and reverse the mistake using an earlier version.  Finally, the application FrameMaker provides another concept for versioning in which two distinct versions of text for two separate products. As you will see, this definition is in stark contrast to earlier definitions. Instead of one product and multiple versions, here we have two products with simultaneously different versions. In “Designing Print Documentation” by David A. McMurrey, Thomas A. Moore and Pam Renwick at http://www.io.com/~tcm/etwr2372/planners/frame/frame556/frame_variables_ver556.html#versioncontrol they say versioning is the ability to make one set of files support two more versions. Their version control “enables you to keep everything in one set of files and hide or show a particular version. This is useful if you must maintain documentation for two versions of the same product. This versioning is available only through FrameMaker and its native documentation. 
     

    
</post>

<date>02,July,2004</date>
<post>


       
        VERSION CONTROL   Not be confused with “revisionist history,” a popular and ubiquitous form of propaganda used to shape public and political opinion.  Version control is used to drive consistency in the development and revision of texts.    Definition provided by Kathylynn St. Pierre   Dysfunctional Families and Version Control  Kathylynn St. Pierre English 6430 – Summer 2004   Oh many, many years ago When I was twenty-three I was married to a widow Who was pretty as can be This widow had a grown-up daughter Who had hair of red My father fell in love with her And soon the two were wed   This made my dad my son-in-law And changed my very life For my daughter was my mother ‘Cause she was my father’s wife To complicate the matter Though it really brought me joy I soon became the father Of a bouncing baby boy   This little baby then became A brother-In-law to Dad And so became my uncle Though it made me very sad For if he was my uncle Then that also made him brother Of the widow’s grown up daughter Who of course Is my step-mother   My father’s wife then had a son Who kept him on the run And he became my grandchild For he was my daughter’s son. My wife is now my mother’s mother And it makes me blue Because although she is my wife She’s my grandmother too.   Now if my wife is my grandmother, Then I’m her grandchild And every time I think of it It nearly drives me wild For now I have become The strangest case you ever saw As husband of my grandma I am my own Granpa.   From “I’m My Own Grandpa” by Dwight Latham and Moe Jaffe     The term “version control” applies to everything from sophisticated technology-driven systems that manage changes in data files (source code for software programs, CAD files, etc.) to the simplest form of file management: the ability to check differences from a previous version, and to provide a layer of assurance that one person’s work isn’t overwritten by someone else’s.    Some sophisticated engineering systems have file management/control and version control systems built in, others require that separate tools be used. But according to Paul D. Sherriff of Klarity.com,  “software change management is not just for development, or QA or the operations and release group. Good documentation and change process must be designed to support the whole organization; it must help everyone to better understand their products and their jobs”.   That makes perfect sense. Everyone needs to pay attention to version control issues, and clearly version control doesn’t just apply to one department, group or industry.  The Pragmatic Programmer website says “Tools to automate change management and control (SCM tools) can only work when implemented together with corporate standards and a well-defined process.” While I’m not sure that the standards are necessary (or realistic) at the corporate level, it is clear that a process must be clearly defined and integrated with every aspect of the documentation process - from file management to quality requirements to approaches to working with outsourced content.    The definitions here apply to publication and documentation departments, where version control often simply means the daily challenge of managing hundreds of files, sometimes with many writers working on them at the same time.     It is clear that there are two ways version control is applied on a daily basis: automated change control systems, and manual systems developed within documentation departments to satisfy their unique needs and circumstances.        Automated change control systems    §	Many systems, (Wiki is a popular one) use a database back-end, which records each modification in a log. Scripts within the system access the log and follow the revision history of a file.  The log is really designed for recovery from abuse or accidents that cause widespread damage to the database, and not for retrieving the individual modifications to one file inside that database. In most publication departments, Wiki-type systems are overkill for day-to-day file management.   §	Many change control systems are marketed on the “time-machine” premise - you can look at any version of any page and then follow links to check out the entire file as it was at that point in time. Engineers like this because they sometimes follow a design path so far only to find out that their design won’t work – this way they can “go back” to the point in their development before they veered off onto the “wrong” path. Some of these keep a long sequence of revisions available and include a diff feature. Engineers like to see the complete evolution of a file that lets them start alternate evolutions at any point.   §	Software Configuration Management (SCM) refers to any collection of processes and tools that are used to effectively manage the development, maintenance, and build processes of software.     §       With Concurrent Versions System (CVS), multiple writers can check out the same files and then check them back in. When working in ASCII modes, such as HTML, this works fine as it calculates the differences from the last version and applies them. Depending on your product version and platform, it figures this out quite well. But this method requires excellent communication between team members. If there is a conflict, it stops the writers from checking files back in, and forces them to manually reconcile the differences.   §	Professional Version Control Software (PVCS) – PVCS is actually a process management tool, but some technical writing projects are forced to use it because the engineering departments of the projects they document use it.     Michael Krasowski, of The Code Component Developer website says, “Without a well understood process, no tool will solve the problem.” With this in mind, it is important to mention that adapting your process to meet an automated version control system is starting the whole process backwards – you need to establish a process that works with your department’s situation, and then find a system that meets those needs.    Manual solutions that solve day to day issues     At the risk of making an unfounded generalization, I’ll admit to thinking that most publication departments find individual and creative ways of dealing with their unique version control issues.   The size of the information product, the size of the team, frequency of releases and publication dates, all play a role in how a publication department may approach version control. But it is clear that a lack of formal guidelines poses a serious problem for publication departments. Even the simplest methods for "getting" versus "checking out" documents create huge issues that most always must be addressed even before the first page is written.   But publication departments can’t rely on their chosen publishing software to take of their needs. Some publishing programs allow writers to "get" a copy of a file in read-only mode and then make it write-able. This overrides the security "check out" feature of the system that of course, proves counter-productive later and creates even more confusion.  Where I work, our documentation team was at one time very large (before company cutbacks and layoffs occurred). We write product and system documentation for a large software product, which usually involves at least two or three major releases per year.    Like the publications departments I mentioned earlier, we devised two version control systems that meet our needs.  These are probably the simplest forms of version control and involve processes similar to what I believe other publication departments like ours probably use to meet their needs.   The first process we use involves two major activities: Checking Out and Checking In.   Check Out and Check In  A copy of all files is put on a local shared server, drive, or machine (what The Pragmatic Programmers call a “repository”) and to work on files, writers must check them out (manually, through a documentation “librarian”, through a manual registry that the writer must fill out each time, or by the writer taking the file out and then “locking” it so that only the writer that checked out the file can check it back in).    This process requires strict adherence to the process itself, but at the same time protects the team from itself - it alerts the rest of the team when someone is working on that file. The last saved version overwrites the previous version, so these alerts are absolutely necessary.     You’re your own Grandpa    On some releases, all files are divided and we’re each responsible for specific files that we “own”. Only the writer assigned to a certain file works on that file. We each keep our own copies and deliver them to a special delivery directory when it is time to do documentation build (usually once a week). Then, the build directory becomes our official directory and we are each responsible for our own version control activities. The only time this becomes tricky is when we are working on two or three releases at the same time. In this case, we have to keep three different copies of the same file and make changes that may apply to one release but not another. When we practice this method, Brian Harroff, our project manager, continuously reminds us “You’re your own Grandpa”.      So much to consider    The version control business (and it is a business in and of itself) has many facets, and I’ve explored it very broadly here and have simplified it for purposes of creating a high level definition. But the bottom line to version control, whether you are using an expensive change management system or a manual system your department has devised to meet your needs, is to remember that with a well-defined process and strict adherence to the policies defined in them, being your own Grandpa isn’t all that dysfunctional after all.     References      ¾¾¾. 2004. “.Automation Through Process Definition” AFORA, International. Available http://www.scmbyafora.com .   ¾¾¾. 2004. “Version Control Applied to Wiki.” Cunningham & Cunningham, Inc. Available http://c2.com .   Grant, Samantha. 2002. “Change Management and Software Development” E-Commerce Times.   Krasowski, Michael, 2004. “Software Configuration and Management Using Visual SourceSafe nd VS. NET”. Available http://www.devx.com.   The Pragmatic Programmers, LLC, 2004 “Pragmatic Version Control with CVS.” Available http://www.whatis.com .   Sherriff, Paul D. 2004. “Approaches to Version Control Software.” Klarity. Available http://www.klariti.com . 
     

    
</post>

<date>02,July,2004</date>
<post>


       
       My college roommate got arrested for that once. What a drag, man. That open container law is a bummer. Oh, Open CONTENT, you say? Well, hmmm. I don’t know what the heck that is. Let’s find out in Steve Crow’s lively discussion (and you won’t even have to hide that drink):   The Basics  The term “open content” is aptly named; it means pretty much what is sounds like it ought to mean. In 1998, David Wiley of Opencontent.org provided this basic definition, “Open Content is a new and evolving term, but in the strictest sense it refers to content (books, articles, documentation, images, databases, etc.) that is "freely available for modification, use, and redistribution under a license similar to those used by the Open Source / Free Software community."”  Free Doesn’t Always Mean Free But “free” doesn’t always mean “free to change it, resell it, etc.” The online dictionary Word IQ : “Open content, coined by analogy with open source, describes any kind of creative work (for example, articles, pictures, audio, video, etc.) that is published under a copyright license, or in the public domain, in a format that explicitly allows the copying of the information. One example is the GNU Free Documentation License, which is used by Wikipedia and Nupedia. "Open content" is also sometimes used to describe content that can be modified by anyone. Of course, this is not without prior review by other participating parties--but there is no closed group like a commercial encyclopedia publisher which is responsible for all the editing. But not every open content is free in the GNU GPL sense (for instance the Open Directory). Some licenses attempt to maximize the freedom of all potential recipients in the future, while others maximize the freedom of the initial recipient.”   Free Content vs. (truly) Open Content The Life OpenContent site gives a little more insight into the distinction between Free content and Open content:  Free (or copyable) Content:   “This content is “open” only in the sense that it is accessible. The content may be copied freely, but not changed. It is not really “open” in the sense of “Open Source”, but it is called “open” by some people. In Software, this would be called “Freeware” - Software to which someone holds a copyright (otherwise it would be “public domain”), but allows other people to use the software for free. The fact that “free” can have two meanings (free beer vs. liberty) is a problem. I.e. the Free Software Foundation, which pioneered some important licenses for both software and content uses “free” in the sense of “liberty”. Maybe it would be better to call this content “copyable”.”   (truly) Open Content :  “This is content that may not only be copied freely, but also modified. It is “open” in the sense that it may be changed. Some other viewpoints consider “open” as “Free for certain users or applications”, i.e. “may be copied freely for personal or academic use.””  Free-definition.com reminds us that “open content” is not a stand-alone term, but that the “open content movement is yet another variation on the "open movement" theme. Others include the open systems movement, open source movement, open information movement, etc. The open content movement seeks the cataloging and/or accumulation of nonproprietary knowledge; particularly machine-readable knowledge about subjects other than computer programming.”   A Little History Please For those who want a more detailed history of Open Content, Joseph Reagle, Jr. has written an interesting review in M/C: A Journal of Media and Culture. His whole article is about 2,000 words; I’ve used fewer than 500 of them here.   “The Free Software movement was begun by Richard Stallman at MIT in the 1980s. Previously, computer science operated within the scientific norm of collaboration and information sharing. When Stallman found it difficult to obtain the source code of a troublesome Xerox printer, he feared that the norms of freedom and openness were being challenged by a different, proprietary, conceptualization of information. To challenge this shift he created the GNU Project in 1984 (Stallman 1998), the Free Software Foundation (FSF) in 1985 (Stallman 1996), and the authored the GNU General Public License in 1989.  What is often meant by the term “open” is a generalization from the Free Software, Open Source and open standards movements. Communities marshaling themselves under these banners cooperatively produce, in public view, software, technical standards, or other content that is intended to be widely shared.  In 1991, Linus Torvalds started development of Linux: a UNIX like operating system kernel, the core computer program that mediates between applications and the underlying hardware. While it was not part of the GNU Project, and differed in design philosophy and aspiration from the GNU’s kernel (Hurd), it was released under the GPL. While Stallman’s stance on “freedom” is more ideological, Torvalds approach is more pragmatic. Furthermore, other projects, such as the Apache web server, and eventually Netscape’s Mozilla web browser, were being developed in open communities and under similar licenses except that, unlike the GPL, they often permit proprietary derivations. With such a license, a company may take open source software, change it, and include it in their product without releasing their changes back to the community.  The tension between the ideology of free software and its other, additional, benefits led to the concept of Open Source in 1998. The Open Source Initiative (OSI) was founded when, “We realized it was time to dump the confrontational attitude that has been associated with ‘free software’ in the past and sell the idea strictly on the same pragmatic, business-case grounds that motivated Netscape.” (OSI 2003) Since the open source label is intended to cover open communities and licenses beyond the GPL, they have developed a meta (more abstract) Open Source Definition (OSI 1997) which defines openness as:  Free redistribution   Accessible source code Permits derived works   Ensures the integrity of the author’s source code   Prohibits discrimination against persons or groups   Prohibits discrimination against fields of endeavor   Prohibits NDA (Non-Disclosure Agreement) entanglements   Ensures the license must not be specific to a product   Ensures the license must not restrict other software   Ensures the license must be technology-neutral   Substantively, Free Software and Open Source are not that different: the differences are of motivation, personality, and strategy. Given the freedom of these communities, forking (a split of the community where work is taken in a different direction) is common to the development of the software and its communities. One can conceive of Open Source movement as having forked from Free Software movement.”  Bottom Line  Open content is virtually always free to use and reuse. However, the devil is in the details, so when you’re considering revising and reusing “open content” for commercial purposes, be sure to read the fine print of the licensing agreement. The GNU Free Documentation License, which is used by Wikipedia and Nupedia, is the industry standard.  Sources: Wiley, David. Opencontent.org. Accessed 29 Jun. 2004  .  Word IQ. Encyclopedia. Accessed 29 Jun. 2004  .   Life OpenContent. Definition. Updated 20 Aug. 2003. Accessed 29 Jun. 2004  . Free-Definition. Accessed 29 Jun. 2004  .  Reagle Jr., Joseph. "Open Content Communities" M/C: A Journal of Media and Culture Accessed 29 Jun. 2004  .      
     

    
</post>

<date>01,July,2004</date>
<post>


       
       Not a rock group, not a geology database, but “data about data”. Are we allowed to say “data about data”? Find out in Jeromy Caballero’s extended definition:   The term metadata is most frequently used in connection with technology. However, its most general definition is simply data about data. Mark Taylor, an editor with the Global Spacial Data Infrastructure Association (GSDI), has said: "Metadata is the term used to describe the summary information or characteristics of a set of data. This very general definition includes an almost limitless spectrum of possibilities ranging from human-generated textual description of a resource to machine-generated data that may be useful to software applications."1  Library Trends offers this definition: "Metadata is the value-added information that documents the administrative, descriptive, preservation, technical, and usage history and characteristics associated with resources."2   In this sense, metadata is anything from the name and address on a stamped envelope to special tags describing a packet of information traversing the Internet. Metadata could even be background information providing context for an article or a story.  Taylor also describes metadata as something that allows knowledge about data to persist from user to user. For example, as people leave an organization, knowledge about certain data might be lost if that data is undocumented.1 But if metadata—additional information describing that data—is attached to the data, knowledge of the data’s use and value can be preserved for future users.  Therefore, metadata can be defined as an interoperability enabler, providing an environment in which two systems can work together by making data from each system usable by the other system. The metadata is the common context in which the systems can cooperate and work with the same data. "The application of metadata is critical in the digital environment," observes Library Technology Reports, "because it allows a digital object or collection to be understood by both machines and humans in ways that promote interoperability."3  However, not only does metadata create context for interoperability, but it serves as a transmission/communication enabler that facilitates packaging, transmitting, routing, receiving, interpreting, and applying data over the Internet. Metadata accompanies all the data that crosses the Internet, allowing it to be correctly transmitted and correctly used when it reaches its destination, wherever that destination might be.   Because of the wide variety of data that crosses the Internet, and because its potential destinations are often unknown, the metadata that accompanies data transmitted over the Internet is complex and must account for a large array of possible uses. This metadata is difficult to incorporate into an information system that expects very specific, tailored metadata. "The metadata required to describe the highly heterogeneous, mixed_media objects on the Internet," observes Library Trends, "is infinitely more complex than simple metadata for resource discovery of textual documents through a library database."2 According to InfoWorld, the data that crosses the Internet is being collected by data stores at its destination, but the metadata is often being discarded as unuseable by those data stores. New methods of creating metadata that serves both to transmit the data and to effectively catalogue it in an information management system are the inevitable result.4   This is the type of metadata that "provides the underlying foundation upon which digital asset management systems rely to provide fast, precise access to relevant resources across networks and between organizations."2   Not only do organizations need data to be transmitted, they need data to be efficiently catalogued and easily retrievable. Information Management Journal offers this definition of metadata:  "Traditionally, the term `metadata' has been widely used to characterize the descriptive information that will support search and retrieval of both paper and electronic material. Over the past three or four years the use of the term metadata has expanded to include additional information that must be acquired and retained in order to effectively manage electronic records over long period[s] of time, including permanently."5  Metadata is a storage and retrieval facilitator that enables organizations and even groups of organizations to catalogue, archive, and access data accurately and effectively in information management systems. Data entered into a database, for example, is accompanied by metadata describing that data in many different ways. This metadata allows the data to be searched and retrieved in a variety of contexts.   Finally, metadata can be defined as a supplemental resource for data.   "Metadata is needed to describe both external and internal characteristics of the resource," explains Library Journal. "External metadata explains and maintains those critical relationships among different versions of the same content.... Internal metadata allows for the description of the content of the resource at the desired level of granularity."6  When a video camera records a segment of footage, for example, it might also record supplemental information such as the time, the pan, angle, amount of light, and so forth. This information is metadata by dint of its accompanying the actual footage and being data about data. In a book, supplemental data is often gathered in an appendix, and data about the overall contents of the book is gathered in an index. In these cases, the data establishes the context in which the metadata has value, not the other way around.      Sources:  1 http://www.gsdi.org/pubs/cookbook/chapter03.html  2 Hunter, Jane L. "A Survey of Metadata Research for Organizing the Web." Library Trends, 00242594, Fall2003, Vol. 52, Issue 2.  3 "Applications of Metadata." Library Technology Reports, Sep/Oct2002, Vol. 38 Issue 5, p60.  4 InfoWorld, 11/10/2003, Vol. 25 Issue 44, p38.  5 Baron, Jason R. "Recordkeeping in the 21st Century." Information Management Journal, 15352897, Jul99, Vol. 33, Issue 3.  6 Thomas, Judith. "Digital Video, the Final Frontier." Library Journal, 03630277, Winter2004 Net Connect, Vol. 129.    
     

    
</post>

<date>01,July,2004</date>
<post>


       
       Ooooh, I know this one! This is where newspaper columnists make millions when their sycophantic drivel gets run and rerun in papers nationwide. Yeah, that’s what it means in print publication, but what about technopublishing? Not surprisingly, it means about the same thing. Come on – didn’t you read about RSS earlier on this wonderful blog of ours? If you need a behind-the-scenes look at RSS, Tiffani’s put all the good stuff together in one place. Read on, weedhopper…     Abstract  The following is a short definition essay of the term “syndication.”  It is written for the benefit of the non-technically gifted content managers out there, though the author included highly technical passages for lack of a better explanation.  The definition essay is separated into the following divisions: ·	Introduction to Syndication  ·	What is Syndication?  ·	How Does Syndication Work?  ·	Why is Syndication so Important?  ·	How Does Syndication Relate to Content Management?    Introduction to Syndication    Understanding syndication is easier if you first understand its origin.  Syndication began as a practice among news reporting agencies and comics artists.  Their columns and comics were syndicated, or licensed and distributed widely to other newspapers for publication (“Syndication”).  This process was practiced for many years and is still widely practiced.  However, due to the evolution in computer technology, the term “syndication” now has dual meaning—one definition for print and one definition for the web.  Henceforth, this short definition will refer only to the web definition of “syndication.”    What is Syndication? “Syndication is a process” (“What is RSS”).  It makes “a section of a website available for other sites to use” (“web syndication”).  Syndication “is a way to keep updated in real-time of changes to your favorite websites” (“Syndication is”).  The process closely resembles the syndication practiced by news agencies, but becomes more complex upon deeper investigation.     Defined more comprehensively, syndication “refers to making feeds available from a site so other people can display an updating list of content from it (“web syndication”).  It “is the process of using RSS/Atom for automated updates, another way of getting the information you want” (“What is RSS”).  RSS and Atom are examples of “feeds,” or “wrapper[s] for pieces of regularly and sequentially-updated content, be they news articles, weblog posts, a series of photographs, and more” (“What is RSS”).  These feeds contain the content for the webpages they are linked to, and are continuously updated.  A result of syndication is a webpage that has news stories.  The stories are automatically updated through the feeds programmed into the page.      How Does Syndication Work?    There are two parts to understanding syndication: knowing the process and knowing the formats.    Syndication Process  Simeon Simeonov, lead web architect at Allaire Corporation, states, “Syndication covers the process of aggregating and reselling content, data, and services.”  He further expounds on the complexity of syndication, saying:   . . . XSLT, XSL, and other transformation engines are employed to convert a system working within a narrow set of specifications to an open information clearinghouse that can interoperate with any content and data format. Affiliate management services provide a framework for exposing syndication to users of an application in a controlled, secure, and business-focused manner. This includes affiliate registration, setting up affiliate profiles, designating access permissions, syndication negotiation, and activity reporting. All these services are fully automated and remotable.    Syndication Formats  There are multiple formats of syndication, including ESF, RSS, and Atom.  Currently, RSS (or “Really Simple Syndication”) is the most widely used of the formats and is XML based (Liorean).     XML syndication is similar to the process described by Simeonov, however, the description below includes the definition and an example of XML syndication:     XML Syndication is a form of electronic broad-based publishing—meaning “distributed to many people.” Publications released in one of the many XML Syndication formats are called “feeds.” Visitors to web sites offering feeds can subscribe to the feeds using an application called a Desktop Reader.  The Desktop Reader application periodically checks information in the feeds it subscribes to, looking for new information. When new information is found, the Desktop Reader gets the user’s attention, alerting that new information is available. This behavior is similar to that of an e-mail program, such as Microsoft Outlook. The way XML Syndication functions is completely different from e-mail, but it serves a similar purpose of allowing publishers to distribute the content to a massive audience. However, since feed subscription is controlled by the audience and not the publisher, XML Syndication is incapable of delivering SPAM. If a user receives unwanted information from a feed, he or she can unsubscribe from it without any interference or further “harassment” from the publisher.  Because of this significant difference from e-mail, XML Syndication is quickly becoming the new standard for broad-based electronic publishing. Many of the Desktop Readers available to users are powerful and free to download. (“What is XML”). This explanation illustrates the importance of syndication in just one way.  But syndication has other justifications.    Why is Syndication so Important?   Syndication is a huge time saver to many organizations, giving them “the ability to deliver accurate, constantly changing content in real-time [which] helps them reach new markets and build brand awareness in their established markets. Syndication also lets organizations build stronger, more meaningful extraprise relationships, fueling improved satisfaction and loyalty of customers and partners” (Eliason).    In addition, syndication plays an important part in society, as it perpetuates the distribution of up-to-date information, informing readers of cutting-edge world developments.  But syndication is also useful to content managers.    How Does Syndication Relate to Content Management?   On first glance, you might not identify syndication as a tool of content management.  However, one writer expounded on this relationship, clearly demonstrating syndication’s importance to the people in the world of content management.  Though the following is written in relation to XML syndication, the principle applies to all syndication formats: As the publisher, XML Syndication can help you to reach a significantly wider audience and reduce overhead associated with previous distribution models. This fosters a new and much more enriching environment for organizations already distributing e-mail newsletters and updates.  E-mail requires the exchange of personally identifiable information: the user’s e-mail address. Because of SPAM, users are now reluctant to release their e-mail addresses to anyone they do not know, or will provide your company with a false or unchecked e-mail address. XML Syndication requires no such transfer of personally identifiable information.  Further, e-mail requires that you maintain a list of e-mail addresses representing your distribution list. In addition, the recommended best practice of the e-mail distribution model is to provide users with easily accessible functions to add or remove themselves from your distribution lists. With XML Syndication, publishers are no longer burdened with maintaining any such list.  Even further, e-mail requires that you conduct a transaction with one or many SMTP servers to transmit your content to users. Afterwards, the SMTP server must then transmit the message to each individual user, which is time-consuming with large lists. Services are available to perform this function for e-mail publishers, but these services are a repeating cost. XML Syndication completely alleviates the encumbrance of individualized transmission.(“What is XML”)   Hopefully you have gained a more extensive knowledge of what syndication is, how it works, and ultimately, how it can contribute to the preservation of customer service relations and the personalization of customer documents.     Works Cited   Eliasson, Daniel. “Content Syndication.” http://zope.it.bond.edu.au.  June 24, 2002.  June 29, 2004. zope.it.bond.edu.au/projects/sdl/SDL-issues/content_syndication.   Liorean. “Syndication, the web, the future, XML and flying pigs.” www.web-graphics.com. May 24, 2004. June 29, 2004. http://www.web-graphics.com/mtarchive/001224.php.   Simeonov, Simon. “The Future of Web Architecture.” June 29, 2004. http://www.infoloom.com/gcaconfs/WEB/philadelphia99/simonov.HTM#N111   “Syndication.” Webopedia.com. October 16, 2003.  June 29, 2004. http://webopedia.com/TERM/s/syndication.html  “Syndication is.”mahajaba.com. June 29, 2004. www.muhajabah.com/xml-whatis.htm.   “Web syndication.” Encyclopedia.thefreedictionary.com. June 28, 2004. http://encyclopedia.thefreedictionary.com/Web%20syndication. “What is RSS/XML/Atom/Syndication?” www.mezzoblue.com  May 19, 2004. June 28, 2004. http://www.mezzoblue.com/subscribe/about/. “What is XML Syndication?” June 29, 2004. http://www.howdev.com/products/syndicationstudio2004/documentation/source/get%20started/learn%20about%20xml%20syndication/what%20is%20xml%20syndication.htm. 
     

    
</post>

<date>01,July,2004</date>
<post>


       
       Wow! In many ways, single sourcing is the heart and soul of every content management system. This is the strategy (both contextually and technologically) that lets writers take content from a single source and use it in multiple documents. If you’re new to the whole CMS game, this is a great place to start.   Single sourcing refers to reusing content. The STC’s single sourcing website defines single sourcing as "Using a single document source to generate multiple types of document outputs."  Simply Written, a company that provides single sourcing services, gave this definition of single sourcing: "So what is ‘single sourcing’? It is an updated approach to creating and maintaining that valuable asset called content or information or whatever you happen to call it. It is using a well-developed document manufacturing process instead of throwing stuff on a static page and sending it out the door. So instead of the ‘pipeline’ approach where you create a document, you create containers of information that are shared, or not shared, as needed" (Simply Written, 6).  Companies and academics discuss two main methods of single sourcing: using tools for single sourcing and using databases for single sourcing. Each of these methods moves beyond copying and pasting information into multiple documents; instead, these methods enable writers to create a master source, and from that single source, writers produce multiple documents.   Using Tools for Single Sourcing  Tool advocates recommend using tools, such as FrameMaker + SGML, to create a master document. From the master document, writers can create multiple types of manuals. For example, using FrameMaker a writer could create a single master document that contains information for both PC and Mac users. From the master document, the writer can produce separate HTML manuals for PC users and Mac users; the writer can also create printed brochures for both types of users.  In this type of single sourcing, one document contains the content for a product, and that document can be used to create multiple types of documents (HTML and PDF); however, the content is not stored in a database for constant reuse. Anne Rockley breaks using tools for single sourcing into two levels: identical content/multiple media and stable customized content (Rockley, 190).   Identical Content/Multiple Media  In this type of single sourcing, identical content is used in multiple formats. For example, a main document produces an identical online user’s manual and printed user’s manual.   Stable Customized Content  In this type of single sourcing, a main document stores the content, but the content appears differently in different formats. For example, the same five steps might contain links online and use colored graphics in a printed Quick Start Guide.   Using Databases for Single Sourcing  Database advocates recommend storing chunks of content in a database for constant reuse. Writers create chunks of content, which are stored in a central database. The content can be shared across products and across departments. Writers can link to the content, and when the writers generate documents, the database automatically populates the document with the latest content. Using databases, writers create new content chunks, and organize and shape documents with existing content chunks.  Databases can also directly provide online users with content, creating unique documents for each user. Anne Rockley breaks this type of single sourcing into two levels: dynamic customized content and electronic performance support system (Rockley, 191).   Dynamic Customized Content  This type of single sourcing stores content in a database, and users access this content on the fly. Users find information through assigned user profiles, surveys and forms, or profiling; this user information gets used to “push” information to users.   Electronic Performance Support System (EPSS)  This type of single sourcing is also based on user profiles; however, users receive information only as they need it. The computer recognizes when the user needs information, and provides the appropriate content at the appropriate time.   Conclusion  Single sourcing involves reusing information, and it runs on a continuum. Copy and paste lies on one end of the continuum, using tools such as FrameMaker lies in the middle, and using databases lies on the other end. Each of these techniques (copying, using tools, and using databases) allows writers to take content from a single source and use it in multiple documents.   Works Cited  Finger, Heidi. “The Joy of Single-sourcing.” http://www.soltys.ca/techcomm/articles/the_joy_of_single-sourcing.html Rockley, Ann. “The Impact of Single Sourcing and Technology.” http://www.rockley.com/articles/Single_Sourcing_and_Technology.pdf Simply Written. http://www.simplywritten.com/library/SSIntro.pdf STC: Single Sourcing. http://www.stcsig.org/ss/ The Center for Information Development Management. “Making a Business Case for Single Sourcing.” http://www.infomanagementcenter/pdfs/ssbusinesscase.pdf 
     

    
</post>

<date>01,July,2004</date>
<post>


       
       I’ve lost a lot of arguments when challenged with the “It’s just semantics” line. But what is semantics supposed to mean? Oh, now I get it – semantics is meaning!! For us CMS groupies, semantics really means that when we name a unit of content within our model, we give it a name that has meaning, This makes it easy for authors to identify exactly what content they should include. One of our teammates, Steven C, has done the digging on semantics…   “Semantic Information” is not a techno-ubiquitous term. Searches for “semantics” and “semantic information” on three tech sites (techdictionary.com, Cnet.com, and techtutorials.com) all generated zero responses. So, I turned to several print publications for clear definitions.    “Semantic” refers to “meaning.” Likewise, in technological jargon, “semantic information” or “semantics” always relates somehow to the meaning of a word, phrase, symbol, or instruction. I am in no way a technophile, but there seem to me to be at least three uses of the term. The first use has to do with programming instruction; the second relates to search engine methodology; the third reflects the way in which we’ll use the term in this course – semantics refers to the strategy used in naming content units in a unified content strategy.    First, let’s see a basic definition and how the term applies to programming instructions. The 1996 Dictionary of Computing generically defines semantics as “that part of the definition of a language concerned with specifying the meaning or effect of a text that is constructed according to the syntax rules of the language.” The 1996 Random House Personal Computer Dictionary takes the definition a bit further, injecting the element of programming instruction: “In linguistics, [semantics is] the study of meanings. In computer science, the term is frequently used to differentiate the meaning of an instruction from its format. The format, which covers the spelling of language components and the rules controlling how components are combined, is called the language’s syntax.”    The connection of semantics to programming instructions/language bridged into the 21st century (although this use may be archaic in 2004). The 2000 Encyclopedia of Computer Science uses several pages explaining the detailed instructional codes it refers to as “Programming Language Semantics.” This is, to me, an incomprehensible algebraic-looking code of Greek letters, subscripts, and arrows, all apparently designed to tell the computer what to do when it encounters various words and/or phrases. The 2001 Dictionary of Computer Science, Engineering, and Technology continues the line of thought, defining semantics as “the meaning of a string or sequence of toke symbols in some language, as opposed to syntax which describes how symbols may be combined independent of their meaning. The semantics of a programming language are a transformation from programs to answers.”   Let’s turn to the use of semantics in search engine methodology. Azeem Azhar, a U.K. writer, analyst and consultant on technology and society, published an enlightening article (4/25/03) discussing a Google acquisition. Forgive the lengthy quote, but Azhar explains well the importance of word meaning (semantics) in search methodology:        Google has purchased privately-held information retrieval company, Applied Semantics, a firm dotcom-formerly known as Oingo. It is a big deal because it brings together two competing schools, of which more below.    Information retrieval is the core of all search businesses. It is about creating software that solves a hard question: getting computers to understand human language with all its vagaries. These vagaries include: ·                    polysemy (words with multiple meanings like DRIVE or SET) ·        synonymy (different words with similar meanings like AIRPLANE and AIRCRAFT) ·        multi-word expressions which need to be treated as such (BILL CLINTON) ·                    errors, typos and poor grammar  For example, a key word search engine would find it hard to distinguish between A RED FISH and A FISH IN THE RED SEA   Broadly speaking there have been two major schools of thought. The first is one I call the statistical school and the second is the semantic. The statistical school held that context could be determined by look at statistical patterns within documents and across documents in a collection. Essentially, they use a variety of techniques to recognise word co-occurrence. So when words like DRIVE, CAR and HIGHWAY are used together frequently, we can make assumptions about the context of those words.   The other approach is the semantic approach. Here knowledge engineers build up a complex network of relationships, an ontology, that relates words together. So a CAR is defined as a type of VEHICLE and identical to the word AUTOMOBILE. A search on the word CAR will also turn up documents with the word AUTOMOBILE in it, even if they don’t mention it. Such semantic networks require a good deal of work and a lot of maintenance to keep them up to date.   So why is this relevant to Google and Applied Semantics? Well Google comes from the statistical school of information retrieval, albeit in a very light way. Currently, Google queries are not parsed very much at all. No stemming is applied, although some word proximity algorithms are used. Google does take advantage of one unique aspect of the Web: the interconnections between documents which provide a context-weighting to documents based on their link popularity. Applied Semantics will add a layer of semantic understanding that Google needs. There AdSense technology is already being used by a raft of web sites to improve targeting based on user-behaviour.    Finally, we turn to how we’ll use the term “semantic information” in designing unified content strategy. In “Managing Enterprise Content” (2003), Rockley defines semantic information as, “a component of an information model; uniquely identifies the content of that element, making it easy for authors to identify exactly what content they should include. Semantic information also enables the identification and reuse of specific content.” Used this way (and compared to other meanings), “semantic information” is actually a straightforward idea. It simply means that as we identify tags that link to content units, we don’t use generic names (e.g.  ). Instead, we use names that have meaning (e.g.  ). According to Rockley (and logically so), these semantic tags are of great benefit to authors engaged in opportunistic reuse of content units. Rockley differentiates semantic tags from metadata; we’ll explore that in Chapter 12.    In summary, “semantic information” always refers to a word, phrase, symbol, or instruction that is recognized for its meaning, not its form. In the realm of a unified content strategy, “semantic information” means that we tag content units with meaningful names.     Sources: Dictionary of Computing (4th Ed.). Oxford: Oxford University Press, 1996.    Margolis, Philip E. Random House Personal Computer Dictionary. New York: Random House, Inc., 1996.   Ralston, Anthony, Edwin D. Reilly, and David Hemmendiner, eds. Encyclopedia of Computer Science (4th Ed.). New York: Grove’s Dictionaries, Inc., 2000.   Laplante, Phillip, ed. Dictionary of Computer Science, Engineering, and Technology. Boca Raton, FL: CRC Press, 2001.   Azhar, Azeem. technology, economics, stuff. Website archive 25 Apr. 03, accessed 21 Jun. 04  .   Rockley, Ann. Managing Enterprise Content: A Unified Content Strategy. Indianapolis, IN: Newriders, 2003.          
     

    
</post>

<date>01,July,2004</date>
<post>


       
       Everybody knows that scalability is the rating system used by mountaineers. Or is it a measure of my willingness to check my personal poundage each morning? Nope. For techies like us, this seems like an easy one: scalability basically refers to how well (or not) a computer system can handle increasing loads. But there’s a lot more to it…    Definitions    1.	The ease with which a system or component can be modified to fit the problem area (http://www.sei.cmu.edu/str/indexes/glossary/scalability.html).  2.	The capacity of a system to handle increasing load or demand (evolt.org website at http://www.evolt.org/article/Scalability_s_New_Meaning/21/238896/).  3.	The capability of increasing the computing capacity of a website or a computer system—and the site’s or system’s ability to process more operations or transactions in a given period, in particular—by adding more, or more powerful, processor (Computerworld website at http://www.computerworld.com/hardwaretopics/hardware/story/0,10801,46308,00.html)  4.	Scalability is about reducing the time to solution for critical tasks (SAS website at hppt://support.sas.com/rnd/scalability/intro/index.html)  5.	Interface scalability is probably the most relevant to technical communicators.   According to Jeff Freund at CMS Watch (http://www.cmswatch.com/Features/TopicWatch/FeaturedTopci/?feature_id=92) “just as your content management infrastructure must scale to meet your growing web publishing needs, the interfaces to your web content management system must scale to meet the ongoing needs of your organization and editorial team.”  Content and content management needs are growing at astounding rates. Rather than wait for a problem to occur, businesses should invest in scalable systems that can expand with and accommodate their growing content management needs. In order for content management systems to be “scalable,” they must be able to handle any type of and high volumes of content. Content includes not only text but digital media (audio & video) and objects in XML applications.     The traditional way to increase scalability is to add large multi-processor servers. A newer way to increase scalability is to add many small servers. Either way, the term “scalability” usually conjures up images of lots of servers. Even though scalability usually refers to hardware, the data explosion of the information age has forced software developers to make scalability a priority and build applications that can be easily adapted to meet growth demands. Some computer languages (Java, VB, C) are more scalable than others. The reason for focus on hardware rather than software to improve scalability is that it costs less to buy big machines than it does to build efficient programs.        The concept of scalability is further broken down into two types referred to as vertical (scaling up) and horizontal (scaling out). Scaling up is achieved by adding large servers, multi-processors, or even a mainframe computer; and scaling out is achieved by adding many small servers. Vertical scaling (large servers) is most effectively used for back-end databases because database transactions work better when ran from a single application—think here of the powerful processing needed to run financial transactions for large banks. Horizontal scaling (small servers) is usually easier and faster to implement than vertical scaling and can be effectively used to run multiple copies of Web or application software because these transactions work when ran from different applications.   Windows new COM+ component architecture that comes with their 2000 operating system handles multiple transactions by balancing the load through distributed computing (multiple servers). It can be set to allow multiple transactions for Web applications or require separate transactions to maintain data integrity in high-volume databases.  	 Whether to scale up with large powerful processors or scale out with small servers depends on how large a company’s database is and the number of transactions that occur on their web applications. As a rule of thumb, when a company’s data storage needs exceed four processors, it’s time to consider scaling up.    The task of assessing scalability usually falls to engineers who are responsible for the company’s technical infrastructure. Engineers assess not only how well their existing applications respond to increased data storage and retrieval demands but how well their applications run with added resources (servers, etc.). Linear scalability means that with fixed resources performance decreases as demand increases, and when server resources increase with demand performance improves. Because people want information on demand, scalability is also about speed.    Many Content Management System (CMS) interfaces begin to break down when content demands increase, but interface scalability problems are harder to identify than technical scalability problems. Web-based interfaces are especially prone to problems. Common web-based interface scalability problems are: limited bulk operation options so that operators are forced to perform the same tasks over and over again, collaboration features that allow usage to grow beyond system scalability, content is difficult to find because it’s not separated into functional units, and lack of behavior-based filtering which would eliminate content hunting problems not solved by permission based filtering. So, to keep editorial teams productive and satisfied, decision makers need to consider interface as well as technical scalability needs.   
     

    
</post>

<date>01,July,2004</date>
<post>


       
       If you don’t already know what RSS is, and you haven’t checked out the definition of “syndication” yet, drop that mouse and step away from the computer!! RSS is a form of syndication; in fact, it means “Really Simple Syndication.”  (Is that the best acronym you’ve ever seen, or what?!) So if you’re smart, you’ll want to get a good understanding of syndication first. Then come back here and tackle RSS.    RSS – May The Best Content Win   Upfront lingo clarifications ·	Aggregator = portal  ·	A “feed” contains a list of items, each identified by a link  ·	Blog = web log, used to be personal diaries of sort, now business potential is being realized      What is RSS?   RSS is the acronym for Really Simple Syndication.  RSS can increase traffic to a site by gathering and distributing news.  RSS is a format for syndicating news and the content of news-like sites, “including major news sites like Wired, news-oriented community sites like Slashdot, and personal weblogs. But it's not just for news.”    Pretty much anything that can be broken down into discrete items can be syndicated via RSS: the "recent changes" page of a wiki, a changelog of CVS checkins, even the revision history of a book. Once information about each item is in RSS format, an RSS-aware program can check the feed for changes and react to the changes in an appropriate way. http://www.xml.com/pub/a/2002/12/18/dive-into-xml.html      Um . . . come again?  RSS is a means of content distribution.  It’s a mini database maintaining headlines and descriptions of what’s new on your site.  It is a form of syndication, hence the name.  If you look at your morning paper, there are articles from various different news sources, AP, Boston Globe, NY Times, etc.  RSS works the same way.  RSS is a format that allows specific content (like headlines) to be added to other sites.  Your specific content (of which you have full control of what or how much) is instantly distributed to other sites.  RSS can sort content by topic, time, date, and other specifics.  RSS allows peoples' computers to fetch and understand the information, so that all of the lists they're interested in can be tracked and personalized for them. It is a format that's intended for use by computers on behalf of people, rather than being directly presented to them (like HTML).  To enable this, a Web site will make an RSS feed, or channel, available, just like any other file or resource on the server. Once a feed is available, computers can regularly fetch the file to get the most recent items on the list. Most often, people will do this with an aggregator, a program that manages a number of lists and presents them in a single interface. http://www.mnot.net/rss/tutorial/   Originally, blogs were a primary market for RSS.  Readers would subscribe to a blog, and the RSS would allow the readers to receive up-to-the-minute headlines about the topics that interested them.   When blogs first began, they seemed to be a form of online diaries, but the idea of blogs has really grown.  Now there are blogs on almost every topic, and it is a cheap and easy market savvy businesses are discovering.  The RSS feeds are a wealth of information:  hourly, daily and weekly updates, new information, new products, product updates, company changes, company specifics, and the list keeps going.      How is this relevant to publication management?  RSS is a way of getting content out to the right people at the right time and in the right format.  RSS makes it easier for your users to keep up with the content they want.  It allows immediate notification when something that interests them is available on your site.   With thousands of sites now RSS-enabled and more on the way, RSS has become perhaps the most visible XML success story to date. RSS democratizes news distribution by making everyone a potential news provider. It leverages the Web's most valuable asset, content, and makes displaying high-quality relevant news on your site easy. Soon we'll see RSS portals with user-rated channels, cool RSS site of the day, build your own topic-specific portal, and highly relevant search engines. A collective weblog would be another intriguing possibility. May the best content win.  http://www.webreference.com/authoring/languages/xml/rss/intro/3.html RSS is not limited to your computer screen.  RSS feeds can also be sent to PDA's, cell phones, email ticklers and even voice updates. Email newsletters can easily be automated with RSS. “Even more compelling, affiliate networks and partners of like-minded sites (say a collection of Linux sites) can harvest each other's RSS feeds, and automatically display the new stories from the other sites in the network, driving more traffic throughout.” http://www.webreference.com/authoring/languages/xml/rss/intro/    Technical information that seems very important but is way over my head, so I’m not even going to attempt to paraphrase it Each RSS channel can contain up to 15 items and is easily parsed using Perl or other open source software. If you want more details on creating RSS files see Jonathan Eisenzopf's excellent article in the February issue of Web Techniques. But you don't have to worry about the details, we've made it easy to create your own RSS channel with free open source scripts, all Web based. More on these later. Once you've created and validated your RSS text file, register it at the various aggregators, and watch the hits roll in. Any site can now grab and display your feed regularly, driving traffic your way. Update your RSS file, and all the external sites that subscribe to your feed will be automatically updated. What can be easier? (http://www.webreference.com/authoring/languages/xml/rss/intro/)   Here are the basics of RSS (no matter what the version – there are currently 7 versions)  1.	It is XML. This means it must be well-formed, include a prolog and DTD, and all elements must be closed.  2.	The first element in the document is the   element. This includes a mandatory version attribute.  3.	The next element is the   element. This is the main container for all RSS data.  4.	The "title" element is the title, either of the entire site (if it's at the top) or of the current item (if it's within an  ).  5.	The "link" element indicates the URL of the Web page that corresponds to the RSS feed, or if it's within an  , the URL to that item.  6.	The   element describes the RSS feed or the item.  7.	The   element is the meat of the feed. These are all the headlines ("title"), URLs ("link") and descriptions that will be in your feed. http://webdesign.about.com/cs/rss/a/aa052603a.htm   Examples Below are just two examples of the thousands of sites who use RSS.  You can tell if a site uses RSS by checking the code.    Moreover http://w.moreover.com/main_site/content/index.html   Moreover asserts the following:    Moreover is proud to offer one of the most comprehensive and diverse range of sources delivering relevant, reliable and timely content;    From more than 6,500 premium websites and 200,000 business value weblogs  In 26 languages, including multiple Asian languages  From 115 countries including provincial and regional sources  Available within minutes of publication  Each source reviewed and ranked by a team of editors  Enhanced with up to 25 individual pieces of metadata    "Moreover does a sweet and swift job of ferreting out the top stories of the world -- news, research and business sites . . . trumps its competition." Time Magazine, Winner, Best of the Web   "As close as a company can get to having eyes and ears everywhere on the Web" Wired News    This site is a prime example of RSS use.  Clearly Moreover does not come up with all of this content every singe day.  Whatever they have specifically requested is delivered to them the second it hits the web.     O’Reilly http://www.oreillynet.com/meerkat/   Another site with clear examples of specific content RSS feed           Works Cited     King, Andrew B.  “WebRef and the Future of RSS”.  Jupitermedia Corporation. 27 March 2000.  Accessed 21 June 2004. http://www.webreference.com/authoring/languages/xml/rss/intro/3.html   Kyrnin, Jennifer.  “RSS – Real Simple Syndication”.  About.com. Accessed 21 June 2004. http://webdesign.about.com/cs/rss/a/aa052603a.htm   Nottingham, Mark.  “RSS Tutorial”. 22 May 2004.  Accessed 21 June 2004 http://www.mnot.net/rss/tutorial/     Pilgram, Mark.  “What is RSS?”.  O’Rilley xlm.com. 18 Dec. 2002. Accessed 21 June 2004  http://www.xml.com/pub/a/2002/12/18/dive-into-xml.html   Rees, L.C., “What is XML?” Accessed 21 June 2004. http://www.geocities.com/SiliconValley/Peaks/5957/xml.html 
     

    
</post>

<date>01,July,2004</date>
<post>


       
       Some acronyms are awesome uses of our 26 letters (see RSS)! But have you ever seen an acronym that struggles to make sense? Well you have now – PHP. PHP is supposed to stand for “PHP: Hypertext Preprocessor.” How can an acronym include itself? Too weird – I think whoever came up with this one must have taken a few extra licks on the blow pop. But anyway, PHP is a scripting language that enables dynamic web pages and boosts visitor interactivity. You’ve got to read more, right?…    Defining PHP  Since it was created in 1994, PHP (a recursive acronym for PHP: Hypertext Preprocessor) has steadily risen in popularity as a scripting language. PHP is an open-source language, meaning that the code used to create it is available for anyone to use and modify, and that the language itself is freely available to download and install.  You’ve probably encountered many web pages already that have been generated using PHP. You don’t have to know anything about PHP in order to open a .php page in your browser. However, installing PHP and getting it to work with your computer and your website is a gentle initiation into geekdom. Even with a manual to walk you through the process, you’ll still have learned plenty about your computer and your website server by the time you have PHP up and running. And once you have PHP ready to go, you still have to learn how to use it.   PHP is a scripting language.  The main online resource for all things PHP is www.php.net. "PHP is a widely-used general-purpose scripting language that is especially suited for Web development and can be embedded into HTML," says php.net on its front page.1 This is probably the most basic definition of PHP. Most people use PHP to beef up the functionalities and capabilities of their websites. PHP is primarily a scripting language that can be inserted directly into an HTML document.  "In an HTML document," says webopedia.com, " PHP script...is enclosed within special PHP tags. Because PHP is embedded within tags, the author can jump between HTML and PHP...instead of having to rely on heavy amounts of code to output HTML. And, because PHP is executed on the server, the client cannot view the PHP code."2  In English, this means that you can write PHP commands in the midst of the HTML tags in the HTML page you are creating for your website. When a visitor opens your page, however, all they see is a straight HTML page, because your web server already processed the PHP commands into HTML for them. This "server-side processing" is different than a scripting language like JavaScript, whose commands are processed by the website visitor’s computer.   PHP enables dynamic web pages.  Server-side processing enables what are known as dynamic web pages. "A dynamic Web page is a page that interacts with the user, so that each user visiting the page sees customized information. Before the page resolves, the Web server calls PHP to interpret and perform the operations called for in the PHP script."3 A dynamic web page built with PHP can display regularly updated information within a consistent layout. Rather than using static HTML tags that can’t be changed without manually opening, saving, and re-uploading the file, PHP commands enable a web page to be created with content pulled from other files placed on the server–or content generated by the computer itself using PHP’s programming functionality. Visit any large news site, like cnn.com or even news.google.com and you will see dynamic web pages at work–constantly updating content within a consistent layout at one web address. PHP boosts visitor interactivity.  Not only can PHP enable a website to display dynamically generated content, it can allow a visitor to interact with the website through their browser. "You can use [PHP] to add dynamic features to HTML pages or use it to create entire dynamic sites that generate HTML on the fly in response to user input," says viptx.net.4 PHP can be used to simply detect the kind of browser a visitor is using; it can be used to create forms, process a visitor’s input, and place the results back on the screen; it can generate pages filled with information retrieved according to the visitor’s activities and requests on the website. Its power comes through being able to add interactivity to a web page without the visitor having to leave the browser and open some other application.    PHP runs the back end.  Web developers like to talk about something called the "back end." The back end is all the code- and data-processing that happens behind the scenes at a website. PHP hangs around at the back end—taking visitor input and telling the server how to process it into more meaningful data, telling the server how to talk to a database and retrieve data, and telling the server how to generate a new page filled with useful content for the visitor.  "One of the strongest and most significant features in PHP is its support for a wide range of databases," says php.net’s PHP manual.5 PHP works wonderfully with many different databases and is compatible with database querying languages like SQL. PHP can enable a website to become a powerful database interface and provide strong search and retrieval capabilities.  "PHP has extremely useful text processing features, from the POSIX Extended or Perl regular expressions to parsing XML documents," continues php.net.5 This allows PHP to help put data from a website visitor into a form that the server can understand. Dr. Dobb’s Journal extends PHP’s data-handling powers even further: "With PHP you are not limited to output HTML. PHP's abilities includes outputting images, PDF files and even Flash movies (using libswf and Ming) generated on the fly. You can also output easily any text, such as XHTML and any other XML file. PHP can autogenerate these files, and save them in the file system, instead of printing it out, forming a server-side cache for your dynamic content."6   PHP can create applications.  Finally, PHP can be used to create entire applications that can be run on browsers or even from the desktop. "With PHP," writes Dr. Dobb’s Journal, "you develop applications that can be viewed by all HTML-capable browsers."6 The php.net manual points to PHP-GTK, an extension of PHP, as a way to create standalone applications that are handled by the user’s computer.5  PHP is a complete scripting language powerful enough to create entire applications, yet versatile enough to enable a range of simple to complex functionality boosts in HTML pages. Because of its power, versatility, compatibility with HTML, and wide availability, it continues to grow in popularity among a wide range of computer users.  1 http://www.php.net 2 http://www.webopedia.com/TERM/P/PHP.html 3 http://www.ameritech_hosting.net/resources/glossary.htm 4 http://www.viptx.net/orientation/glossary.html 5 http://www.php.net/manual/en/intro_whatcando.php 6 Ross, Darryl; Zymaris, Con. "DB FORMS: PHP, MYSQL, AND PHPLIB." Dr. Dobb's Journal, August 1, 2000, Vol. 25, Issue 8.  
     

    
</post>

<date>01,July,2004</date>
<post>


       
       Isn’t localization simply the process of translating your documents to whatever language you want? Get an English to Spanish (or Polish, or German, etc.) translator, and voila! You’ve localized your document. But wait, that approach has caused major problems for some companies:   The Dairy Association's huge success with the campaign "Got Milk?" prompted them to expand advertising to Mexico. It was soon brought to their attention that the Spanish translation read, "Are you lactating?"  Coors put its slogan, "Turn It Loose," into Spanish, where it was read as "Suffer From Diarrhea."  And my personal favorite:  In a Tokyo bar: "Special today for the ladies with nuts."  Still confused? Read Becky Johnston’s excellent definition that points out the important  differences between localization and mere translation:   Dictionary.com defines localization as a noun meaning to limit to a specific area. In businesses, the term localization usually refers to translating content; in this sense, the content becomes available for a specific area of the world.   Several localization providers do not specify a difference between straight translation and localization. However, LISA, the Localization Industry Standards Association, defines three levels of localization: translation, localization, and globalization/internationalization.  Translation Translation is the act of changing content to a new language such as changing a user’s manual from English to Spanish. ASE&T, a translation company, argues against “the erroneous notion that translation is simply ‘typing in a foreign language.’”   According to ASE&T, “The company that performs your translation is crafting your image to your foreign customers. If your documentation has taken six months to compile, revise, and edit, it is critical to allocate sufficient time to achieve similar high-quality results in a foreign language. For large-scale projects, short time lines or simultaneous release, the scheduling and management of the translation process are as important as the translation itself.”  Although translation is concerned with consistently using terms in the same document and with accurately changing to a new language, translation does not alter the original content.  Localization Localization is more involved than translation. Localization involves translating and polishing content, so it is culturally appropriate.   According to the Localization Institute, “Localization is the process of creating or adapting a product to a specific locale, i.e., to the language, cultural context, conventions and market requirements of a specific target market. With a properly localized product a user can interact with this product using his/her own language and cultural conventions. It also means that all user-visible text strings and all user documentation (printed and electronic) use the language and cultural conventions of the user. Finally, the properly localized product meets all regulatory and other requirements of the user's country/region.”  Localized content might be altered from the original content because localized content is designed for a specific region. LISA used the example of the changes that might occur when localizing a software package. “On the software programming side, screen dialog boxes and field lengths may have to be altered; date, time and currency formats changed; delimiters for figures replaced; and icons and colors adapted; to give only a few examples. What is more, in the case of bi-directional languages (such as Arabic and Hebrew) and double-byte character sets (such as those for Chinese, Japanese and Korean), more extensive reprogramming may be required to ensure that localized text and numerals are displayed correctly on the target platforms.”  Localizing involves changing the content to fit cultural expectations; whereas, translation involves only changing the language of the content. LISA also describes some of the localization changes made to chunks of content: “[T]he color, size, and shape of objects such as coins and notes, taxis, telephones and mailboxes, and buses and ambulances, traditionally vary from country to country. Vehicles may suddenly have to drive on the other side of the road, while dress codes will vary, and symbols take on a new significance. Similarly, mainstream business applications such as address databases and financial accounting packages have to be adapted to the procedures and conventions applicable in their new environments.”  Localization makes content more accessible than mere translation.  Globalization/Internationalization Globalization involves an overarching strategy for creating a global company. When a company uses globalization techniques, content becomes easier to translate and localize because the content is already culturally neutral. Many consulting companies, such as welocalize, get involved early in the content creation process. According to welocalize, this early interaction makes the content more consistent across languages and speeds up the launch date of localized content.   According to LISA, “[I]nternationalization is the ‘opposite’ or forerunner of localization. In other words, it is the process of designing and implementing a product which is as culturally and technically ‘neutral’ as possible, and which can therefore easily be localized for a specific culture or cultures. This reduces the time and resources required for the localization process, thus saving producers money and improving their time-to-market abroad. As with localization, language, technical and contents issues are involved, with project management and coordination also playing a significant role. Internationalization has now reached the point where major software publishers can release 30 or more different localized versions within a month or two of the original version, a process known as ‘sim-ship’ (short for ‘simultaneous shipment’).”  Globalized text must still be localized; however, the localization becomes less costly and more efficient. According to the Localization Institute, “Internationalization is a way of designing and producing products that can be easily adapted to different locales. This requires extracting all language, country/regional and culturally dependent elements from a product. In other words, the process of developing an application whose feature design and code design do not make assumptions based on a single locale, and whose source code simplifies the creation of different local editions of a program, is called internationalization.”  Summary Translation means changing content’s language without changing the content. Localization refers to adapting content, so it becomes applicable to specific regions. Finally, globalization or internationalization refers to writing your initial content so it is easier to translate and localize.  References  ASE&T http://www.asetquality.com/gf.htm  Dictionary.com www.dictionary.com  LISA (Localization Industry Standards Association) http://www.lisa.org/  The Localization Institute http://www.localizationinstitute.com/  welocalize http://www.welocalize.com/english/services/software_localization.html 
     

    
</post>

<date>01,July,2004</date>
<post>


       
       A tech writer friend told me how proud his 93-year old grandma (“Grammy Bertha”) was when she found out that he had become an architect. He couldn’t bring himself to tell her that he wasn’t the kind of architect she had in mind. Probably just as well. She would not have understood that while a traditional architect designs and directs the construction of buildings, an information architect is designs the organization of a website's structure and content, the labeling and categorizing of information and the design of navigation and search systems. It’s a whole new world, Bertha.      Although the term information architecture has become a commonly used term in the world of web design, its origin stems back to the 1970’s. Long before the internet was a household mainstay, Richard Saul Wurman, a trained architect who later became a skilled graphic designer coined the term information architecture. Wurman recognized the similarities between designing a building and the problems of organizing and clearly presenting information. Just as an information organizer or information architect evaluates and arranges information, when designing a building, an architect analyzes the building’s use and designs the flow and layout so that it best meets user needs.   Since Wurman’s conception of the term information architecture, many variations have evolved stemming from a need to organize an ever increasing influx of information presented by the Internet and other communication platforms. The variations fall within three basic categories: the actual structure, the physical act of creating the structure and the community that promotes the structure and practices the act of creating it.  The following definition expressed by Asilomar Institute for Information Architecture (AIfIA) further defines these categories: “1. The structural design of shared information environments. 2. The art and science of organizing and labeling web site, intranets, online communities and software to support usability and findability. 3. An emerging community of practice focused on bringing principles of design and information architecture to the digital landscape.”(AIfIA)  The Actual Structure  Information architecture is often used to describe the structure or the blue print behind the information. This can best be described by an example taken from Shel Kimen’s definition of information architecture, “In a library, for example, information architecture is a combination of the catalog system and the physical design of the building that holds the books. On the Web, information architecture is a combination of organizing a site’s content into categories and creating an interface to support those categories.” (Kimen)  The Physical Act of Creating the Structure              The occupation of web designer has become more prevalent, and the term information architecture has been adopted to describe what web designers do.  Because of this, “the physical act of creating a structure” definition of information architecture has grown to be the most common use of the term. The act of creating dynamic web sites that guide people through information is the foundation, as both of the definitions below illustrate. Mattie Langenburg refers to information architecture as, “taking content and creating a structure to present that content to an audience.” (Cohen)  Additionally, WebWord says, “Information architecture involves the design of organization and navigation systems to help people find and manage information more successfully.” (Rhodes)   The Community that Promotes the Structure and Practices the Act of Creating it  Information architecture also refers to the community that is affected, works within and works for the information environment. This community evolves as the value of structure gains visibility with key business members. The global information community serves as a communication link for the information architect to the rest of the business. This global community connects the architect to tools, development opportunities and research while advocating the information architecture basis. Because this definition of community is evolving, few concrete definitions exist. However, considering there are entire organizations and web sites devoted to the advancement of information architecture as a community, it warrants discussion as a definition of information architecture.  Although information architecture carries multiple definitions, the term itself is becoming more common as web sites become more complex and the need for specialized knowledge increases. Information architecture can refer to the actual structure of the information on a web page, while also referring to the roles of the architect in creating the structure and the community that supports it.  References  Asilomar Institute for Information Architecture (AIfIA). http://aifia.org/pg/about_aifia.php  Cohen, Sacha. Becoming an Informational Architect.  http://technology.monster.com/articles/infoarchitect/  Kimen, Shel.10 Questions about Information Architecture. CNet. June 22, 1999. http://builder.com.com/5100-31-5074224.html  Shimple, John. Information Architecture. HotWired. http://hotwired.com/webmonkey/98/28/index0a.html?tw=design     
     

    
</post>

<date>01,July,2004</date>
<post>


       
       An old boss once told me I was “extensible”, I was thrilled.  Didn’t that mean he thought I was forward compatible, able to be extended beyond my current abilities? Wow! What a compliment! Later on, I realized he actually meant to say that I was “expendible”.  Oops. Now that’s a horse of different color. Maybe I’ll suggest that my old boss read the following definition by Tim Carter (which I find not at all expendable):     Extensibility seems to be a uniform idea. As I researched this term, I didn’t find different definitions. Most applications of the term were the same in nature. What was different were the strategies and the approaches as extensibility was applied to computer programs or coding by various programmers and application producers.  From the TechWeb Encyclopedia, I found this definition which seemed to be consistent in each of the other resources. (http://www.techweb.com/encyclopedia/defineterm?term=extensible)  “The capability of being expanded or customized. For example, with extensible programming languages, programmers can add new control structures, statements or data types.”  Basically programmers can create the code for applications and operating systems to accept adaptations from third party programmers who wish to extend or diversify the original code. The reasons for extending the capabilities of original code are wide and varied, from adding software to making small ‘personality tweaks’ that can change from programmer to programmer. It is the strategy for implementing extensibility that differs from party to party.  From his Architecture of the World Wide Web, First Edition Ian Jacobs writes, “Designers can facilitate the transition process by making careful choices about extensibility during the design of a language or protocol specification. Language designers SHOULD provide mechanisms that allow any party to create extensions that do not interfere with conformance to the original specification.” This seems to be a holistic approach and one that, in the community of pure programming, exists – flying in the face of free enterprise. For example, Bill Gates and Microsoft do not allow extensions to their software except in a very narrow and controlled capacity because they need to protect the bottom line for their organization. Again from Ian Jacobs, “Application needs determine the most appropriate extension strategy for a specification. As part of defining an extensibility mechanism, a specification should set expectations about agent behavior in the face of unrecognized extensions.” These extensibility agents are either allowed at the factory or denied as the strategy of the original programmers dictates.  http://www.w3.org/TR/webarch/#pr-allow-exts  Utilizing another definition and strategy for implementation, Brian Bershad, et al. in the SPIN Operating System show the need for extensibility not only in applications but in the larger environment of operating systems. “Extensibility needs to be introduced to the extent that allows applications to safely change the operating system's interface and implementation. Extensions allow an application to specialize the underlying operating system in order to achieve a particular level of performance and functionality.”  Bershad, et al. go on to elaborate on operating system extensibility. “The need for extensibility in operating systems is shown clearly by systems such as MS-DOS, Windows, or the Macintosh Operating System. Although these systems were not designed to be extensible, their weak protection mechanisms have allowed application programmers to directly modify operating system data structures and code [Schulman et al. 92]. While individual applications have benefited from this level of freedom, the lack of safe interfaces to either operating system services or operating system extension services has created system configuration "chaos" [Draves 93].  http://216.239.51.104/search?q=cache:riW5fu6hH-gJ:www.cs.washington.edu/research/projects/spin/www/papers/SOSP95/sosp95.ps+extensibility&hl=en  From a global or open perspective, in The Object-Oriented Database System Manifesto Malcolm Atkinson, et al. write that programmers need to write their applications using extensibility “… in the following sense: there is a means to define new types and there is no distinction in usage between system defined and user defined types. Of course, this requirement strengthens that capability by saying that newly created types must have the same status as existing ones.” In other words, programmers should expect extensions, and the programmers doing the extending should respect the original code.  http://www-2.cs.cmu.edu/People/clamen/OODBMS/Manifesto/htManifesto/node10.html  Now contrast that ideal, open society to the mandates and licensing of Microsoft products. For Microsoft Visual Studio, a developer tools suite for Windows and Web applications, consumers can purchase this software to create extensibility in other Microsoft products. In the Microsoft Visual Studio 6.0 documentation, extensibility and the rules for its use are laid down in no uncertain terms: “Extensibility is the capacity to extend, or stretch, the functionality of the Microsoft Visual Studio integrated development environment (IDE). The IDE provides you with a programming interface known as the Extensibility object model, a set of powerful, easy-to-understand interfaces for customizing the environment. It allows you to hook into the IDE to create extensions known as add-ins. There are two types of extensibility: built-in and user-designed.   ·         Built-in extensibility features are already contained in the development environment. They're conveniences such as keyboard rebinding, customizable command bars, docked and tab-linked window configurations that you can name and return to at any time, new templates that you can create for the Template wizard, and so forth.  ·         User-designed extensibility features are constructed with code. They take two different forms: add-ins and wizards."   In addition to these rules, Microsoft also lays down a series of best practice ideas for the use of extensibility. Clearly, Microsoft products are extensible; however, the extensibility is controlled by Microsoft and the consumer must pay to achieve it.  http://msdn.microsoft.com/library/default.asp?url=/library/en-us/csext/html/csconExtensibilityOverview.asp  The Object-Oriented Database System Manifesto  Malcolm Atkinson, University of Glasgow; François Bancilhon, Altaïr; David DeWitt, University of Wisconsin; Klaus Dittrich, University of Zurich; David Maier, Oregon Graduate Center; Stanley Zdonik, Brown University  SPIN Operating System  Brian N. Bershad Stefan Savage Przemyslaw Pardyak Emin G"un Sirer Marc E. Fiuczynski David Becker Craig Chambers Susan Eggers  Department of Computer Science and Engineering University of Washington at Seattle  Citationsfrom SPIN Operating System [Schulman et al. 92] Schulman, A., Maxey, D., and Pietrek, M. Undocumented Windows. Addison-Wesley, 1992. [Draves 93] Draves, R. The Case for Run-Time Replaceable Kernel Modules. In Proceedings of the Fourth Workshop on Workstation Operating Systems, pages 160-164, Napa, CA, October 1993.  
     

    
</post>

<date>01,July,2004</date>
<post>


       
       DISTRIBUTED KNOWLEDGE 
     

    
</post>

<date>01,July,2004</date>
<post>


       
       EPPS (ELECRONTIC PERFORMANCE SUPPORT SYSTEM)     And now, ladies and gentlemen, the  Lord of the Blog  is pleased to present . . . the EPPS, the Web’s hit system.  Or, at least it’s one of the systems that can be used electronically, anyway.  An EPSS key features are its electronic nature, immediate accessibility, and. . . hey.  If you want to know more, you’ll just have to read extended definition.       What is an Electronic Performance Support System (EPSS)?  If we simply look at the words “Electronic Performance Support System” we can begin to form an idea of what it means—an electronic system that supports performance.  Of course this is almost too simple a definition, and a bit more detail is needed in order to fully understand what EPSS is.    Starting with the root of EPSS, we will begin our discussion with a definition provided by Gloria Grey, the individual given credit for originating EPSS. According to Grey, EPSS is “an integrated electronic environment that is available to and easily accessible by each employee and is structured to provide immediate, individualized on-line access to the full range of information, software, guidance, advice, and assistance, data, images, tools, and assessment and monitoring systems to permit job performance with minimal support and intervention by others” (Sleight, 1993). I’d like to point out several key words addressed in Grey’s definition because they will be reoccurring themes as we move on to further discuss the definition of EPSS. The words “electronic,” “immediate” and “permits job performance” typify the essential elements of EPSS.   The definition above mentions “electronic;” however, the broad term electronic leaves much open to interpretation. The following definition taken from Hyperdictionary differs from that above in that it further defines “electronic” by listing the types of information EPSS’s display such as text, graphical displays, sound and video. EPSS is described as “A system that provides electronic task guidance and support to the user at the moment of need. EPSS can provide application help, reference information, guided instructions and/or tutorials, subject matter expert advice and hints on how to perform a task more efficiently. An EPSS can combine various technologies to present the desired information. The information can be in the form of text, graphical displays, sound, and video presentations.”   Following closely with Grey’s idea of EPSS, the following definitions also explains EPSS as an electronic system that provides the user with instant information needed to efficiently perform a task. According to HPT Quick Reference Guide, “The Electronic version of Performance Support Systems is one of numerous tools of the technology age that provide workers with almost instant access to information vital to do their jobs.”  Similarly, a source sited in an essay by Tony Mostek defines EPSS as “An integration of artificial intelligence technologies, hypermedia, and computer-based training to produce a system whose components include embedded training, hypermedia help, artificial intelligence based coaches, and adaptable, model-based user interfaces.” (Mostek)   Although most of the definitions of EPSS specifically stress the electronic portion, the definition below takes a more generalized approach in describing EPSS in that no mention is made of electronic or technologies used. In fact, the definition is so broad that it could even apply to a paper-based system provided it offered problem solving and learning opportunities immediately available to the user. Another source identified in Mostek’s essay describes EPSS as “A human activity system that is able to manipulate large amounts of task related information in order to provide both a problem solving capability and learning opportunities to augment human performance in a job task by providing information and concepts in either a linear or nonlinear way, as and when they are required by the user.” (Mostek)   What can we discern from the definitions we’ve discussed? In short, EPSS’s consist of or perform the following: •	Are Predominantly Electronic •	Are Immediately available to the user •	Assist user in performing tasks •	Present information in forms such as software guidance and assistance, data, images, tools, computer based training, subject matter expert advice and assessment and monitoring tools •	Utilize technologies such as hypermedia, text, graphical displays, sound, and video      Resources  Albert Sleight, Deborah, (1993). What is Electronic Performance Support and What Isn’t?. Retrieved June, 29, 2004, from  urlLink http://www.msu.edu/~sleightd/epssyn.html . Electronic Performance Support System: Dictionary Entry and Meaning. Retrieved June 29, 2004, from  urlLink http://hyperdictionary.com/dictionary/Electronic+Performance+Support+System . An HPT Quick Reference Guide. Retrieved June 29, 2004, from  urlLink http://www.greenworks.org/hpt/non_instructional.htm . Mostek, Tony. Electronic Performance Support Systems (EPSS)-Definitions. Retrieved June, 29, 2004, from  urlLink http://www.comet.ucar.edu/~mostek/ep/epdef.htm .  
     

    
</post>

<date>01,July,2004</date>
<post>


       
       CRM (CUSTOMER RELATIONSHIP MANAGEMENT)     Colloquial Raging Musicians? Crazy Rabid Monsters?  Uh . . . no.  But good guess.  CRM stands for Customer Relationship Management and this practice is as vital to many organizations as the Colloquial Raging Musicians are to the underground classical music crowd.  Learn more about the multiple definitions of CRM as you read this extended definition.     I picked this term because I just rolled off a project in which we assessed 10 of our organizational capabilities, one of which was Customer Management Service (CRM).  I discovered that the definition we used was only a sliver of what CRM means to the rest of the planet.  Basically for us, CRM is about help desk for internal customers to deal with their issues so that our field personnel don’t have to can spend more time managing . . . customer relationships!   Okay, true confessions of a graduate student: My first stab at researching CRM entailed typing “crm definition” into my favorite search engine.  The first hit on the page was CRM Information.com!  CRM Information.com ominously stated, “What exactly is the definition of Customer Relationship Management.  Ask a dozen professionals, get a dozen different definitions.”  Further research on my part validated this claim.    Reciprocal relationships  CRM Informaiton.com posits a fairly generic definition which emphasizes the reciprocal relationship between company and customer: “CRM is used to learn more about your key customers needs in order to develop a stronger relationship with them.”  Bland as this definition may be, it pretty much covers the key points.  The site goes on to elaborate that CRM leverages technology to capture data about customers that drive activities aimed at growing and retaining customer base.  The site also highlights the fact that CRM focuses on enhancing long-term relationships with customers as a key strategic impulse.  Although customer-orientation is as old as dirt, CRM adds technology as a key leverage point between the company and the customer.   As Ephraim Schwartz state sin “CRM Gets Strategic,” “it looks as if CRM is coming back with a vengeance—due in a large part to the enterprise’s unprecedented rediscovery of that always distasteful creature: the customer.”     Zachry et al. pick up the reciprocal relationship perspective in “The Changing Face of Technical Communication,” indicating that CRM narratives have two recurrent themes 1) “establishing long-term, mutually beneficial relationships between companies and customers” and 2) “innovative uses of database technologies and their corresponding interfaces” (250).     In “Managing Customer Relationships” by Lisa Cross, the author positions data as the “fundamental factor” to CRM and places customers at the heart.  Cross’s simple CRM model, however, states that for a CRM strategy to be successful, the entire company has to “adapt a customer-centric philosophy and to change its corporate culture where needed” (52).   The result of such dogged customer-focus will improve customer retention, ease competitor pressures, and differentiate the company in the customer’s eyes through superior customer service (52).      Advantaged relationships  An article from Information Week, “CRM Tools Offer Sales-Force Solution,” focuses on improved sales as the ultimate end of CRM.  In this case, “mutually beneficial” is a bit one-sided in favor of the company.   Here the data on customers helps the sales-force to better manage the sales pipeline.  The real value in CRM technology in this instance is valued in its ability to integrate and crunch vast amounts of customer data quickly and consistently.  If a customer communicates a change in their account to one part of the company, the sales person should be aware of that change when they walk into the customer’s place of business.        Another company-advantaged view on CRM is from Ira Brenskin’s article, “Rust Belt CRM,” which concentrates on streamlining sales efficiency by using customer data to tightly segment accounts.  Specifically, CRMs allow a company to 1) “more accurately forecast needs,” 2) better “avoid too many low-profit or high-risk bids,” and 3) “to identify cross-selling opportunities.”  An interesting result of this skewed take on customer relationship building is the behavior modification factor it imposes on customers.  Brenskin points out that companies can use CRM data to give customers the “chance to modify the product design or production requirements to cut costs—or face higher prices.”      Multi-layered customer-orientation  Sam S Adkins, in “The Brave New World of Learning” published in ASTD’s T&D, highlights the emerging trend of workflow strategies and the technologies that can actualize such strategies.  By workflow strategies, Adkins is gesturing at the world of just-in-time interventions which avoid interrupting work productivity.  In such a world, “learning is experienced as a by-product of real-time collaboration with people and machines in the context of workflow” (31).   Although Adkins speaks specifically to training and development, he does mention CRM several times and underscores wide-breadth implications.  Specifically, CRM is one multiple TLAs (three-letter acronyms) that can help deliver workflow solutions.  In this case, the customer becomes internal to company.  A better-served internal customer can then better serve the external customer.     Summary  All in CRM spans the customer relationship spectrum from reciprocal customer-company relationships, to company advantaged customer relationships, to multi-layered customer-orientation.  In all the instances, data places an intricate role in understanding the customer.      Works Cited (documents attached)     Adkins, Sam S.  “The Brave New World of Learning.”  T&D. June 2003.   Breskin, Ira.  “Rust Belt CRM.”  Computerworld.  December 15, 2003.   CRM Information.com.   urlLink http://www.crminformation.com/   Cross, Lisa.  “Managing Customer Relationships.”  Graphics Arts Monthly. October  2001.   Informationweek. “CRM Tools Offer Sales-Force Solutions.” August 21, 2000.  Schwartz, Ephraim.  “CRM Gets Strategic.” Infoworld.com.  April 26, 2004.   Zachry, Mark et al. “The Changing Face of Technical Communication: New Directions for the Field in a New Millennium.” SIGDOC ’01. October 21-24, 2001. 
     

    
</post>

<date>01,July,2004</date>
<post>


       
       CHECK OUT/CHECK IN     No, this does not refer to your state of mind between the time your physics professor begins and ends the lecture.  This extended definition of check out/check in explains this practice of version control in the document production process and the various manners of implementation.    Most version control processes, whether sophisticated or manual, involve some sort of check out, check in arrangement. Some processes require manual intervention from a “librarian” of some sort; some processes are entirely automated.   It is probably best to make it clear (when applied to documentation projects) that check out, check in is a part of version control, which is a part of content management, which is a part of publishing. It’s a process within a process, which means that there are many factors that might change the intricacies of the process, and trying to define the process for every one of those factors is impossible. The definitions described here are my best at a high level interpretation of what check out, check in is and how it relates to version control, content management, and publishing.   Documentation teams use check out, check in in several ways. Most of the time, a check out, check in process is necessary when developing documentation (any sort of information product, including books, guides, online help systems, websites, web pages, etc.) in a collaborative environment. That is, individuals on a team working together on the same files or pages.    The following check out, check in approaches may be used this sort of team environment:   •	 You can’t have it until I’m finished  This sort of system allows only one person to have a particular file at any given time, preventing team members from overwriting each other's changes. When someone needs to work in a file, they check it out (this may be a manual or automated process), and while they have it no one else can access it.   •	 I have it right now and you better leave it alone  This approach does not prevent someone from opening a file that is checked out. They may receive a warning before doing so, but some automated check out, check in systems (like the one within the Dreamweaver software) allow others to override the warning and open a file anyway.  One can clearly see the innate problems within this process. The Macromedia website addresses potential problems by stating “To maximize the effectiveness of the Check In/Check Out feature, the collaborative team members must use it properly.” I’d say. A process like this one requires strict adherence to previously set standards and guidelines, otherwise the version control system it is a part of will completely fail.    •	 Let’s share and compare   Some automated check out, check in processes allow two or more people to check out the same file. When the writers check the file back in, the system compares the file, calculates the differences from the last version and applies them. If there is a conflict, it stops the writers from checking files back in, and forces them to manually reconcile the differences. I guess this system would work okay if the two writers were working in different sections of the document, but I can see where it could become a nightmare.   •	 It’s mine and I’ll check it in whenever I want  This approach deploys only half of the check out, check in process. Some teams divide files between team members, who in effect “own” their individual set of files. Each writer is responsible for his or her own working directory, so two team members working on the same file is never a problem. At designated intervals, the writers then submit their files in to an official directory, build machine - or some such location, where the “official” files live. This of course can pose its own set of problems, previously discussed in the “You’re your own Grandpa” discussion in my Version Control submission for Content Management Concepts I.   Almost every version control software supplier out will say something like “Check In/Check Out procedures are not by themselves a content management system.” (Clavister AB.)  Version control, too, means much more than simply making sure files don’t get overwritten as a result of more than one person working in a particular file. But having good check out, check in procedures is one part of an overall content management system that need not cost thousands of dollars to implement. All it takes is a well-defined process and the team’s strict adherence to that process.    References     ----. 2004. “Check In/Check Out: managing a Web site in a team environment”. Macromedia.  Available  urlLink http://www.macromedia.com  .   ----. 2004. “Sharing files between team members.” Macromedia.  Available  urlLink http://www.macromedia.com  .   ----. 2003. “The Check Out and Check In Concept.” Clavister AB. Available  urlLink http://www.clavister.com  .   
     

    
</post>

<date>01,July,2004</date>
<post>


       
       ASSOCIATIONS (HIERARCHY & TAXONOMY)     “Isn’t taxonomy the art of stuffing dead animals?” you might ask.  Well, not quite, though your characterization of the word “taxonomy” can serve as an association with the term taxidermy.  In this extended definition by Ms. Alison Leyda Straquadine, associations are discussed in detail from their origins to their present-day well, associations with hierarchy and taxonomy.    There are many definitions of association. Following are the definitions of association I consider most relevant to a discussion of content management.  Something linked in memory or imagination with a thing or person  the process of forming mental connections or bonds between sensations, ideas, or memories Empiricism, a theory that all knowledge originates in experience, started with Aristotle. Aristotle accepted the concept of association (connection between ideas/events) as the process through which experience is converted to knowledge. When people associate ideas and/or events, one triggers the other.   Taking association beyond the knowledge stage, the theory was further developed by philosophers and learning theorists to become one of the two main behavioral learning theories. Association is a behavioral theory because it holds that memory and behavior are stimulated through associative ideas and events.  Association as a connection between ideas and events is related to connectionism, contiguity, and situated learning theories. E Thorndike created the theory of connectionism which holds that we learn by forming associations between stimuli and response. E. Guthrie’s contiguity theory gets into the behavioral domain. Contiguity theory holds that a stimulus accompanied by movement will result trigger the same movement when it recurs. (The stimulus response theory led to Pavlovian research and the behavioral tradition).  James Mill defined contiguity as complex ideas made up of simple sensations that have become association with each other. Because the more we experience contiguous (close together in time or space) events the more strongly we associate with them, contiguous events can change behavior. Consistent with contiguity theory which holds that learning involves the conditioning of behavior, instruction must present specific tasks ( urlLink http://www.educationaau.edu.au/archives/cp/04b.htm ).  J. Lave came up with situated learning theory which holds that we learn through social interaction. The focus of situated learning theory is on “learning by doing”  urlLink (http://otec.uoregon.edu/lerning_theory.htm) . On this website, I found Greg Kearsley’s discussion of Jean Lave particularly relevant to the social interaction and learning that takes place in our online discussions in this class. In it, Kearsley states that learners become involved in a community of practice which embodies certain beliefs and behaviors to be acquired. As the beginner or newcomer moves from the periphery of this community to its center, they become more active and engaged within the culture and hence assume the role of expert or oldtimer. I found Kearsley’s statement relevant to class community because I can see myself moving from the periphery to the center as I become more familiar and comfortable with how the online seminars work.    Also related to association theory are hierarchy and taxonomy. Maslow’s hierarchy of human needs which evolves from physiological, safety/security/protection, belonging/love/affection, and self-esteem to self-actualization, can be applied to learning theories and teaching strategies. A humanistic orientation to Maslow’s hierarchy holds that as people fulfill each subsequent need on the hierarchy, the are motivated to fulfill th next. Therefore, people who have fulfilled the lower level needs of security and affection are motivated to fulfill the higher level needs of self-esteem and self-actualization through learning.    Taxonomy is defined as the study of the general principles of scientific classification. In Benjamin Bloom’s famous taxonomy of learning domains, he classifies learning into three domains (cognitive/mental, affective/emotional, and psychomotor/physical) which he further divides into hierarchical step staring with the simplest and progressing to the most complex. The cognitive domain begins with knowledge (recall of data) and moves from comprehension (interpretation), application, and analysis, to synthesis, and finally to evaluation. Bloom developed the classification and hierarchy of the cognitive domain, and his buddy David Krathwohl developed the classification and hierarchy of the affective domain. The guy that was supposed to develop the psychomotor domain with Bloom and Krathwohl didn’t do it, but others have developed it since.     In Taxonomy of Educational Objectives: The Classification of Educational Goals, Handbook 1: the Cognitive Domain, Bloom devotes an entire chapter to defining the difference between taxonomy and classifications and discusses the problems with hierarchies. Bloom decided to go with a hierarchy to develop the cognitive domain because each area tended to build on behaviors from the previous class.   Familiarity with the theories of association (connections between ideas), hierarchy (information that builds from previous information) and taxonomy (classification schemas) may become more useful to us as we get into the content reuse, maps and information architecture aspects of our content management plan. 
     

    
</post>

<date>01,July,2004</date>
<post>


       
       ASSET RESPOSITORY    What is an asset repository?  You mean you don’t know?  Well, here’s the beef.  An asset repository is similar to a bank depository—it stores valuables, although in the case of asset repository, it stores valuable information.  These repositories are high class, elaborate, and require the same thing that banks do—lots and lots of money.  Read more about asset repositories in the following extended definition.     “An asset is only an asset when you can find it,  or you know that you have it in the first place”   Teri Ross, Techexchange.com    An asset repository is rather a combination two elements: digital asset management and asset repository.  Whereas the digital asset management (DAM) is a tool to organize digital media assets for storage and retrieval, an asset repository is where and how the data is stored, typically on “a high-end UNIX server, [with] large online storage space and high-speed networks” (Ross).  The DAM application organizes and retrieves the digital media and the asset repository stores it.    Digital Asset Management   Digital Asset Management (DAM) is a means to keep, find and secure files of text, images or sound.  DAM also tracks ownership and rights of the content and implements task management.  Well organized and accessible content results in the ability to quickly find images and data, which systemizes workflow, saving time and money (Brown).   In the computer world . . . there are systems that will pick up after us and remember where we put things. These systems will retrieve items, let us pass them around, and then clean the place up when we're through. I refer, of course, to the software and hardware systems that are called, variously, digital media management, media asset management or digital asset management (DAM) (Smith).  Digital asset management is the software and hardware needed for content management.  The location in which content is stored is the asset repository.   Asset (or Content) Repository  Actual “asset repositories” are generally found in large, high-end asset management systems ($100,000 and up).  Asset repositories are a “secure collection of the source files themselves”.  The image at the webpage below shows the asset repository as a sort of holding tank for content.   urlLink http://www.assetnow.com/index.cfm/4,437,html   A large asset repository performs like a database.  It contains: security, replication, backup safeguards, disaster recovery, referential integrity, centralized data management and a hierarchical storage structure. “Such a system lends itself to workflows that require access security, including management of rights or permissions and access for suppliers or customers. These systems require a high-performance server, a fast network and significant online storage capacity” (Brown).  Asset repositories are sophisticated and expensive systems that contain the actual content within a secure database.  This usually means higher performance software like , “high-end UNIX servers, formidable online storage, and very high-speed networks”.  This allows for “security levels, replication, referential integrity, and centralized data management. Also included is the comfort of full hierarchical storage management and disaster recovery” (Ross).     Solutions based on the asset repository model are ideal when systematizing studios with industrial workflow, managing rights and permissions (such as the intellectual property of either your company or a third party), and structuring global access by employees, contractors, suppliers, partners, and customers (Smith).   Media Catalog vs. Asset Repository  Instead of an asset repository, smaller companies often use media catalogs “that store proxies alone, indexed to the source files" (Brown).  Media catalogs have proxies (like thumbnails) which are stored in a database indexed by keywords.  The actual source files are under the control of the operating system and are not touched.  Media catalogs are low cost, easy to install and administer and have “scalability across multiple divisions of an enterprise” (Ross).  Media catalogs don’t manage the actual content, so anyone with system access to view, change, move or delete content elements (Ross).     Media Catalog  Entry level solution Desktop or within a workgroup Organizes small collections of images, graphics and text Provides multiple access Cost – under $50,000   Asset Repository  Mid-range solution Client/server technology Wide area networks Manages all assets used in a production job (layouts, images, fonts) Files stored on server, access via DAM system only Check-in/check-out to control versions Stores assets with links for simple retrieval Cost – under $100,00   Upper-End Solutions 3-tier client/server architecture Designed to “drive enterprise on the Internet” Cost – over $100,00 (Smith) 
     

    
</post>


</Blog>