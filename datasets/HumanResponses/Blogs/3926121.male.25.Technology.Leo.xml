<Blog>

<date>30,July,2004</date>
<post>

	 
      Testing Java classes that have a main function (i.e. they're meant to be run from the command line) can be a bit tricky.  Most main functions print to the standard out and err streams and call  System.exit(1)  if something goes wrong.  In order to test the correct functionality of the method for all conditionals, the test method is going to have to read the output streams and prevent  System.exit  from ending the test itself.    The solutions given here are generalized for most main functions, but will work on other code that has  System.out ,  System.err , or  System.exit  calls.    Hopefully the code below is mostly self-explanitory.  The output streams are captured by calling the  System.setOut  and  System.setErr  methods.  The output to those streams are stored in ByteArrayOutputStreams which are easily converted into Strings by using the toString method.  You should usually test for both expecting and not expecting error and output.  If you know the exact error or output that is expected, or even a phrase or keyword, that should be tested for also.    The  System.exit  is handled by setting a restrictive security policy.  This can be done using Java's security.policy file (I think...), but it is more verbose and manipulatable to set the JVM's SecurityManager class. I create an anonymous inner class that overwrites the checkExit function and throws a  SecurityException  in every case.  I also reset the JVM's SecurityManager back to the original one after the test is done to prevent any unintentional side effects from the test method.          public class MainFunctionTest extends TestCase {    ByteArrayOutputStream out;    ByteArrayOutputStream err;      /**     * Sets up the fixture by creating System.out and System.err redirects.     * This method is called before a test is executed.     */    protected void setUp() throws Exception {      super.setUp();      out = new ByteArrayOutputStream();      System.setOut(new PrintStream(out));      err = new ByteArrayOutputStream();      System.setErr(new PrintStream(err));    }      public void testOut() {      String[] args = new String[]{"arg1"};      JBench.main(args);        // Expect nothing on the error stream, something on the output stream      assertTrue(err.toString().length() == 0);      assertTrue(out.toString().length() > 0);    }        public void testNoArgs() {      SecurityManager curSecurityManager = System.getSecurityManager();      System.setSecurityManager(          new java.lang.SecurityManager() {            public void checkExit(int code) {              throw new SecurityException();            }          });      try {        JBench.main(new String[]{});        fail();      } catch (SecurityException ex) {        /* expected */      } finally {        System.setSecurityManager(curSecurityManager);      }    }  }          Do not take the above code as a good example of a UnitTest class.  I left a lot of "good practices" out for the sake of brevity.    -----------------     Quick update on JBench.  The project has been dead for 2 years.  I contacted the original developer and he is currently working on .NET stuff and won't be updating JBench.  I'm working on refactoring it and adding a bunch of new features.  The HtmlReport feature is really cool.  It creates an HTML page with all the benchmark information, stats, and charts generated by  urlLink JFreeChart .  I'll have a longer update when I've finished it.   
    
</post>

<date>20,July,2004</date>
<post>

	 
      I just ran across a Java Benchmarking Framework called  urlLink JBench . I was quite interested since I haven't tried any official benchmarking, so I downloaded and reviewed it. It's not a stand-alone benchmarking tool like a CPU or graphics card benchmarker. It is a framework similar to JUnit that requires the programmer to put some thought into what they are testing and actually write some code. Without further ado, here's my review along with some generic benchmark testing notes.        BENCHMARK TESTS    ---------------------------     Purpose of Benchmark Tests   The purpose of benchmark tests is to determine how a certain implementation performs. Benchmark tests are commonly used to compare different algorithms, data structures, implementation, etc. against each other. Another benefit of benchmark tests is to prove that your implementation is correct for the situation. It can be used in documentation and to answer future "Why did you do it that way?" questions.    Benchmarks are not functional tests. While they might test some functionality, their main purpose is not to verify the functional correctness of the code.    Benchmarks are not load or stress tests. While they put load and stress on the code, their main purpose is to test the speed of the code and not to test how the code handles a stress-induced crash or how it effects the system resources.    Benchmarks are not performance tests. They do output performance statistics, but those statistics are not used to determine a pass/fail of the performance requirements.       Benchmark Purpose Examples   Test performance of code with logging at different levels.  Test in-house code against third-party code that does something similar.  Test a generic implementation against a specific implementation.  Test an algorithm's efficiency with different parameters, different sizes, etc.  Test the same code under different resources (JVMs, OSs, CPUs, RAM sizes, etc.)       JBench   JBench is a framework for benchmarking java code much in the same way that JUnit is a framework for unittesting java code.    To create a Benchmark test using the JBench framework, create a class that implements the uk.org.skeet.jbench.BenchTask interface. Overwrite the constructor to do one-time setups, checkConfiguration() to verify the data in the properties file, prepareTests() for setup that needs to be done before the tests are run (only called once per task, not once per jbench.runs),  runTest() to actually execute the code being benchmarked, checkResults() to do any post-test verification, and getDescription that returns a String which is printed out to describe the test being run. All variables defined for the test in the properties file should have a setVariable() method.    All the test parameters and scenarios are defined in a properties file given as a command-line option to the JBench program. Some items listed in this file is how many iterations of the runTest function is done for each test, which classes are the benchmark tests along with test variables, and information to output. The same test can be listed twice with different variables which allows it to be benchmarked in different ways.    The one concern I have is that it hasn't been developed for 3 years. There is only mention of java 1.1 and 1.2 on the website, so the behavior on 1.3 and 1.4 is unknown. Also, operating systems that have been developed in the past 3 years (Linux 2.6 kernel, Solaris 9, Windows XP) might produce unknown behavior. Judging by the small examples I have done (java 1.4 on Linux 2.6 kernel), it seems to perform correctly.    Website: http://www.yoda.arachsys.com/java/jbench/       JBench Output Analysis   Before the tests are run, data is printed out on the environment, like the JVM, OS, and JRE used. This data can be used to distinguish between tests where those parameters change. Next the data for each test is printed out. This includes a description of the test, the result of the test, and if successful, the time it took for each test iteration (specified by jbench.runs). At the bottom of each test is the mean, variance, and standard deviation.    To give a quick refresher of the terms:  - The mean of a data set is simply the arithmetic average of the values in the set, obtained by summing the values and dividing by the number of values.    - The variance of a data set is the arithmetic average of the squared differences between the values and the mean.    - The standard deviation is the square root of the variance    The mean is a measure of the center of the distribution. The variance and the standard deviation are both measures of the spread of the distribution about the mean. For benchmarking purposes, the smaller the mean the better (i.e. the faster the code executed), and the smaller the standard deviation the better. A large standard deviation indicates that the function is inconsistant, the testing platform is unstable, or other events are occuring regularly like garbage collection.       Benchmark Coding Standards (my personal preferences)   The benchmark tests should be kept in a seperate directory structure that is parallel to the source tree. No test code, including benchmark tests, should be kept in the same location or file as production code. All configuration files, libs, and all other supporting files should also be kept under this seperate and parallel directory structure.    The test class should be named the same as the class it is benchmarking, but appended with JBench (i.e. ListJBench). If the test class is only testing a function or specific functionality of that class, then it should be named approprietly (i.e. Class_functionJBench). Note that if a different framework is used for benchmarking, then a unique name for that framework should be appended instead of JBench.    Example Setup:  root/[src,conf,lib]  root/src/com/jake/commons/datastructure/ReadWriteLock.java  root/bench/lib/jbench-0.61.jar  root/bench/conf/datastructure.jbench  root/bench/src/com/jake/commons/datastructure/ReadWriteLockJBench.java  root/bench/src/com/jake/commons/datastructure/ReadWriteLock_allWritersJBench.java       JBench Example Run   LD_LIBRARY_PATH=benchmark/shlib/linux java -Xruncputimer -classpath $CLASSPATH:/home/jake/IdeaProjects/examples/target/test-classes: /home/jake/IdeaProjects/examples/target/classes: /home/jewerdt/IdeaProjects/examples/benchmark/lib/jbench-0.61.jar uk.org.skeet.jbench.JBench benchmark/conf/datastructure.jbench   
    
</post>

<date>15,July,2004</date>
<post>

	 
      Here's an example of the diagram below in a Use Case instead of Sequence format.  There is a little more information in it, but as you can see the readability and comprehension is much lower.   urlLink   
    
</post>

<date>14,July,2004</date>
<post>

	 
      Ok, I experimented a bit with diagramming the development process of a new feature to an existing system.  The UML diagram that worked the best for me was a Sequence Diagram. Maybe I'll post some of the other UML diagrams for a compare/contrast. There is a ton of information left out like what the requirements and specs should contain, the different types of developer and QA tests, and integration of the feature into the existing product, infrastructure and build systems.  I think accompaning the diagram with a checklist (pdf) of tasks and tasks' tasks is the next step.   urlLink     CLICK the image for a larger and readable picture    A nice site with examples of the different types of UML diagrams:  urlLink SmartDraw UML Center 
    
</post>

<date>14,July,2004</date>
<post>

	 
      This was going to be a post of how to tell when you or someone else is actually done with a task.  However, I am going to expand more to cover the entire process, albeit quite undetailed, for both a development and a testing scenario.  One issue that comes up is the difference between a project and a task (what is a task to one person can be a project to another person) and realizing that -done- and different percentages of -done- can be different in each case.  This is also recursive, so each checkpoint of done-ness of the project has to be -done- and therefore needs it's own checkpoints of done-ness. I suppose at a certain point, the task will be small/independent enough that it can't be considered a project.  At this point the person responsible for the task can say (and be trusted) how close the task is to being finished.  This first scenario is for a significant addition or change to an existing system.  With a little tweaking, this can probably be adapted to an entirely new system.  Below is the breakdown of the various parts of the process with percentages (of -done-) assigned to the completion of each process.  I would really like to break down each section individually and create a chart like this based on only that section (and recursive), but this will do for now.  10% - Requirements are finalized and signed off on. 20% - Development planning is done and documented, with appropriate diagrams and additional specs (or additions to the original spec).  These specs should also include boundary cases and testing cases/ideas/areas and performance requirement/expectations. 30% - Development is done (phase 1). 40% - Developer tests are done (unit tests in the 90-100% code coverage range) and signed off on by QA.  QA should sign off that the test cases are thorough, developed according to set standards and conventions, are easily runnable, and actually work. 50% - QA writes tests for the developed piece.  Depending on the size/function of what was developed, a separate functional test can be written for the piece, otherwise it can be combined into existing functional tests.  Performance tests should also be written if any performance specs were given. 60% - Development is done with incorporating QA bugs, suggestions, changes (phase 2). 70% - QA verifies phase 2 of development.  This step and the previous step should be repeated as often as needed.  QA then integrates the piece into the current acceptance, regression, load, etc. tests and/or writes new ones for the piece. 80% - Development is done incorporating QA bugs, suggestions, changes (phase 3). 90% - QA verifies and signs off on phase 3 of development.  The previous step should be repeated as often as needed. 100% - Documentation is completed by the developer, which includes javadocs, updating current documentation, and updating the original specs and requirements with the information on how the final product is different.  The project manager analyzes the project for information like how accurate the time estimates were, where the problems occurred, etc.     The second scenario is for a post-production bug fix or small change request (will be referred to as a bug for this example).  One thing to keep in mind here is that not everyone is tasked 100% for the duration of the project, however it is not always safe to task them on a different project as they may be needed back at the current project in the future.  10% - Complete description of the bug, symptoms, and how to tell it has been fixed. 20% - QA duplicates the bug and writes test that can be used to automatically duplicate the bug.  The test should be written to give the developer specific steps taken, log files, thread dumps, performance statistics, and whatever else is needed. 30% - Developer analyzes bug description, gives written analysis of what is causing the bug and what will be involved (resources, schedule estimate, possible side-effects) to fix the bug. 40% - QA analyzes the bug description and the developer's analysis, and gives written analysis on what will be involved (resources, schedule estimates) to verify the bug. 50% - Sign off by BIZ, DEV, and QA to commit to the fixing of the bug. 60% - Development planning is done and documented, with appropriate diagrams and additional specs.  If this bug fix will change something specified in a previous spec, doc, or diagram, an addendum should be made to that documentation. These specs should also include boundary cases and testing cases/ideas/areas and performance requirement/expectations. 70% - QA runs verification test, writes additional tests as needed, and incorporates change into current tests.  This step and the previous step should be repeated as often as needed. 80% - Documentation is completed by the developer, which includes javadocs, updating current documentation, and updating the original specs and requirements with the information on how the final product is different.  The project manager analyzes the project for information like how accurate the time estimates were, where the problems occurred, etc.   100% - A new build of the software is completed and released successfully.  I seem to have opened a can of worms that I don't know how to get myself out of.  I'll awkwardly end the post here, but I'll probably re-visit this topic after I have covered other topics.  writing this gives me other ideas for blogs. - What makes good requirements, differentiated by the size and function of what is being developed. - What determines well-written developer tests.  Coding conventions, non-dependence on other units, boundary cases, negative tests, etc. - Expand on the project lifecycle.  Mark tasks as BIZ, DEV, DOC, and QA.  Mark tasks as optional if needed. - Create a project checklist that can be used (and customized according to project/task). - A UML Use-Case diagram for different development processes.  Inspiration for this blog was from this article:  urlLink StickyMinds - When is Done Really Done?  
    
</post>

<date>14,July,2004</date>
<post>

	 
      This is my first time writting a blog.  The blogs I write here will mainly focus on the problems I see at work (and am trying to fix) and the good things I see at work that I want to write about for future reference.  I don't have any expectations for this blog besides personal use (I have a bad memory, so writting my thoughts down helps me remember them later).  However, if I write an astoundingly good blog, I might rewrite it as an article and submit it to a web site or magazine.  Who knows?
    
</post>


</Blog>